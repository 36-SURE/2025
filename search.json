[
  {
    "objectID": "r-setup.html#r-and-rstudio",
    "href": "r-setup.html#r-and-rstudio",
    "title": "R and RStudio Setup",
    "section": "R and RStudio",
    "text": "R and RStudio\nR is an open-source programming language for statistical computing. R is widely-used in both academia and industry, due to its capacity for statistical analysis and data science.\nIn order to use R effectively, you need a suitable editing environment, i.e. RStudio. For more context, RStudio is an integrated development environment (IDE) developed specifically for R programming. Although R can be run without RStudio, RStudio provides a more user-friendly experience with additional functionality.\nTo download R and RStudio, visit https://posit.co/download/rstudio-desktop.\nYou will see the following two tasks to be completed:\n\n1: Install R\n2: Install RStudio"
  },
  {
    "objectID": "r-setup.html#step-1-install-r",
    "href": "r-setup.html#step-1-install-r",
    "title": "R and RStudio Setup",
    "section": "Step 1: Install R",
    "text": "Step 1: Install R\n(Note that the following instructions apply to the latest R version (4.5.0) as of May 1, 2025)\nClick on DOWNLOAD AND INSTALL R. This will direct you to the CRAN (Comprehensive R Archive Network) website.\nmacOS\n\nClick on Download R for macOS.\nChoose the .pkg file suitable for your Mac (Apple silicon (M1,2,..) Macs or older Intel Macs.)\nOpen the .pkg file after the download is complete.\nFollow the installation instructions.\n\nWindows\n\nClick on Download R for Windows.\nClick on install R for the first time (on the same line as the base subdirectory.)\nChoose Download R-4.5.0 for Windows.\nOpen the .exe file after the download is complete.\nFollow the installation instructions."
  },
  {
    "objectID": "r-setup.html#step-2-install-rstudio",
    "href": "r-setup.html#step-2-install-rstudio",
    "title": "R and RStudio Setup",
    "section": "Step 2: Install RStudio",
    "text": "Step 2: Install RStudio\n\n\n\n\n\n\nImportant\n\n\n\nR must be installed before RStudio.\n\n\nClick on DOWNLOAD RSTUDIO DESKTOP… Your operating system is automatically detected. (If your OS is not correctly detected, scroll down and choose the right version for your system.)\nmacOS\n\nOpen the .dmg file after the download is complete.\nDrag and drop it to your Applications folder.\n\nWindows\n\nOpen the .exe file after the download is complete.\nFollow the installation instructions."
  },
  {
    "objectID": "r-setup.html#step3",
    "href": "r-setup.html#step3",
    "title": "R and RStudio Setup",
    "section": "Step 3: Check R and RStudio installations",
    "text": "Step 3: Check R and RStudio installations\nOpen RStudio and type in the following command in the Console pane.\n\nversion\n\nThis will print out the current version of R on your machine. The output should look similar to what shown below (the first 4 lines might be different, depending on your operating system.)\n\n\n               _                           \nplatform       aarch64-apple-darwin20      \narch           aarch64                     \nos             darwin20                    \nsystem         aarch64, darwin20           \nstatus                                     \nmajor          4                           \nminor          5.0                         \nyear           2025                        \nmonth          04                          \nday            11                          \nsvn rev        88135                       \nlanguage       R                           \nversion.string R version 4.5.0 (2025-04-11)\nnickname       How About a Twenty-Six"
  },
  {
    "objectID": "r-setup.html#step-4-install-an-r-package",
    "href": "r-setup.html#step-4-install-an-r-package",
    "title": "R and RStudio Setup",
    "section": "Step 4: Install an R package",
    "text": "Step 4: Install an R package\nIn R, a package is a collection of functions, data, and compiled code. In addition to a set of built-in base packages, there are numerous external R packages written by the community to add specific functionality.\nIn general, to install an R package, you can use the install.packages() function and pass in the package name.\nThe following example shows how to install the tidyverse package in R. The tidyverse is a suite of R packages we will be using throughout this program. It features popular packages such as ggplot2 for data visualization and dplyr for data manipulation.\nAfter installing R and RStudio, open RStudio and enter the following command in the Console pane.\n\ninstall.packages(\"tidyverse\")\n\nTo verify that tidyverse is successfully installed, run the following command:\n\nlibrary(tidyverse)\n\nYou should get a message similar to the output below.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nAsk us for help if you encounter any issues or errors in any of the installation steps above."
  },
  {
    "objectID": "r-setup.html#r-primers-on-posit-cloud",
    "href": "r-setup.html#r-primers-on-posit-cloud",
    "title": "R and RStudio Setup",
    "section": "R Primers on Posit Cloud",
    "text": "R Primers on Posit Cloud\nIn addition to following the steps above for installing R and RStudio on your computer, we recommend you make a free Posit Cloud (formerly RStudio Cloud) account at https://posit.cloud/. This is a free, browser-based version of R and RStudio that also provides access to a growing number of relevant R tutorials / primers.\nAfter you create a Posit Cloud account, navigate to the menu on the left and click on “Recipes”. This brings up a menu of tutorials, with code primers you can choose to work through. Please complete the tutorials listed under “R Basics” (you can skip the first three). Also, feel free to explore the other tutorials."
  },
  {
    "objectID": "lectures/01-explore.html#workflow-diagram",
    "href": "lectures/01-explore.html#workflow-diagram",
    "title": "Exploring data: into the tidyverse",
    "section": "Workflow diagram",
    "text": "Workflow diagram\n\n\n\nSource: R for Data Science (2e)\n\n\n\nExploring data: data wrangling and data visualization\nAspects of data wrangling\n\nimport: load in data (e.g., read_csv())\ntidy: each row is an observation, each column is a variable\ntransform: filter observations, create new variables, etc."
  },
  {
    "objectID": "lectures/01-explore.html#exploratory-data-analysis",
    "href": "lectures/01-explore.html#exploratory-data-analysis",
    "title": "Exploring data: into the tidyverse",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nWhat is the goal of EDA?\n\nto perform initial investigations on the data in order to better understand the data, discover trends/patterns, spot anomalies, etc.\n\n\n\n\n“EDA is an iterative cycle”\n\nquestions about data ⟶ wrangling ⟶ visualization\n\n\n\n\n\n“EDA is a state of mind”\n\n\n\n\nThe term EDA was coined by statistician John Tukey in the 1970s."
  },
  {
    "objectID": "lectures/01-explore.html#exploratory-data-analysis-contd",
    "href": "lectures/01-explore.html#exploratory-data-analysis-contd",
    "title": "Exploring data: into the tidyverse",
    "section": "Exploratory data analysis (cont’d)",
    "text": "Exploratory data analysis (cont’d)\n\nData can be explored numerically (tables, descriptive statistics, etc.) or visually (graphs)\nExamples of questions\n\nWhat type of variation do the variables display?\nWhat type of relationships exist between variables?\n\n\n\n\nEDA is NOT a replacement for statistical inference and learning\n\n\n\n\nEDA is an important and necessary step to build intuition"
  },
  {
    "objectID": "lectures/01-explore.html#first-example-mlb-batting",
    "href": "lectures/01-explore.html#first-example-mlb-batting",
    "title": "Exploring data: into the tidyverse",
    "section": "First example: MLB batting",
    "text": "First example: MLB batting\n\nImport Batting table of historical batting statistics from the Lahman\n\n\nlibrary(tidyverse) # load the tidyverse\nlibrary(Lahman) # load the Lahman package to access its datasets\nBatting &lt;- as_tibble(Batting) # initialize the Batting dataset\n\n\n\nBasic info about the Batting dataset\n\n\n# number of rows and columns\n# can also do nrow(Batting) and ncol(Batting)\ndim(Batting) \n\n[1] 113799     22\n\n\n\nclass(Batting)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\ntbl (pronounced tibble) is the tidyverse way of storing tabular data, like a spreadsheet or data.frame"
  },
  {
    "objectID": "lectures/01-explore.html#first-example-mlb-batting-1",
    "href": "lectures/01-explore.html#first-example-mlb-batting-1",
    "title": "Exploring data: into the tidyverse",
    "section": "First example: MLB batting",
    "text": "First example: MLB batting\n\nView the first 6 (by default) rows with head()\n\n\n# try just typing Batting into your console, what happens?\n# also try glimpse(Batting)\nhead(Batting) \n\n# A tibble: 6 × 22\n  playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 aardsda01   2004     1 SFN    NL       11     0     0     0     0     0     0\n2 aardsda01   2006     1 CHN    NL       45     2     0     0     0     0     0\n3 aardsda01   2007     1 CHA    AL       25     0     0     0     0     0     0\n4 aardsda01   2008     1 BOS    AL       47     1     0     0     0     0     0\n5 aardsda01   2009     1 SEA    AL       73     0     0     0     0     0     0\n6 aardsda01   2010     1 SEA    AL       53     0     0     0     0     0     0\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;\n\n\n\nIs the Batting dataset tidy?\n\nEach row: a player’s season stint with a team (i.e. players can play for multiple teams in year)\nEach column: different measurement or recording about the player-team-season observation (get all column names with colnames(Batting) or names(Batting))"
  },
  {
    "objectID": "lectures/01-explore.html#descriptive-statistics",
    "href": "lectures/01-explore.html#descriptive-statistics",
    "title": "Exploring data: into the tidyverse",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nSummarize quantitative (e.g. yearID, AB) and categorical (e.g. teamID, lgID) variables in different ways…\n\n\nSummary statistics for quantitative variables with the summary() function\n\n\nsummary(Batting$yearID)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1871    1939    1979    1970    2004    2023 \n\n\n\nCounts of categorical variables with the table() function\n\n\n# be careful it ignores NA values\n# can do table(Batting$lgID, useNA = \"always\")\ntable(Batting$lgID)\n\n\n   AA    AL    FL    NA    NL    PL    UA \n 1893 52599   472   737 57615   149   334"
  },
  {
    "objectID": "lectures/01-explore.html#the-dplyr-package",
    "href": "lectures/01-explore.html#the-dplyr-package",
    "title": "Exploring data: into the tidyverse",
    "section": "The dplyr package",
    "text": "The dplyr package\n\ndplyr is a package within the tidyverse with functions for data wrangling\nThe dplyr data verbs for manipulating data\n\nfilter()\nselect()\narrange()\nmutate()\ngroup_by()\nsummarize()"
  },
  {
    "objectID": "lectures/01-explore.html#filter",
    "href": "lectures/01-explore.html#filter",
    "title": "Exploring data: into the tidyverse",
    "section": "filter()",
    "text": "filter()\n\nUse filter() to extract ROWS (observations) that meet certain conditions\nNeed to specify a logical condition (aka boolean expression)"
  },
  {
    "objectID": "lectures/01-explore.html#filter-1",
    "href": "lectures/01-explore.html#filter-1",
    "title": "Exploring data: into the tidyverse",
    "section": "filter()",
    "text": "filter()\nExample: Extract batting stats for 2 leagues AL and NL only\n\nfilter(Batting, lgID %in% c(\"AL\", \"NL\")) # or filter(Batting, lgID == \"AL\" | lgID == \"NL\")\n\n# A tibble: 110,214 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 aardsda01   2004     1 SFN    NL       11     0     0     0     0     0     0\n 2 aardsda01   2006     1 CHN    NL       45     2     0     0     0     0     0\n 3 aardsda01   2007     1 CHA    AL       25     0     0     0     0     0     0\n 4 aardsda01   2008     1 BOS    AL       47     1     0     0     0     0     0\n 5 aardsda01   2009     1 SEA    AL       73     0     0     0     0     0     0\n 6 aardsda01   2010     1 SEA    AL       53     0     0     0     0     0     0\n 7 aardsda01   2012     1 NYA    AL        1     0     0     0     0     0     0\n 8 aardsda01   2013     1 NYN    NL       43     0     0     0     0     0     0\n 9 aardsda01   2015     1 ATL    NL       33     1     0     0     0     0     0\n10 aaronha01   1954     1 ML1    NL      122   468    58   131    27     6    13\n# ℹ 110,204 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#filter-2",
    "href": "lectures/01-explore.html#filter-2",
    "title": "Exploring data: into the tidyverse",
    "section": "filter()",
    "text": "filter()\nExample: Extract batting stats for Pirates players in 2022\n\n# multiple conditions\nfilter(Batting, yearID == 2022 & teamID == \"PIT\")\n\n# A tibble: 68 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 alforan01   2022     1 PIT    NL        2     4     0     1     0     0     0\n 2 alldrca01   2022     1 PIT    NL        1     0     0     0     0     0     0\n 3 allengr01   2022     1 PIT    NL       46   118    17    22     4     0     2\n 4 andujmi01   2022     2 PIT    NL        9    36     4     9     3     1     0\n 5 baeji01     2022     1 PIT    NL       10    33     5    11     3     0     0\n 6 bandaan01   2022     1 PIT    NL       23     0     0     0     0     0     0\n 7 banuema01   2022     2 PIT    NL       31     0     0     0     0     0     0\n 8 bednada01   2022     1 PIT    NL       45     0     0     0     0     0     0\n 9 beedety01   2022     2 PIT    NL       25     0     0     0     0     0     0\n10 briceau01   2022     1 PIT    NL        4     0     0     0     0     0     0\n# ℹ 58 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#logical-conditions",
    "href": "lectures/01-explore.html#logical-conditions",
    "title": "Exploring data: into the tidyverse",
    "section": "Logical conditions",
    "text": "Logical conditions\n\n\n\nx &lt; y: less than\nx &lt;= y: less than or equal to\nx == y: equal to\nx != y: not equal to\nx &gt; y: greater than\nx &gt;= y: greater than or equal to\n\n\n\nx %in% y: whether the value is present in a given vector\nis.na(x): is missing\n!is.na(x): is not missing\nx & y: and\nx | y: or\n!x: not\n\n\n… and basically anything that returns a TRUE/FALSE value"
  },
  {
    "objectID": "lectures/01-explore.html#common-mistakes",
    "href": "lectures/01-explore.html#common-mistakes",
    "title": "Exploring data: into the tidyverse",
    "section": "Common mistakes",
    "text": "Common mistakes\n\n\n\n= instead of ==\n\nnay\n\nfilter(Batting, team = \"PIT\")\n\nyay\n\nfilter(Batting, team == \"PIT\")\n\n\n\nForgetting quotes (for string/character)\n\nnay\n\nfilter(Batting, team == PIT)\n\nyay\n\nfilter(Batting, team == \"PIT\")"
  },
  {
    "objectID": "lectures/01-explore.html#select",
    "href": "lectures/01-explore.html#select",
    "title": "Exploring data: into the tidyverse",
    "section": "select()",
    "text": "select()\n\nUse select() to extract COLUMNS (variables) of interest\nJust simply specify the column names…\n\n\nselect(Batting, playerID, yearID, G, AB, R, H, HR, BB)\n\n# A tibble: 113,799 × 8\n   playerID  yearID     G    AB     R     H    HR    BB\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 aardsda01   2004    11     0     0     0     0     0\n 2 aardsda01   2006    45     2     0     0     0     0\n 3 aardsda01   2007    25     0     0     0     0     0\n 4 aardsda01   2008    47     1     0     0     0     0\n 5 aardsda01   2009    73     0     0     0     0     0\n 6 aardsda01   2010    53     0     0     0     0     0\n 7 aardsda01   2012     1     0     0     0     0     0\n 8 aardsda01   2013    43     0     0     0     0     0\n 9 aardsda01   2015    33     1     0     0     0     0\n10 aaronha01   1954   122   468    58   131    13    28\n# ℹ 113,789 more rows"
  },
  {
    "objectID": "lectures/01-explore.html#mutate",
    "href": "lectures/01-explore.html#mutate",
    "title": "Exploring data: into the tidyverse",
    "section": "mutate()",
    "text": "mutate()\n\nUse mutate() to create new variables\nNew variables created via mutate() are usually based on existing variables\n\nMake sure to give your new variable a name\nNote that naming the new variable the same as the existing variable will overwrite the original column"
  },
  {
    "objectID": "lectures/01-explore.html#mutate-1",
    "href": "lectures/01-explore.html#mutate-1",
    "title": "Exploring data: into the tidyverse",
    "section": "mutate()",
    "text": "mutate()\nExample: Get the batting average and strikeout-to-walk ratio for every player\n\nmutate(Batting, batting_avg = H / AB, so_bb_ratio = SO / BB)\n\n# A tibble: 113,799 × 24\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 aardsda01   2004     1 SFN    NL       11     0     0     0     0     0     0\n 2 aardsda01   2006     1 CHN    NL       45     2     0     0     0     0     0\n 3 aardsda01   2007     1 CHA    AL       25     0     0     0     0     0     0\n 4 aardsda01   2008     1 BOS    AL       47     1     0     0     0     0     0\n 5 aardsda01   2009     1 SEA    AL       73     0     0     0     0     0     0\n 6 aardsda01   2010     1 SEA    AL       53     0     0     0     0     0     0\n 7 aardsda01   2012     1 NYA    AL        1     0     0     0     0     0     0\n 8 aardsda01   2013     1 NYN    NL       43     0     0     0     0     0     0\n 9 aardsda01   2015     1 ATL    NL       33     1     0     0     0     0     0\n10 aaronha01   1954     1 ML1    NL      122   468    58   131    27     6    13\n# ℹ 113,789 more rows\n# ℹ 12 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;, batting_avg &lt;dbl&gt;,\n#   so_bb_ratio &lt;dbl&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#arrange",
    "href": "lectures/01-explore.html#arrange",
    "title": "Exploring data: into the tidyverse",
    "section": "arrange()",
    "text": "arrange()\n\nSort observations (rows) by variables (columns)\n\nascending order is the default (low to high for numeric columns, alphabetical order for character columns)\n\n\n\nExample: Who holds the single-season home run record?\n\narrange(Batting, desc(HR)) # desc() for descending order\n\n# A tibble: 113,799 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 bondsba01   2001     1 SFN    NL      153   476   129   156    32     2    73\n 2 mcgwima01   1998     1 SLN    NL      155   509   130   152    21     0    70\n 3 sosasa01    1998     1 CHN    NL      159   643   134   198    20     0    66\n 4 mcgwima01   1999     1 SLN    NL      153   521   118   145    21     1    65\n 5 sosasa01    2001     1 CHN    NL      160   577   146   189    34     5    64\n 6 sosasa01    1999     1 CHN    NL      162   625   114   180    24     2    63\n 7 judgeaa01   2022     1 NYA    AL      157   570   133   177    28     0    62\n 8 marisro01   1961     1 NYA    AL      161   590   132   159    16     4    61\n 9 ruthba01    1927     1 NYA    AL      151   540   158   192    29     8    60\n10 ruthba01    1921     1 NYA    AL      152   540   177   204    44    16    59\n# ℹ 113,789 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#arrange-1",
    "href": "lectures/01-explore.html#arrange-1",
    "title": "Exploring data: into the tidyverse",
    "section": "arrange()",
    "text": "arrange()\nExample: arrange by multiple columns — at bats from high to low (first sort), then home runs from low to high (second sort) — variable order matters\n\narrange(Batting, desc(AB), HR)\n\n# A tibble: 113,799 × 22\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 rolliji01   2007     1 PHI    NL      162   716   139   212    38    20    30\n 2 wilsowi02   1980     1 KCA    AL      161   705   133   230    28    15     3\n 3 suzukic01   2004     1 SEA    AL      161   704   101   262    24     5     8\n 4 samueju01   1984     1 PHI    NL      160   701   105   191    36    19    15\n 5 pierrju01   2006     1 CHN    NL      162   699    87   204    32    13     3\n 6 cashda01    1975     1 PHI    NL      162   699   111   213    40     3     4\n 7 alouma01    1969     1 PIT    NL      162   698   105   231    41     6     1\n 8 reyesjo01   2005     1 NYN    NL      161   696    99   190    24    17     7\n 9 jensewo01   1936     1 PIT    NL      153   696    98   197    34    10    10\n10 soriaal01   2002     1 NYA    AL      156   696   128   209    51     2    39\n# ℹ 113,789 more rows\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#performing-multiple-operations",
    "href": "lectures/01-explore.html#performing-multiple-operations",
    "title": "Exploring data: into the tidyverse",
    "section": "Performing multiple operations",
    "text": "Performing multiple operations\n\nWhat if we want to perform several different tasks using multiple dplyr verbs?\n\n\n\nIntroducing the pipe operator |&gt;\n\n\n\n\nYou might have seen the magrittr pipe %&gt;%…\n\n…from the maggritr package, automatically loaded when loading tidyverse\n\nRecently, many people (including Hadley Wickham) have switched to |&gt;, the built-in “native” pipe to base R\n\nWhat are the differences?\n\n\n\n\n\nShortcut for the pipe operator in RStudio: Command (or Ctrl) + Shift + M"
  },
  {
    "objectID": "lectures/01-explore.html#the-pipe-operator",
    "href": "lectures/01-explore.html#the-pipe-operator",
    "title": "Exploring data: into the tidyverse",
    "section": "The pipe operator",
    "text": "The pipe operator\n\nUse |&gt; to perform a sequence of operations\nThe pipe takes an object (e.g., tibble, data frame, matrix, vector, etc.) on the left and passes it as the first argument of the function on the right\n\n\n# the workflow\nobject |&gt;\n  first_operation(...) |&gt;\n  second_operation(...) |&gt; \n  .\n  .\n  .\n  last_operation(...)"
  },
  {
    "objectID": "lectures/01-explore.html#performing-multiple-operations-1",
    "href": "lectures/01-explore.html#performing-multiple-operations-1",
    "title": "Exploring data: into the tidyverse",
    "section": "Performing multiple operations",
    "text": "Performing multiple operations\nExample: Which Pirates players had the highest batting average in 2022, among those with at least 50 at bats?\n\nWhat are the tasks to be done here?\n\n\n\nfilter(): only Pirates players in 2022 with at least 50 at bats\nmutate(): create a new column for batting average\narrange(): sort by batting average in descending order\nselect(): report player name, at bats, and batting average"
  },
  {
    "objectID": "lectures/01-explore.html#performing-multiple-operations-2",
    "href": "lectures/01-explore.html#performing-multiple-operations-2",
    "title": "Exploring data: into the tidyverse",
    "section": "Performing multiple operations",
    "text": "Performing multiple operations\n\nBatting |&gt; \n  filter(yearID == 2022, teamID == \"PIT\", AB &gt;= 50) |&gt; \n  mutate(batting_avg = H / AB) |&gt; \n  arrange(desc(batting_avg)) |&gt; \n  select(playerID, AB, batting_avg)\n\n# A tibble: 23 × 3\n   playerID     AB batting_avg\n   &lt;chr&gt;     &lt;int&gt;       &lt;dbl&gt;\n 1 newmake01   288       0.274\n 2 reynobr01   542       0.262\n 3 hayeske01   505       0.244\n 4 marisja01    77       0.234\n 5 perezro02    60       0.233\n 6 castrro01   253       0.233\n 7 cruzon01    331       0.233\n 8 gamelbe01   371       0.232\n 9 chavimi01   401       0.229\n10 vogelda01   237       0.228\n# ℹ 13 more rows\n\n\n\nWithout the pipe, the code looks every ugly with functions nested within functions…\n\nselect(arrange(mutate(filter(Batting, yearID == 2022, teamID == \"PIT\", AB &gt;= 50), batting_avg = H / AB), \ndesc(batting_avg)), playerID, AB, batting_avg)"
  },
  {
    "objectID": "lectures/01-explore.html#summarize-by-itself",
    "href": "lectures/01-explore.html#summarize-by-itself",
    "title": "Exploring data: into the tidyverse",
    "section": "summarize() (by itself)",
    "text": "summarize() (by itself)\n\nUse summarize() to collapse the data down to a single row (per group) by aggregating variables into single values\nUseful for computing summaries (e.g., mean, median, max, min, correlation, etc.)\n\n\nBatting |&gt; \n  summarize(median_at_bats = median(AB))\n\n# A tibble: 1 × 1\n  median_at_bats\n           &lt;int&gt;\n1             44\n\nBatting |&gt; \n  summarize(cor_ab_hr = cor(AB, HR))\n\n# A tibble: 1 × 1\n  cor_ab_hr\n      &lt;dbl&gt;\n1     0.706"
  },
  {
    "objectID": "lectures/01-explore.html#group_by-and-summarize",
    "href": "lectures/01-explore.html#group_by-and-summarize",
    "title": "Exploring data: into the tidyverse",
    "section": "group_by() and summarize()",
    "text": "group_by() and summarize()\n\ngroup_by() converts the data into a “grouped tbl” where operations are performed by group\n\ni.e., it splits the data into groups based on values in a column\n\ngroup_by() becomes powerful when combining with summarize()\nAfter the operation at the group-level is done, use ungroup() to remove grouping"
  },
  {
    "objectID": "lectures/01-explore.html#group_by-and-summarize-1",
    "href": "lectures/01-explore.html#group_by-and-summarize-1",
    "title": "Exploring data: into the tidyverse",
    "section": "group_by() and summarize()",
    "text": "group_by() and summarize()\nExample: How many home runs, strike outs, and walks did each team accumulate in each season from 2015 to 2019?\n\nBatting |&gt; \n  filter(yearID %in% 2015:2019) |&gt; \n  group_by(teamID) |&gt; \n  summarize(total_hr = sum(HR), total_so = sum(SO), total_bb = sum(BB)) |&gt; \n  arrange(desc(total_hr))\n\n# A tibble: 30 × 4\n   teamID total_hr total_so total_bb\n   &lt;fct&gt;     &lt;int&gt;    &lt;int&gt;    &lt;int&gt;\n 1 NYA        1209     6659     2839\n 2 HOU        1159     6294     2759\n 3 TOR        1139     6741     2752\n 4 LAN        1111     6751     2991\n 5 BAL        1103     6914     2162\n 6 TEX        1041     7008     2572\n 7 SEA        1036     6693     2489\n 8 MIN        1035     6694     2604\n 9 OAK        1033     6474     2610\n10 MIL        1031     7434     2724\n# ℹ 20 more rows"
  },
  {
    "objectID": "lectures/01-explore.html#count",
    "href": "lectures/01-explore.html#count",
    "title": "Exploring data: into the tidyverse",
    "section": "count()",
    "text": "count()\n\n\ncount() returns the number of observations in each group\n\nBatting |&gt; \n  count(lgID, name = \"freq\")\n\n# A tibble: 7 × 2\n  lgID   freq\n  &lt;fct&gt; &lt;int&gt;\n1 AA     1893\n2 AL    52599\n3 FL      472\n4 NA      737\n5 NL    57615\n6 PL      149\n7 UA      334\n\n\n\n# recall that in base R...\ntable(Batting$lgID)\n\n\n   AA    AL    FL    NA    NL    PL    UA \n 1893 52599   472   737 57615   149   334 \n\n\n\nThis can also be done with group_by() and summarize()\n\n# note: count is a \"shortcut\" of this\nBatting |&gt; \n  group_by(lgID) |&gt; \n  summarize(freq = n()) |&gt; \n  ungroup()\n\n# A tibble: 7 × 2\n  lgID   freq\n  &lt;fct&gt; &lt;int&gt;\n1 AA     1893\n2 AL    52599\n3 FL      472\n4 NA      737\n5 NL    57615\n6 PL      149\n7 UA      334"
  },
  {
    "objectID": "lectures/01-explore.html#slice_-family-for-subsetting-rows",
    "href": "lectures/01-explore.html#slice_-family-for-subsetting-rows",
    "title": "Exploring data: into the tidyverse",
    "section": "slice_*() family for subsetting rows",
    "text": "slice_*() family for subsetting rows\n\nslice(): extract rows (observations) based on the row index\n\n\nBatting |&gt; \n  slice(c(1, 99, 101, 500))\n\n# A tibble: 4 × 22\n  playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 aardsda01   2004     1 SFN    NL       11     0     0     0     0     0     0\n2 abbotgl01   1973     1 OAK    AL        5     0     0     0     0     0     0\n3 abbotgl01   1975     1 OAK    AL       30     0     0     0     0     0     0\n4 adamshe01   1949     1 CHA    AL       56   208    26    61     5     3     0\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#slice_-family-for-subsetting-rows-1",
    "href": "lectures/01-explore.html#slice_-family-for-subsetting-rows-1",
    "title": "Exploring data: into the tidyverse",
    "section": "slice_*() family for subsetting rows",
    "text": "slice_*() family for subsetting rows\n\nslice_head() / slice_tail(): extract the first / last n rows\n\n\n# Batting |&gt; slice_tail(n = 5)\nBatting |&gt; \n  slice_head(n = 5)\n\n# A tibble: 5 × 22\n  playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 aardsda01   2004     1 SFN    NL       11     0     0     0     0     0     0\n2 aardsda01   2006     1 CHN    NL       45     2     0     0     0     0     0\n3 aardsda01   2007     1 CHA    AL       25     0     0     0     0     0     0\n4 aardsda01   2008     1 BOS    AL       47     1     0     0     0     0     0\n5 aardsda01   2009     1 SEA    AL       73     0     0     0     0     0     0\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#slice_-family-for-subsetting-rows-2",
    "href": "lectures/01-explore.html#slice_-family-for-subsetting-rows-2",
    "title": "Exploring data: into the tidyverse",
    "section": "slice_*() family for subsetting rows",
    "text": "slice_*() family for subsetting rows\n\nslice_min() / slice_max(): extract rows with the smallest or largest values of a variable\n\n\n# single-season home run record (top 5)\nBatting |&gt; \n  slice_max(HR, n = 5)\n\n# A tibble: 5 × 22\n  playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 bondsba01   2001     1 SFN    NL      153   476   129   156    32     2    73\n2 mcgwima01   1998     1 SLN    NL      155   509   130   152    21     0    70\n3 sosasa01    1998     1 CHN    NL      159   643   134   198    20     0    66\n4 mcgwima01   1999     1 SLN    NL      153   521   118   145    21     1    65\n5 sosasa01    2001     1 CHN    NL      160   577   146   189    34     5    64\n# ℹ 10 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-explore.html#slice_-family-for-subsetting-rows-3",
    "href": "lectures/01-explore.html#slice_-family-for-subsetting-rows-3",
    "title": "Exploring data: into the tidyverse",
    "section": "slice_*() family for subsetting rows",
    "text": "slice_*() family for subsetting rows\n\nslice_sample(): randomly sample a specified number / fraction of observation in the data\n\nUseful for performing resampling (e.g., bootstrap, cross-validation, etc.)\n\n# randomly sample 1000 rows (without replacement, by default)\nBatting |&gt; \n  slice_sample(n = 1000)\n\n# randomly sample 70% of the rows, with replacement\nBatting |&gt; \n  slice_sample(prop = 0.7, replace = TRUE)"
  },
  {
    "objectID": "lectures/01-explore.html#putting-it-all-together",
    "href": "lectures/01-explore.html#putting-it-all-together",
    "title": "Exploring data: into the tidyverse",
    "section": "Putting it all together",
    "text": "Putting it all together\nExample: Get batting stats for each year: each row is a year with the following variables\n\ntotal hits, home runs, strikeouts, walks, atbats\ntotal batting average for each year = total H / total AB\nonly keeps AL and NL leagues\n\n\n\nyearly_batting &lt;- Batting |&gt;\n  filter(lgID %in% c(\"AL\", \"NL\")) |&gt;\n  group_by(yearID) |&gt;\n  summarize(total_h = sum(H, na.rm = TRUE),\n            total_hr = sum(HR, na.rm = TRUE),\n            total_so = sum(SO, na.rm = TRUE),\n            total_bb = sum(BB, na.rm = TRUE),\n            total_ab = sum(AB, na.rm = TRUE)) |&gt;\n  mutate(batting_avg = total_h / total_ab)"
  },
  {
    "objectID": "lectures/01-explore.html#putting-it-all-together-1",
    "href": "lectures/01-explore.html#putting-it-all-together-1",
    "title": "Exploring data: into the tidyverse",
    "section": "Putting it all together",
    "text": "Putting it all together\nWhat are the top three years with the most HRs?\n\n\nyearly_batting |&gt; \n  slice_max(total_hr, n = 3)\n\n# A tibble: 3 × 7\n  yearID total_h total_hr total_so total_bb total_ab batting_avg\n   &lt;int&gt;   &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;       &lt;dbl&gt;\n1   2019   42039     6776    42823    15895   166651       0.252\n2   2017   42215     6105    40104    15829   165567       0.255\n3   2021   39484     5944    42145    15794   161941       0.244\n\n\n\n# or this \nyearly_batting |&gt;\n  arrange(desc(total_hr)) |&gt;\n  slice(1:3)\n\n# A tibble: 3 × 7\n  yearID total_h total_hr total_so total_bb total_ab batting_avg\n   &lt;int&gt;   &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;       &lt;dbl&gt;\n1   2019   42039     6776    42823    15895   166651       0.252\n2   2017   42215     6105    40104    15829   165567       0.255\n3   2021   39484     5944    42145    15794   161941       0.244"
  },
  {
    "objectID": "lectures/01-explore.html#putting-it-all-together-2",
    "href": "lectures/01-explore.html#putting-it-all-together-2",
    "title": "Exploring data: into the tidyverse",
    "section": "Putting it all together",
    "text": "Putting it all together\nWhich years have the best and worst strikeout to walk ratios?\n\n\nyearly_batting |&gt;\n  mutate(so_bb_ratio = total_so / total_bb) |&gt;\n  arrange(so_bb_ratio) |&gt;\n  slice(c(1, n()))\n\n# A tibble: 2 × 8\n  yearID total_h total_hr total_so total_bb total_ab batting_avg so_bb_ratio\n   &lt;int&gt;   &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1   1893   15913      460     3341     6143    56898       0.280       0.544\n2   1879    6171       58     1843      508    24155       0.255       3.63"
  },
  {
    "objectID": "lectures/01-explore.html#whats-next",
    "href": "lectures/01-explore.html#whats-next",
    "title": "Exploring data: into the tidyverse",
    "section": "What’s next?",
    "text": "What’s next?\n\n\nDATA VISUALIZATION\n\nThe simple graph has brought more information to the data analyst’s mind than any other device. — John Tukey\n\n\nUse ggplot2 (and the grammar of graphics) to visually explore data\nMore intuitive than base R plotting\nDifferent types of visualizations for categorical and quantitative data, faceting, etc.\ndplyr verbs and |&gt; leads to natural pipeline for EDA"
  },
  {
    "objectID": "lectures/01-explore.html#check-out-this-song",
    "href": "lectures/01-explore.html#check-out-this-song",
    "title": "Exploring data: into the tidyverse",
    "section": "Check out this song",
    "text": "Check out this song"
  },
  {
    "objectID": "lectures/08-density.html#data-caitlin-clarks-shots",
    "href": "lectures/08-density.html#data-caitlin-clarks-shots",
    "title": "Data visualization: density estimation",
    "section": "Data: Caitlin Clark’s shots",
    "text": "Data: Caitlin Clark’s shots\nShot attempts by the Caitlin Clark in the 2024 WNBA season using wehoop\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nclark_shots &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/clark_shots.csv\")\nglimpse(clark_shots)\n\nRows: 658\nColumns: 6\n$ scoring_play  &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRU…\n$ score_value   &lt;dbl&gt; 3, 0, 0, 3, 0, 0, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 2, 0, 0,…\n$ shot_x        &lt;dbl&gt; -17, -1, -14, -17, -18, -1, 5, 6, 1, -16, -19, 0, 2, -7,…\n$ shot_y        &lt;dbl&gt; 22, 33, 16, 19, 20, 0, 6, 30, 1, 21, 13, 4, 1, 32, 1, 14…\n$ shot_distance &lt;dbl&gt; 27.802878, 33.015148, 21.260292, 25.495098, 26.907248, 1…\n$ shot_type     &lt;chr&gt; \"Running Pullup Jump Shot\", \"Pullup Jump Shot\", \"Step Ba…\n\n\n\n\nEach row is a shot attempt by Caitlin Clark in the 2024 WNBA season\nCategorical / qualitative variables: scoring_play, shot_type\nContinuous / quantitative variables: shot_x, shot_y, shot_distance, score_value"
  },
  {
    "objectID": "lectures/08-density.html#revisiting-histograms",
    "href": "lectures/08-density.html#revisiting-histograms",
    "title": "Data visualization: density estimation",
    "section": "Revisiting histograms",
    "text": "Revisiting histograms\n\n\nfd_bw &lt;- 2 * IQR(clark_shots$shot_distance) / length(clark_shots$shot_distance)^(1/3)\nclark_shots |&gt;\n  ggplot(aes(x = shot_distance)) +\n  geom_histogram()\n\n\n\n\nSplit observed data into bins\nCount number of observations in each bin\n\nNeed to choose the number of bins, adjust with:\n\nbins: number of bins (default is 30)\nbinwidth: width of bins (overrides bins), various rules of thumb\nbreaks: vector of bin boundaries (overrides both bins and binwidth)"
  },
  {
    "objectID": "lectures/08-density.html#adjusting-the-binwidth",
    "href": "lectures/08-density.html#adjusting-the-binwidth",
    "title": "Data visualization: density estimation",
    "section": "Adjusting the binwidth",
    "text": "Adjusting the binwidth\n\n\nSmall binwidth \\(\\rightarrow\\) “undersmooth” / spiky\n\nclark_shots |&gt;\n  ggplot(aes(x = shot_distance)) +\n  geom_histogram(binwidth = 0.5)\n\n\n\n\n\n\n\n\n\nLarge binwidth \\(\\rightarrow\\) “oversmooth” / flat\n\nclark_shots |&gt;\n  ggplot(aes(x = shot_distance)) +\n  geom_histogram(binwidth = 5)"
  },
  {
    "objectID": "lectures/08-density.html#adjusting-the-binwidth-1",
    "href": "lectures/08-density.html#adjusting-the-binwidth-1",
    "title": "Data visualization: density estimation",
    "section": "Adjusting the binwidth",
    "text": "Adjusting the binwidth\n\nA binwidth that is too narrow shows too much detail\n\ntoo many bins: low bias, high variance\n\nA binwidth that is too wide hides detail\n\ntoo few bins: high bias, low variance\n\nAlways pick a value that is “just right” (The Goldilocks principle)\n\nTry several values, the R / ggplot2 default is NOT guaranteed to be an optimal choice"
  },
  {
    "objectID": "lectures/08-density.html#a-subtle-point-about-the-histogram-code",
    "href": "lectures/08-density.html#a-subtle-point-about-the-histogram-code",
    "title": "Data visualization: density estimation",
    "section": "A subtle point about the histogram code…",
    "text": "A subtle point about the histogram code…\n\n\nBy default the bins are centered on the integers\n\nleft-closed, right-open intervals\nstarting at -0.5 to 0.5, 0.5 to 1.5, …\n\n\nclark_shots |&gt;\n  ggplot(aes(x = shot_distance)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n\nSpecify center of one bin (e.g. 0.5)\n\nUse closed = \"left\"\n\n\nclark_shots |&gt;\n  ggplot(aes(x = shot_distance)) +\n  geom_histogram(binwidth = 1, center = 0.5, \n                 closed = \"left\")"
  },
  {
    "objectID": "lectures/08-density.html#how-do-histograms-relate-to-the-pdf-and-cdf",
    "href": "lectures/08-density.html#how-do-histograms-relate-to-the-pdf-and-cdf",
    "title": "Data visualization: density estimation",
    "section": "How do histograms relate to the PDF and CDF?",
    "text": "How do histograms relate to the PDF and CDF?\n\nHistograms approximate the PDF with bins, and points are equally likely within a bin\nPDF is the derivative of the cumulative distribution function (CDF)"
  },
  {
    "objectID": "lectures/08-density.html#kernel-density-estimation",
    "href": "lectures/08-density.html#kernel-density-estimation",
    "title": "Data visualization: density estimation",
    "section": "Kernel density estimation",
    "text": "Kernel density estimation\nGoal: estimate the PDF \\(f(x)\\) for all possible values (assuming it is smooth)\n\nThe kernel density estimator (KDE) is \\(\\displaystyle \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\\)\n\n\n\n\\(n\\): sample size\n\\(x\\): new point to estimate \\(f(x)\\) (does NOT have to be in the dataset!)\n\\(h\\): bandwidth, analogous to histogram binwidth, ensures \\(\\hat{f}(x)\\) integrates to 1\n\\(x_i\\): \\(i\\)th observation in the dataset\n\n\n\n\n\\(K_h(x - x_i)\\): kernel function, creates weight given distance of \\(i\\)th observation from new point\n\nas \\(|x - x_i| \\rightarrow \\infty\\) then \\(K_h(x - x_i) \\rightarrow 0\\), i.e. the further apart the \\(i\\)th observation is from \\(x\\), the smaller the weight\nas bandwidth \\(h\\) increases, weights are more evenly spread out\nChoice of kernel functions: Gaussian/normal, etc.\n\\(K_h(x - x_i)\\) is large when \\(x_i\\) is close to \\(x\\)"
  },
  {
    "objectID": "lectures/08-density.html#kernel-density-estimation-1",
    "href": "lectures/08-density.html#kernel-density-estimation-1",
    "title": "Data visualization: density estimation",
    "section": "Kernel density estimation",
    "text": "Kernel density estimation\nIntuition:\n\nsmooth each data point into a small density bumps\nsum all these small bumps together to obtain the final density estimate"
  },
  {
    "objectID": "lectures/08-density.html#how-do-we-compute-and-display-the-density-estimate",
    "href": "lectures/08-density.html#how-do-we-compute-and-display-the-density-estimate",
    "title": "Data visualization: density estimation",
    "section": "How do we compute and display the density estimate?",
    "text": "How do we compute and display the density estimate?\n\n\n\nWe make kernel density estimates with geom_density()\n\n\nclark_shots |&gt;\n  ggplot(aes(x = shot_distance)) + \n  geom_density() +\n  geom_rug(alpha = 0.3)\n\n\nPros:\n\nDisplays full shape of distribution\nCan easily layer\nAdd categorical variable with color\n\nCons:\n\nNeed to pick bandwidth and kernel…"
  },
  {
    "objectID": "lectures/08-density.html#what-about-the-bandwidth",
    "href": "lectures/08-density.html#what-about-the-bandwidth",
    "title": "Data visualization: density estimation",
    "section": "What about the bandwidth?",
    "text": "What about the bandwidth?\nUse Gaussian reference rule (rule-of-thumb) \\(\\approx 1.06 \\cdot \\sigma \\cdot n^{-1/5}\\) (\\(\\sigma\\): observed standard deviation)\nModify the bandwidth using the adjust argument - value to multiply default bandwidth by\n\n\n\nclark_shots |&gt;\n  ggplot(aes(x = shot_distance)) + \n  geom_density(adjust = 0.5) +\n  geom_rug(alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\nclark_shots |&gt;\n  ggplot(aes(x = shot_distance)) + \n  geom_density(adjust = 2) +\n  geom_rug(alpha = 0.3)"
  },
  {
    "objectID": "lectures/08-density.html#notes-on-density-estimation",
    "href": "lectures/08-density.html#notes-on-density-estimation",
    "title": "Data visualization: density estimation",
    "section": "Notes on density estimation",
    "text": "Notes on density estimation\n\nIn KDE, the bandwidth parameter is analogous to the binwidth in histograms.\nIf the bandwidth is too small, the density estimate can become overly peaky and the main trends in the data may be obscured.\nIf the bandwidth is too large, then smaller features in the distribution of the data may disappear\nThe choice of the kernel can affect the shape of the density curve.\n\nA Gaussian kernel typically gives density estimates that look bell-shaped (ish)\nA rectangular kernel can generate the appearance of steps in the density curve\nKernel choice matters less with more data points\n\n\nDensity plots are often reliable and informative for large datasets but can be misleading for smaller ones."
  },
  {
    "objectID": "lectures/08-density.html#common-pitfall-bounded-data",
    "href": "lectures/08-density.html#common-pitfall-bounded-data",
    "title": "Data visualization: density estimation",
    "section": "Common pitfall: bounded data",
    "text": "Common pitfall: bounded data\n\n\n\nset.seed(36)\nbounded_data &lt;- tibble(x = runif(100))\nbounded_data |&gt; \n  ggplot(aes(x)) +\n  geom_density() +\n  geom_rug(alpha = 0.5) +\n  stat_function(data = tibble(x = c(0, 1)),\n                fun = dunif, color = \"red\") +\n  scale_x_continuous(limits = c(-0.5, 1.5))\n\n\nObserve density estimates for impossible values (in the tails) - ALWAYS be mindful\nReflection method: first perform standard KDE, then “reflect” tails outside of desired interval to be inside\nSee also: evmix package"
  },
  {
    "objectID": "lectures/08-density.html#use-density-curves-and-ecdfs-together",
    "href": "lectures/08-density.html#use-density-curves-and-ecdfs-together",
    "title": "Data visualization: density estimation",
    "section": "Use density curves and ECDFs together",
    "text": "Use density curves and ECDFs together"
  },
  {
    "objectID": "lectures/08-density.html#code-interlude-easy-way-to-arrange-multiple-figures",
    "href": "lectures/08-density.html#code-interlude-easy-way-to-arrange-multiple-figures",
    "title": "Data visualization: density estimation",
    "section": "Code interlude: easy way to arrange multiple figures",
    "text": "Code interlude: easy way to arrange multiple figures\nUse the cowplot package to easily arrange your plots (see also patchwork)\n\nlibrary(cowplot)\n\nclark_shot_dens &lt;- clark_shots |&gt;\n  ggplot(aes(x = shot_distance)) + \n  geom_density() +\n  geom_rug(alpha = 0.3) +\n  theme_bw() +\n  labs(x = \"Shot distance (in feet)\",\n       y = \"Number of shot attempts\")\n\nclark_shot_ecdf &lt;- clark_shots |&gt;\n  ggplot(aes(x = shot_distance)) + \n  stat_ecdf() +\n  geom_rug(alpha = 0.3) +\n  theme_bw() +\n  labs(x = \"Shot distance (in feet)\",\n       y = \"Proportion of shot attempts\")\n\n# library(patchwork)\n# clark_shot_dens + clark_shot_ecdf\nplot_grid(clark_shot_dens, clark_shot_ecdf)"
  },
  {
    "objectID": "lectures/08-density.html#use-density-curves-and-ecdfs-together-1",
    "href": "lectures/08-density.html#use-density-curves-and-ecdfs-together-1",
    "title": "Data visualization: density estimation",
    "section": "Use density curves and ECDFs together",
    "text": "Use density curves and ECDFs together"
  },
  {
    "objectID": "lectures/08-density.html#another-code-interlude-collect-the-legends-with-patchwork",
    "href": "lectures/08-density.html#another-code-interlude-collect-the-legends-with-patchwork",
    "title": "Data visualization: density estimation",
    "section": "Another code interlude: collect the legends with patchwork",
    "text": "Another code interlude: collect the legends with patchwork\n\nclark_shot_dens_made &lt;- clark_shots |&gt;\n  ggplot(aes(x = shot_distance, \n             color = scoring_play)) + \n  geom_density() +\n  geom_rug(alpha = 0.3) +\n  labs(x = \"Shot distance (in feet)\",\n       y = \"Number of shot attempts\")\n\nclark_shot_ecdf_made &lt;- clark_shots |&gt;\n  ggplot(aes(x = shot_distance,\n             color = scoring_play)) + \n  stat_ecdf() +\n  geom_rug(alpha = 0.3) +\n  labs(x = \"Shot distance (in feet)\",\n       y = \"Proportion of shot attempts\")\n\nlibrary(patchwork)\nclark_shot_dens_made + clark_shot_ecdf_made + plot_layout(guides = \"collect\")"
  },
  {
    "objectID": "lectures/08-density.html#ridgeline-plots",
    "href": "lectures/08-density.html#ridgeline-plots",
    "title": "Data visualization: density estimation",
    "section": "Ridgeline plots",
    "text": "Ridgeline plots\n\n\n\nCheck out the ggridges package for a variety of customization options\n\n\nlibrary(ggridges)\nclark_shots |&gt;\n  ggplot(aes(x = shot_distance, y = shot_type)) + \n  geom_density_ridges(rel_min_height = 0.01) \n\n\nUseful to display conditional distributions across many levels"
  },
  {
    "objectID": "lectures/08-density.html#going-from-1d-to-2d-density-estimation",
    "href": "lectures/08-density.html#going-from-1d-to-2d-density-estimation",
    "title": "Data visualization: density estimation",
    "section": "Going from 1D to 2D density estimation",
    "text": "Going from 1D to 2D density estimation\nIn 1D: estimate density \\(f(x)\\), assuming that \\(f(x)\\) is smooth:\n\\[\n\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\nIn 2D: estimate joint density \\(f(x_1, x_2)\\)\n\\[\\hat{f}(x_1, x_2) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h_1h_2} K\\left(\\frac{x_1 - x_{i1}}{h_1}\\right) K\\left(\\frac{x_2 - x_{i2}}{h_2}\\right)\\]\n\n\nIn 1D there’s one bandwidth, now we have two bandwidths\n\n\\(h_1\\): controls smoothness as \\(X_1\\) changes, holding \\(X_2\\) fixed\n\\(h_2\\): controls smoothness as \\(X_2\\) changes, holding \\(X_1\\) fixed\n\nGaussian kernels are still a popular choice"
  },
  {
    "objectID": "lectures/08-density.html#display-densities-for-2d-data",
    "href": "lectures/08-density.html#display-densities-for-2d-data",
    "title": "Data visualization: density estimation",
    "section": "Display densities for 2D data",
    "text": "Display densities for 2D data"
  },
  {
    "objectID": "lectures/08-density.html#how-to-read-contour-plots",
    "href": "lectures/08-density.html#how-to-read-contour-plots",
    "title": "Data visualization: density estimation",
    "section": "How to read contour plots",
    "text": "How to read contour plots\nBest known in topography: outlines (contours) denote levels of elevation"
  },
  {
    "objectID": "lectures/08-density.html#d-density-estimation",
    "href": "lectures/08-density.html#d-density-estimation",
    "title": "Data visualization: density estimation",
    "section": "2D density estimation",
    "text": "2D density estimation\nWe can visualize all of the shot locations: (shot_x, shot_y)\n\n\n\nclark_shots |&gt;\n  ggplot(aes(x = shot_x, y = shot_y)) +\n  geom_point(size = 4, alpha = 0.3)\n\n\nAdjust transparency with alpha for overlapping points"
  },
  {
    "objectID": "lectures/08-density.html#create-contours-of-2d-kernel-density-estimate-kde",
    "href": "lectures/08-density.html#create-contours-of-2d-kernel-density-estimate-kde",
    "title": "Data visualization: density estimation",
    "section": "Create contours of 2D kernel density estimate (KDE)",
    "text": "Create contours of 2D kernel density estimate (KDE)\n\n\n\nUse geom_density2d() to display contour lines\n\n\nclark_shots |&gt;\n  filter(shot_y &lt; 35) |&gt; # remove outliers\n  ggplot(aes(x = shot_x, y = shot_y)) + \n  geom_point(size = 4, alpha = 0.3) + \n  geom_density2d() +\n  coord_fixed() +\n  theme(legend.position = \"bottom\")\n\n\nExtend KDE for joint density estimates in 2D (see section 14.4.2 for details)\nInner lines denote “peaks”\ncoord_fixed() forced a fixed ratio"
  },
  {
    "objectID": "lectures/08-density.html#create-contours-of-2d-kernel-density-estimate-kde-1",
    "href": "lectures/08-density.html#create-contours-of-2d-kernel-density-estimate-kde-1",
    "title": "Data visualization: density estimation",
    "section": "Create contours of 2D kernel density estimate (KDE)",
    "text": "Create contours of 2D kernel density estimate (KDE)\n\n\n\nWe make 2D KDE contour plots using geom_density2d()\n\n\nclark_shots |&gt;\n  filter(shot_y &lt; 35) |&gt; \n  ggplot(aes(x = shot_x, y = shot_y)) + \n  geom_point(size = 4, alpha = 0.3) + \n  geom_density2d(adjust = 0.1) +\n  coord_fixed() +\n  theme(legend.position = \"bottom\")\n\n\nCan use adjust to modify the multivariate bandwidth"
  },
  {
    "objectID": "lectures/08-density.html#contours-are-difficult-lets-make-a-heatmap-instead",
    "href": "lectures/08-density.html#contours-are-difficult-lets-make-a-heatmap-instead",
    "title": "Data visualization: density estimation",
    "section": "Contours are difficult… let’s make a heatmap instead",
    "text": "Contours are difficult… let’s make a heatmap instead\n\n\n\nUse stat_density_2d() and the after_stat() function to make 2D KDE heatmaps\nMay be easier to read than nested lines with color\nDefault color scale is awful. Always change it.\n\n\nclark_shots |&gt;\n  filter(shot_y &lt; 35) |&gt; \n  ggplot(aes(x = shot_x, y = shot_y)) + \n  stat_density2d(aes(fill = after_stat(level)),\n                 h = 0.6, bins = 60, geom = \"polygon\") +\n  scale_fill_gradient(low = \"midnightblue\", \n                      high = \"gold\") +\n  coord_fixed() +\n  theme(legend.position = \"bottom\")\n\nMultivariate density estimation can be difficult"
  },
  {
    "objectID": "lectures/08-density.html#turn-off-contours-and-use-tiles-instead",
    "href": "lectures/08-density.html#turn-off-contours-and-use-tiles-instead",
    "title": "Data visualization: density estimation",
    "section": "Turn off contours and use tiles instead",
    "text": "Turn off contours and use tiles instead\n\n\n\nDivide the space into a grid and color the grid according to high/low values\n\n\nclark_shots |&gt;\n  filter(shot_y &lt; 35) |&gt; \n  ggplot(aes(x = shot_x, y = shot_y)) + \n  stat_density2d(aes(fill = after_stat(density)),\n                 h = 0.6, bins = 60, contour = FALSE,\n                 geom = \"raster\") +\n  # scale_fill_gradient(low = \"white\", high = \"red\") +\n  scale_fill_gradient(low = \"midnightblue\", \n                      high = \"gold\") +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/08-density.html#best-alternative-hexagonal-binning",
    "href": "lectures/08-density.html#best-alternative-hexagonal-binning",
    "title": "Data visualization: density estimation",
    "section": "Best alternative? Hexagonal binning",
    "text": "Best alternative? Hexagonal binning\n\n\n\nUse geom_hex() to make hexagonal heatmaps\nNeed to have the hexbin package installed\n2D version of histogram\n\n\nclark_shots |&gt;\n  filter(shot_y &lt; 35) |&gt;\n  ggplot(aes(x = shot_x, y = shot_y)) + \n  geom_hex(binwidth = c(1, 1)) +\n  scale_fill_gradient(low = \"midnightblue\", \n                      high = \"gold\") + \n  theme(legend.position = \"bottom\") +\n  coord_fixed()\n\n\nCan specify binwidth in both directions\nAvoids limitations from smoothing"
  },
  {
    "objectID": "lectures/08-density.html#what-about-shooting-efficiency",
    "href": "lectures/08-density.html#what-about-shooting-efficiency",
    "title": "Data visualization: density estimation",
    "section": "What about shooting efficiency?",
    "text": "What about shooting efficiency?\n\nCan compute a function of another variable inside hexagons with stat_summary_hex()\n\n\n\n\nclark_shots |&gt;\n  filter(shot_y &lt; 35) |&gt;\n  ggplot(aes(x = shot_x, y = shot_y, \n             z = scoring_play, group = -1)) +\n  stat_summary_hex(binwidth = c(2, 2), fun = mean, \n                   color = \"black\") +\n  scale_fill_gradient(low = \"midnightblue\", \n                      high = \"gold\") + \n  theme(legend.position = \"bottom\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/08-density.html#appendix-making-shot-charts-and-drawing-courts-with-sportyr",
    "href": "lectures/08-density.html#appendix-making-shot-charts-and-drawing-courts-with-sportyr",
    "title": "Data visualization: density estimation",
    "section": "Appendix: Making shot charts and drawing courts with sportyR",
    "text": "Appendix: Making shot charts and drawing courts with sportyR\n\nlibrary(sportyR)\nwnba_court &lt;- geom_basketball(\"wnba\", display_range = \"offense\", rotation = 270, x_trans = -41.5)\nwnba_court +\n  geom_hex(data = clark_shots, aes(x = shot_x, y = shot_y), binwidth = c(1, 1)) + \n  scale_fill_gradient(low = \"midnightblue\", high = \"gold\")"
  },
  {
    "objectID": "lectures/08-density.html#appendix-code-to-build-dataset",
    "href": "lectures/08-density.html#appendix-code-to-build-dataset",
    "title": "Data visualization: density estimation",
    "section": "Appendix: Code to build dataset",
    "text": "Appendix: Code to build dataset\n\n# install.packages(\"wehoop\")\nlibrary(wehoop)\nwnba_pbp &lt;- load_wnba_pbp(2024)\nclark_shots &lt;- wnba_pbp |&gt; \n  filter(shooting_play) |&gt; \n  filter(str_detect(text, \"Caitlin Clark\")) |&gt; \n  filter(!str_detect(text, \"Caitlin Clark assists\")) |&gt; \n  filter(!str_detect(text, \"free throw\")) |&gt; \n  mutate(\n    shot_x = coordinate_x_raw - 25,\n    shot_y = coordinate_y_raw,\n    shot_distance = sqrt((abs(shot_x) ^ 2) + shot_y ^ 2), \n    shot_type = type_text,\n    scoring_play,\n    score_value,\n    .keep = \"none\"\n  )"
  },
  {
    "objectID": "lectures/07-hierarchical.html#the-big-picture",
    "href": "lectures/07-hierarchical.html#the-big-picture",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "The big picture",
    "text": "The big picture\n\n\\(k\\)-means clustering: partition the observations into a pre-specified number of clusters\n\n\n\nHierarchical clustering: does not require commitment to a particular choice of clusters\n\nIn fact, we end up with a tree-like visual representation of the observations, called a dendrogram\nThis allows us to view at once the clusterings obtained for each possible number of clusters\nCommon approach: agglomerative (bottom-up) hierarchical clustering: build a dendrogram starting from the leaves and combining clusters up to the trunk\nThere’s also divisive (top-down) hierarchical clustering: start with one large cluster and then break the cluster recursively into smaller and smaller pieces"
  },
  {
    "objectID": "lectures/07-hierarchical.html#data-nba-player-statistics-per-100-possessions-2024-25-regular-season",
    "href": "lectures/07-hierarchical.html#data-nba-player-statistics-per-100-possessions-2024-25-regular-season",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Data: NBA player statistics per 100 possessions (2024-25 regular season)",
    "text": "Data: NBA player statistics per 100 possessions (2024-25 regular season)\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nnba_players &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/nba_players.csv\")\nhead(nba_players)\n\n# A tibble: 6 × 33\n     rk player    age team  pos       g    gs    mp    fg   fga fg_percent   x3p\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1   152 A.J. G…    25 MIL   SG       73     7  1659   5.3  12.4      0.429   4.5\n2   379 A.J. L…    24 TOR   SG       26     2   486   7.9  18.8      0.421   3.3\n3   338 AJ Joh…    20 2TM   SG       29    11   639   6.1  15.9      0.385   1.8\n4   185 Aaron …    29 DEN   PF       51    42  1447   8.8  16.5      0.531   2.5\n5   305 Aaron …    28 HOU   PG       62     3   792   7.2  16.5      0.437   4.4\n6   250 Aaron …    25 IND   SF       45    37  1123   8.2  16.2      0.507   3.6\n# ℹ 21 more variables: x3pa &lt;dbl&gt;, x3p_percent &lt;dbl&gt;, x2p &lt;dbl&gt;, x2pa &lt;dbl&gt;,\n#   x2p_percent &lt;dbl&gt;, e_fg_percent &lt;dbl&gt;, ft &lt;dbl&gt;, fta &lt;dbl&gt;,\n#   ft_percent &lt;dbl&gt;, orb &lt;dbl&gt;, drb &lt;dbl&gt;, trb &lt;dbl&gt;, ast &lt;dbl&gt;, stl &lt;dbl&gt;,\n#   blk &lt;dbl&gt;, tov &lt;dbl&gt;, pf &lt;dbl&gt;, pts &lt;dbl&gt;, o_rtg &lt;dbl&gt;, d_rtg &lt;dbl&gt;,\n#   awards &lt;lgl&gt;"
  },
  {
    "objectID": "lectures/07-hierarchical.html#general-setup",
    "href": "lectures/07-hierarchical.html#general-setup",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "General setup",
    "text": "General setup\n\n\n\nGiven a dataset with \\(p\\) variables (columns) and \\(n\\) observations (rows) \\(x_1,\\dots,x_n\\)\nCompute the distance/dissimilarity between observations\ne.g. Euclidean distance between observations \\(i\\) and \\(j\\)\n\n\\[d(x_i, x_j) = \\sqrt{(x_{i1}-x_{j1})^2 + \\cdots + (x_{ip}-x_{jp})^2}\\]\nWhat are the distances between these counties using x3pa (3-point attempts) and trb (total rebounds)?\n\n\n\nnba_players |&gt; \n  ggplot(aes(x = x3pa, y = trb)) +\n  geom_point(size = 4)"
  },
  {
    "objectID": "lectures/07-hierarchical.html#remember-to-standardize",
    "href": "lectures/07-hierarchical.html#remember-to-standardize",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Remember to standardize!",
    "text": "Remember to standardize!\n\n\n\nnba_players &lt;- nba_players |&gt; \n  mutate(\n    std_x3pa = as.numeric(scale(x3pa)),\n    std_trb = as.numeric(scale(trb))\n  )\n\nnba_players |&gt; \n  ggplot(aes(x = std_x3pa, y = std_trb)) +\n  geom_point(size = 2) +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/07-hierarchical.html#compute-the-distance-matrix-using-dist",
    "href": "lectures/07-hierarchical.html#compute-the-distance-matrix-using-dist",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Compute the distance matrix using dist()",
    "text": "Compute the distance matrix using dist()\n\nCompute pairwise Euclidean distance\n\n\nplayers_dist &lt;- nba_players |&gt; \n  select(std_x3pa, std_trb) |&gt; \n  dist()\n\n\nReturns an object of dist class… but not a matrix\nConvert to a matrix, then set the row and column names:\n\n\nplayers_dist_matrix &lt;- as.matrix(players_dist)\nrownames(players_dist_matrix) &lt;- nba_players$player\ncolnames(players_dist_matrix) &lt;- nba_players$player\nplayers_dist_matrix[1:4, 1:4]\n\n             A.J. Green A.J. Lawson AJ Johnson Aaron Gordon\nA.J. Green    0.0000000   0.8626792  1.0457677    1.5086463\nA.J. Lawson   0.8626792   0.0000000  1.3441734    1.1393048\nAJ Johnson    1.0457677   1.3441734  0.0000000    0.9839163\nAaron Gordon  1.5086463   1.1393048  0.9839163    0.0000000"
  },
  {
    "objectID": "lectures/07-hierarchical.html#agglomerative-hierarchical-clustering",
    "href": "lectures/07-hierarchical.html#agglomerative-hierarchical-clustering",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "(Agglomerative) Hierarchical clustering",
    "text": "(Agglomerative) Hierarchical clustering\nLet’s pretend all \\(n\\) observations are in their own cluster\n\n\nStep 1: Compute the pairwise dissimilarities between each cluster\n\ne.g., distance matrix on previous slides\n\n\n\n\n\nStep 2: Identify the pair of clusters that are least dissimilar\n\n\n\n\nStep 3: Fuse these two clusters into a new cluster!\n\n\n\n\nRepeat Steps 1 to 3 until all observations are in the same cluster\n\n\n\n“Bottom-up”, agglomerative clustering that forms a tree/hierarchy of merging\nNo mention of any randomness. And no mention of the number of clusters \\(k\\)."
  },
  {
    "objectID": "lectures/07-hierarchical.html#agglomerative-hierarchical-clustering-1",
    "href": "lectures/07-hierarchical.html#agglomerative-hierarchical-clustering-1",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "(Agglomerative) Hierarchical clustering",
    "text": "(Agglomerative) Hierarchical clustering\n\n\nStart with all observations in their own cluster\n\nStep 1: Compute the pairwise dissimilarities between each cluster\nStep 2: Identify the pair of clusters that are least dissimilar\nStep 3: Fuse these two clusters into a new cluster!\nRepeat Steps 1 to 3 until all observations are in the same cluster"
  },
  {
    "objectID": "lectures/07-hierarchical.html#agglomerative-hierarchical-clustering-2",
    "href": "lectures/07-hierarchical.html#agglomerative-hierarchical-clustering-2",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "(Agglomerative) Hierarchical clustering",
    "text": "(Agglomerative) Hierarchical clustering\n\n\nStart with all observations in their own cluster\n\nStep 1: Compute the pairwise dissimilarities between each cluster\nStep 2: Identify the pair of clusters that are least dissimilar\nStep 3: Fuse these two clusters into a new cluster!\nRepeat Steps 1 to 3 until all observations are in the same cluster\n\n\n\n\n\n\n\n\n\n\n\nForms a dendrogram (typically displayed from bottom-up)"
  },
  {
    "objectID": "lectures/07-hierarchical.html#dissimilarity-between-clusters",
    "href": "lectures/07-hierarchical.html#dissimilarity-between-clusters",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Dissimilarity between clusters",
    "text": "Dissimilarity between clusters\n\nWe know how to compute distance/dissimilarity between two observations\nBut how do we handle clusters?\n\nDissimilarity between a cluster and an observation, or between two clusters\n\n\n\nWe need to choose a linkage function. Clusters are built up by linking them together"
  },
  {
    "objectID": "lectures/07-hierarchical.html#types-of-linkage",
    "href": "lectures/07-hierarchical.html#types-of-linkage",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Types of linkage",
    "text": "Types of linkage\nFirst, compute all pairwise dissimilarities between the observations in the two clusters\ni.e., compute the distance matrix between observations, \\(d(x_i, x_j)\\) for \\(i \\in C_1\\) and \\(j \\in C_2\\)\n\n\nComplete linkage: use the maximum (largest) value of these dissimilarities \\(\\underset{i \\in C_1, j \\in C_2}{\\text{max}} d(x_i, x_j)\\) (maximal inter-cluster dissimilarity)\n\n\n\n\nSingle linkage: use the minimum (smallest) value of these dissimilarities \\(\\underset{i \\in C_1, j \\in C_2}{\\text{min}} d(x_i, x_j)\\) (minimal inter-cluster dissimilarity)\n\n\n\n\nAverage linkage: use the average value of these dissimilarities \\(\\displaystyle \\frac{1}{|C_1||C_2|} \\sum_{i \\in C_1} \\sum_{j \\in C_2} d(x_i, x_j)\\) (mean inter-cluster dissimilarity)"
  },
  {
    "objectID": "lectures/07-hierarchical.html#complete-linkage-example",
    "href": "lectures/07-hierarchical.html#complete-linkage-example",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Complete linkage example",
    "text": "Complete linkage example\n\n\n\nUse hclust() with a dist() objsect\nUse complete linkage by default\n\n\nnba_complete &lt;- players_dist |&gt; \n  hclust(method = \"complete\")\n\n\nUse cutree() to return cluster labels\nReturns compact clusters (similar to \\(k\\)-means)\n\n\nnba_players |&gt; \n  mutate(\n    cluster = as.factor(cutree(nba_complete, k = 3))\n  ) |&gt;\n  ggplot(aes(x = std_x3pa, y = std_trb,\n             color = cluster)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/07-hierarchical.html#what-are-we-cutting-dendrograms",
    "href": "lectures/07-hierarchical.html#what-are-we-cutting-dendrograms",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "What are we cutting? Dendrograms",
    "text": "What are we cutting? Dendrograms\n\n\nUse the ggdendro package (instead of plot())\n\nlibrary(ggdendro)\nnba_complete |&gt; \n  ggdendrogram(labels = FALSE, \n               leaf_labels = FALSE,\n               theme_dendro = FALSE) +  \n  labs(y = \"Dissimilarity between clusters\") +\n  theme(axis.text.x = element_blank(), \n        axis.title.x = element_blank(),\n        panel.grid = element_blank())\n\n\nEach leaf is one observation\nHeight of branch indicates dissimilarity between clusters\n\n(After first step) Horizontal position along x-axis means nothing"
  },
  {
    "objectID": "lectures/07-hierarchical.html#textbook-example",
    "href": "lectures/07-hierarchical.html#textbook-example",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Textbook example",
    "text": "Textbook example"
  },
  {
    "objectID": "lectures/07-hierarchical.html#cut-dendrograms-to-obtain-cluster-labels",
    "href": "lectures/07-hierarchical.html#cut-dendrograms-to-obtain-cluster-labels",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Cut dendrograms to obtain cluster labels",
    "text": "Cut dendrograms to obtain cluster labels\n\n\nSpecify the height to cut with h (instead of k)\n\n\n\n\n\n\n\n\n\n\nFor example, cutree(nba_complete, h = 4)"
  },
  {
    "objectID": "lectures/07-hierarchical.html#single-linkage-example",
    "href": "lectures/07-hierarchical.html#single-linkage-example",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Single linkage example",
    "text": "Single linkage example\n\n\nChange the method argument to single\n\n\n\n\n\n\n\n\n\n\nResults in a chaining effect"
  },
  {
    "objectID": "lectures/07-hierarchical.html#average-linkage-example",
    "href": "lectures/07-hierarchical.html#average-linkage-example",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Average linkage example",
    "text": "Average linkage example\n\n\nChange the method argument to average\n\n\n\n\n\n\n\n\n\n\nCloser to complete but varies in compactness"
  },
  {
    "objectID": "lectures/07-hierarchical.html#more-linkage-functions",
    "href": "lectures/07-hierarchical.html#more-linkage-functions",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "More linkage functions",
    "text": "More linkage functions\n\nCentroid linkage: Computes the dissimilarity between the centroid for cluster 1 and the centroid for cluster 2\n\ni.e. distance between the averages of the two clusters\nuse method = centroid\n\n\n\n\nWard’s linkage: Merges a pair of clusters to minimize the within-cluster variance\n\ni.e. aim is to minimize the objection function from \\(K\\)-means\ncan use ward.D or ward.D2 (different algorithms)"
  },
  {
    "objectID": "lectures/07-hierarchical.html#post-clustering-analysis",
    "href": "lectures/07-hierarchical.html#post-clustering-analysis",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Post-clustering analysis",
    "text": "Post-clustering analysis\n\nFor context, how does position relate clustering results?\nTwo-way table to compare the clustering assignments with player positions\n(What’s the way to visually compare these two variables?)\n\n\ntable(\"Cluster\" = cutree(nba_complete, k = 3), \"Position\" = nba_players$pos)\n\n       Position\nCluster   C  PF  PG  SF  SG\n      1  15  40  69  51 100\n      2   4  25  15  22  19\n      3  70  20   1   5   1\n\n\n\nTakeaway: positions tend to fall within particular clusters"
  },
  {
    "objectID": "lectures/07-hierarchical.html#include-more-variables",
    "href": "lectures/07-hierarchical.html#include-more-variables",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Include more variables",
    "text": "Include more variables\n\n\n\nIt’s easy to include more variables - just change the distance matrix\n\n\nnba_players_features &lt;- nba_players |&gt; \n  select(x3pa, x2pa, fta, trb, ast, stl, blk, tov)\n  \nplayer_dist_mult_features &lt;- nba_players_features |&gt; \n  dist()\n\n\nThen perform hierarchical clustering as before\n\n\nnba_players_hc_complete &lt;- player_dist_mult_features |&gt; \n  hclust(method = \"complete\") # can try out other methods\n\n\nVisualize with dendrogram\n\n\nnba_players_hc_complete |&gt; \n  ggdendrogram(labels = FALSE, \n               leaf_labels = FALSE,\n               theme_dendro = FALSE) +\n  labs(y = \"Dissimilarity between clusters\") +\n  theme(axis.text.x = element_blank(), \n        axis.title.x = element_blank(),\n        panel.grid = element_blank())"
  },
  {
    "objectID": "lectures/07-hierarchical.html#visualizing-clustering-results-with-pca",
    "href": "lectures/07-hierarchical.html#visualizing-clustering-results-with-pca",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Visualizing clustering results with PCA",
    "text": "Visualizing clustering results with PCA\n\nSimilar to \\(k\\)-means, if there are more than two dimensions (variables), we can perform PCA\nThen plot the observations onto the first two principal components\n\n\n\n\nlibrary(factoextra)\nfviz_cluster(\n  list(data = nba_players_features,\n       cluster = cutree(nba_players_hc_complete, k = 3)),\n  geom = \"point\",\n  ellipse = FALSE\n) +\n  ggthemes::scale_color_colorblind() +\n  theme_light()"
  },
  {
    "objectID": "lectures/07-hierarchical.html#choosing-number-of-clusters",
    "href": "lectures/07-hierarchical.html#choosing-number-of-clusters",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Choosing number of clusters",
    "text": "Choosing number of clusters\n\nJust like \\(k\\)-means, there are heuristics for choosing the number of clusters for hierarchical clustering\nOptions: elbow method, silhouette, gap statistic (but again, use these with caution)\n\n\n\n\nnba_players_features |&gt; \n  fviz_nbclust(FUN = hcut, method = \"wss\")\n\n# silhouette\n# nba_players_features |&gt; \n#   fviz_nbclust(FUN = hcut, method = \"silhouette\")\n\n# gap statistic\n# library(cluster)\n# nba_hc_gap_stat &lt;- nba_players_features |&gt; \n#   clusGap(FUN = hcut, nstart = 30, K.max = 10, B = 50)\n# nba_hc_gap_stat |&gt; \n#   fviz_gap_stat()"
  },
  {
    "objectID": "lectures/07-hierarchical.html#practical-issues",
    "href": "lectures/07-hierarchical.html#practical-issues",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Practical issues",
    "text": "Practical issues\n\nWhat dissimilarity measure should be used?\nWhat type of linkage should be used?\nHow many clusters to choose?\nWhich features should we use to drive the clustering?\n\nCategorical variables?\n\nHard clustering vs. soft clustering\n\nHard clustering (\\(k\\)-means, hierarchical): assigns each observation to exactly one cluster\nSoft (fuzzy) clustering: assigns each observation a probability of belonging to a cluster"
  },
  {
    "objectID": "lectures/07-hierarchical.html#appendix-code-to-build-dataset",
    "href": "lectures/07-hierarchical.html#appendix-code-to-build-dataset",
    "title": "Unsupervised learning: hierarchical clustering",
    "section": "Appendix: code to build dataset",
    "text": "Appendix: code to build dataset\n\nlibrary(tidyverse)\nlibrary(rvest)\nnba_url &lt;- \"https://www.basketball-reference.com/leagues/NBA_2025_per_poss.html\"\nnba_players &lt;- nba_url |&gt; \n  read_html() |&gt; \n  html_element(css = \"#per_poss\") |&gt; \n  html_table() |&gt; \n  janitor::clean_names() |&gt; \n  group_by(player) |&gt; \n  slice_max(g) |&gt; \n  ungroup() |&gt; \n  filter(mp &gt;= 200) # keep players with at least 200 minutes played"
  },
  {
    "objectID": "lectures/02-graphics.html#goals-of-data-visualization",
    "href": "lectures/02-graphics.html#goals-of-data-visualization",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Goals of data visualization",
    "text": "Goals of data visualization\n\nto represent the data in a visual way (enough with tables… though tables are useful in a lot of situations)\n\n\n\nmost importantly, to deliver the information to your audience and help them understand the story behind the data\n\n\n\n\nData Visualization (good DataViz anyway) answers a question. - Greggy J. M."
  },
  {
    "objectID": "lectures/02-graphics.html#always-visualize-your-data-before-modeling-and-analysis",
    "href": "lectures/02-graphics.html#always-visualize-your-data-before-modeling-and-analysis",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "ALWAYS visualize your data before modeling and analysis",
    "text": "ALWAYS visualize your data before modeling and analysis\nAnscombe’s quartet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 4 × 6\n  set   x_mean x_var y_mean y_var x_y_cor\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 1          9    11   7.50  4.13   0.816\n2 2          9    11   7.50  4.13   0.816\n3 3          9    11   7.5   4.12   0.816\n4 4          9    11   7.50  4.12   0.817"
  },
  {
    "objectID": "lectures/02-graphics.html#always-visualize-your-data-before-modeling-and-analysis-1",
    "href": "lectures/02-graphics.html#always-visualize-your-data-before-modeling-and-analysis-1",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "ALWAYS visualize your data before modeling and analysis",
    "text": "ALWAYS visualize your data before modeling and analysis\nThe Datasaurus dozen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 13 × 6\n   dataset    x_mean x_var y_mean y_var x_y_cor\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 away         54.3  281.   47.8  726. -0.0641\n 2 bullseye     54.3  281.   47.8  726. -0.0686\n 3 circle       54.3  281.   47.8  725. -0.0683\n 4 dino         54.3  281.   47.8  726. -0.0645\n 5 dots         54.3  281.   47.8  725. -0.0603\n 6 h_lines      54.3  281.   47.8  726. -0.0617\n 7 high_lines   54.3  281.   47.8  726. -0.0685\n 8 slant_down   54.3  281.   47.8  726. -0.0690\n 9 slant_up     54.3  281.   47.8  726. -0.0686\n10 star         54.3  281.   47.8  725. -0.0630\n11 v_lines      54.3  281.   47.8  726. -0.0694\n12 wide_lines   54.3  281.   47.8  726. -0.0666\n13 x_shape      54.3  281.   47.8  725. -0.0656"
  },
  {
    "objectID": "lectures/02-graphics.html#viz-crime",
    "href": "lectures/02-graphics.html#viz-crime",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Viz crime?",
    "text": "Viz crime?\nFlorence Nightingale’s rose diagram"
  },
  {
    "objectID": "lectures/02-graphics.html#viz-crime-1",
    "href": "lectures/02-graphics.html#viz-crime-1",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Viz crime?",
    "text": "Viz crime?"
  },
  {
    "objectID": "lectures/02-graphics.html#previously",
    "href": "lectures/02-graphics.html#previously",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Previously…",
    "text": "Previously…\nGet batting stats for each year: each row is a year with the following variables\ntotal hits, home runs, strikeouts, walks, atbats total batting average for each year = total H / total AB only keeps AL and NL leagues"
  },
  {
    "objectID": "lectures/02-graphics.html#previously-1",
    "href": "lectures/02-graphics.html#previously-1",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Previously…",
    "text": "Previously…\n\n\nYearly MLB batting statistics from Lahman with tidyverse:\n\ntotal hits, home runs, strikeouts, walks, at bats\ntotal batting average for each year= total H / total AB\nonly keeps AL and NL leagues\n\n\nlibrary(tidyverse)\nlibrary(Lahman) \nyearly_batting &lt;- Batting |&gt;\n  filter(lgID %in% c(\"AL\", \"NL\")) |&gt;\n  group_by(yearID) |&gt;\n  summarize(total_h = sum(H, na.rm = TRUE),\n            total_hr = sum(HR, na.rm = TRUE),\n            total_so = sum(SO, na.rm = TRUE),\n            total_bb = sum(BB, na.rm = TRUE),\n            total_ab = sum(AB, na.rm = TRUE)) |&gt;\n  mutate(batting_avg = total_h / total_ab)\n\n\nHow do we make data visualization?\nWhat are the steps to make this figure below?"
  },
  {
    "objectID": "lectures/02-graphics.html#the-grammar-of-graphics",
    "href": "lectures/02-graphics.html#the-grammar-of-graphics",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "The grammar of graphics",
    "text": "The grammar of graphics\n\nKey idea: specify plotting “layers” and combine them to produce a graphic\nggplot2 provides an implementation of the grammar of graphics\nThe following layers are building blocks of data graphics\n\n\n\ndata - one or more datasets (in tidy tabular format)\ngeom - geometric objects to visually represent the data (e.g. points, lines, bars, etc.)\naes - mappings of variables to visual properties (i.e. aesthetics) of the geometric objects\nscale - one scale for each variable displayed (e.g. axis limits, log scale, colors, etc.)\nfacet - similar subplots (i.e. facets) for subsets of the same data using a conditioning variable\nstat - statistical transformations and summaries (e.g. identity, count, smooth, quantile, etc.)\ncoord - one or more coordinate systems (e.g. cartesian, polar, map projection)\nlabs - labels/guides for each variable and other parts of the plot (e.g. title, subtitle, caption, etc.)\ntheme - customization of plot layout (e.g. text size, alignment, legend position, etc.)\n\n\n\n\nLeland Wilkinson wrote the book “The Grammar of Graphics”, originally published in 1999."
  },
  {
    "objectID": "lectures/02-graphics.html#first-start-with-the-data",
    "href": "lectures/02-graphics.html#first-start-with-the-data",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "First, start with the data",
    "text": "First, start with the data\n\n\n\n\nggplot(data = yearly_batting)\n\n\nor equivalently, using |&gt;\n\nyearly_batting |&gt; \n  ggplot()\n\n\nSo far, nothing is displayed"
  },
  {
    "objectID": "lectures/02-graphics.html#specify-variables-and-geometric-object",
    "href": "lectures/02-graphics.html#specify-variables-and-geometric-object",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Specify variables and geometric object",
    "text": "Specify variables and geometric object\n\n\n\n\nyearly_batting |&gt; \n  ggplot() +\n  geom_point(aes(x = yearID, y = total_hr))\n\n\nAdding (+) a geometric layer of points to the plot\nMap yearID to the x-axis and total_hr to the y-axis via aes()\nImplicitly using coord_cartesian()\n\n\nyearly_batting |&gt; \n  ggplot() + \n  geom_point(aes(x = yearID, y = total_hr)) +\n  coord_cartesian()"
  },
  {
    "objectID": "lectures/02-graphics.html#now-can-we-add-another-geometric-layer",
    "href": "lectures/02-graphics.html#now-can-we-add-another-geometric-layer",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Now, can we add another geometric layer?",
    "text": "Now, can we add another geometric layer?\n\n\n\n\nyearly_batting |&gt; \n  ggplot() +\n  geom_point(aes(x = yearID, y = total_hr)) +\n  geom_line(aes(x = yearID, y = total_hr))\n\n\nAdding (+) a line geometric layer\nInclude mappings shared across geometric layers inside ggplot()\n\n\nyearly_batting |&gt;\n  ggplot(aes(x = yearID, y = total_hr)) +\n  geom_point() +\n  geom_line()"
  },
  {
    "objectID": "lectures/02-graphics.html#scaling-axes-changing-axis-label-breaks",
    "href": "lectures/02-graphics.html#scaling-axes-changing-axis-label-breaks",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Scaling axes: changing axis label breaks",
    "text": "Scaling axes: changing axis label breaks\n\n\n\n\nyearly_batting |&gt;\n  ggplot(aes(x = yearID, y = total_hr)) +\n  geom_point() +\n  geom_line() +\n  scale_y_continuous(breaks = seq(0, 6000, 1000))"
  },
  {
    "objectID": "lectures/02-graphics.html#scaling-axes-customizing-axis-limits",
    "href": "lectures/02-graphics.html#scaling-axes-customizing-axis-limits",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Scaling axes: customizing axis limits",
    "text": "Scaling axes: customizing axis limits\n\n\n\n\nyearly_batting |&gt;\n  ggplot(aes(x = yearID, y = total_hr)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(limits = c(2000, 2015))"
  },
  {
    "objectID": "lectures/02-graphics.html#scaling-axes-having-different-axis-scales",
    "href": "lectures/02-graphics.html#scaling-axes-having-different-axis-scales",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Scaling axes: having different axis scales",
    "text": "Scaling axes: having different axis scales\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point() +\n  geom_line() + \n  scale_x_reverse() +\n  scale_y_log10()\n\nWe can easily adjust variable scales without directly modifying the columns in the data"
  },
  {
    "objectID": "lectures/02-graphics.html#adding-a-statistical-summary",
    "href": "lectures/02-graphics.html#adding-a-statistical-summary",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Adding a statistical summary",
    "text": "Adding a statistical summary\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point() +\n  geom_line() + \n  stat_smooth()\n\n\nSmoothing regression summary (will cover later) using yearID and total_hr\nGeometric layers implicitly use a default statistical summary\nTechnically we’re already using geom_point(stat = \"identity\")\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point() +\n  geom_line() +\n  geom_smooth()"
  },
  {
    "objectID": "lectures/02-graphics.html#mapping-additional-variables",
    "href": "lectures/02-graphics.html#mapping-additional-variables",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Mapping additional variables",
    "text": "Mapping additional variables\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr,\n             color = total_so,\n             size = total_bb)) +\n  geom_point() +\n  geom_line()\n\n\ntotal_hr, total_so, and total_bb are all displayed\ncolor and size are being shared globally across layers\nThis is a bit odd to look at…"
  },
  {
    "objectID": "lectures/02-graphics.html#customizing-mappings-by-layer",
    "href": "lectures/02-graphics.html#customizing-mappings-by-layer",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Customizing mappings by layer",
    "text": "Customizing mappings by layer\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point(aes(color = total_so, size = total_bb)) +\n  geom_line()\n\n\nNow mapping total_so and total_bb to color and size of the point layer only"
  },
  {
    "objectID": "lectures/02-graphics.html#changing-aesthetics-without-mapping-variables",
    "href": "lectures/02-graphics.html#changing-aesthetics-without-mapping-variables",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Changing aesthetics without mapping variables",
    "text": "Changing aesthetics without mapping variables\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point(aes(color = total_so, size = total_bb)) +\n  geom_line(color = \"darkred\", linetype = \"dashed\")\n\n\nManually set the color and linetype of the line layer"
  },
  {
    "objectID": "lectures/02-graphics.html#remember-one-scale-for-each-mapped-variable",
    "href": "lectures/02-graphics.html#remember-one-scale-for-each-mapped-variable",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Remember: one scale for each mapped variable",
    "text": "Remember: one scale for each mapped variable\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point(aes(color = total_so, size = total_bb)) +\n  geom_line(color = \"darkred\", linetype = \"dashed\") +\n  scale_color_gradient(low = \"darkblue\", high = \"gold\") +\n  scale_size_continuous(breaks = seq(0, 20000, 2500))"
  },
  {
    "objectID": "lectures/02-graphics.html#always-label-your-plots-seriously",
    "href": "lectures/02-graphics.html#always-label-your-plots-seriously",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Always label your plots! (seriously…)",
    "text": "Always label your plots! (seriously…)\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point(aes(color = total_so, size = total_bb)) +\n  geom_line(color = \"darkred\", linetype = \"dashed\") +\n  scale_color_gradient(low = \"darkblue\", high = \"gold\") +\n  labs(\n    x = \"Year\",\n    y = \"Homeruns\",\n    color = \"Strikeouts\",\n    size = \"Walks\",\n    title = \"The rise of three true outcomes in baseball\",\n    caption = \"Data courtesy of Lahman\"\n  )\n\n\nEach mapped aesthetic can be labeled"
  },
  {
    "objectID": "lectures/02-graphics.html#custom-theme",
    "href": "lectures/02-graphics.html#custom-theme",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Custom theme",
    "text": "Custom theme\n\n\n\n\nyearly_batting |&gt; \n  ggplot(aes(x = yearID, y = total_hr)) + \n  geom_point(aes(color = total_so, size = total_bb)) +\n  geom_line(color = \"darkred\", linetype = \"dashed\") +\n  scale_color_gradient(low = \"darkblue\", high = \"gold\") +\n  labs(\n    x = \"Year\",\n    y = \"Homeruns\",\n    color = \"Strikeouts\",\n    size = \"Walks\",\n    title = \"The rise of three true outcomes in baseball\",\n    caption = \"Data courtesy of Lahman\"\n  ) +\n  theme_bw(base_size = 20) +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5, \n                                  face = \"bold\"))\n\n\nFor more theme options, check out the ggthemes and hrbrthemes packages"
  },
  {
    "objectID": "lectures/02-graphics.html#a-lesson-about-data-visualization",
    "href": "lectures/02-graphics.html#a-lesson-about-data-visualization",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "A lesson about data visualization…",
    "text": "A lesson about data visualization…\n\nSo far we’ve plotted total home runs across the years, with point size representing total walks and point color representing total strikeouts\n\n\n\nSimpler is better. What can we do to improve and make the plot simpler?\n\n\n\n\nHow about creating three separate plots for home runs, strikeouts, and walks, with each mapped to the y-axis?\nBut how do we do this without repeating the same code?"
  },
  {
    "objectID": "lectures/02-graphics.html#pivoting",
    "href": "lectures/02-graphics.html#pivoting",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Pivoting",
    "text": "Pivoting\nRemember: data should be in tidy format\nWithin the tidyverse, the tidyr package offers functions for reshaping the data\n\npivot_longer: casts/gathers information spread out across variables\n\ntransforms data from wide format into long format\nincrease number of rows and decrease number of columns\n\npivot_wider: melts/spreads information out from observations\n\ntransforms data from long format into wide format\ndecrease number of rows and increase number of columns\n\n\n\n\nSome terminology\nPredecessors: reshape and reshape2"
  },
  {
    "objectID": "lectures/02-graphics.html#pivoting-1",
    "href": "lectures/02-graphics.html#pivoting-1",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Pivoting",
    "text": "Pivoting\n\nyearly_batting |&gt; \n  select(yearID, HRs = total_hr, Strikeouts = total_so, Walks = total_bb) |&gt; # renaming while also selecting\n  pivot_longer(HRs:Walks, # can also do !yearID (to select everything but yearID)\n               names_to = \"stat\",\n               values_to = \"val\")\n\n# A tibble: 444 × 3\n   yearID stat         val\n    &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n 1   1876 HRs           40\n 2   1876 Strikeouts   589\n 3   1876 Walks        336\n 4   1877 HRs           24\n 5   1877 Strikeouts   726\n 6   1877 Walks        345\n 7   1878 HRs           23\n 8   1878 Strikeouts  1081\n 9   1878 Walks        364\n10   1879 HRs           58\n# ℹ 434 more rows\n\n\nWe’ve pivoted the data and created the following variables\n\nstat, to represent the name of the batting statistics\nval, to represent the total value of each statistic in each year."
  },
  {
    "objectID": "lectures/02-graphics.html#faceting",
    "href": "lectures/02-graphics.html#faceting",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Faceting",
    "text": "Faceting\n\n\n\nyearly_batting |&gt;\n  select(yearID, HRs = total_hr, \n         Strikeouts = total_so, Walks = total_bb) |&gt;\n  pivot_longer(HRs:Walks, \n               names_to = \"stat\", \n               values_to = \"val\") |&gt;\n  ggplot(aes(yearID, val)) +\n  geom_line(color = \"darkblue\") +\n  geom_point(alpha = 0.8, color = \"darkblue\") +\n  facet_wrap(~ stat, scales = \"free_y\", ncol = 1) +\n  labs(\n    x = \"Year\", \n    y = \"Total of statistic\",\n    title = \"The rise of three true outcomes in baseball\",\n    caption = \"Data courtesy of Lahman\"\n  ) +\n  theme_bw(base_size = 20) +\n  theme(strip.background = element_blank(),\n        plot.title = element_text(hjust = 0.5, \n                                  face = \"bold\"))\n\n\nCreate a multi-panel plot faceted by a conditioning variable (in our case, stat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese facet/panel plots are sometimes called trellis plots (or lattice plots)"
  },
  {
    "objectID": "lectures/02-graphics.html#exercise",
    "href": "lectures/02-graphics.html#exercise",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Exercise",
    "text": "Exercise\nThe babynames package contains a dataset (also) named babynames, which contains information on the number of children of each sex given each name from 1880 to 2017, provided by the United States Social Security Administration.\nHow does the popularity (in terms of frequency) of your own name (combination of name and sex) change over time? Also, stick a thick, red, vertical dashed line on the plot at your birth year (try geom_vline()).\nNext, pick two other names and compare their popularity over time with your own name.\n\n# install.packages(\"babynames\")\n# library(babynames)\n# babynames |&gt; \n#   INSERT CODE HERE"
  },
  {
    "objectID": "lectures/02-graphics.html#resources",
    "href": "lectures/02-graphics.html#resources",
    "title": "Data visualization: the grammar of graphics and ggplot2",
    "section": "Resources",
    "text": "Resources\n\nggplot2 website: cheatsheets, FAQs, extensions, and more\nTidyTuesday\nDavid Robinson’s TidyTuesday screencasts\nVizBuzz: LIVE data viz replication game show\n\nQuang on VizBuzz (will participate again this summer!)"
  },
  {
    "objectID": "health/eda/hospitals.html#overview",
    "href": "health/eda/hospitals.html#overview",
    "title": "EDA project: hospital ratings",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 5 and conclude with a 6-minute presentation on Tuesday, June 17 during lab.\nStudents will be randomly placed into groups of 2–3 and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/hospitals.html#deliverables",
    "href": "health/eda/hospitals.html#deliverables",
    "title": "EDA project: hospital ratings",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 6-minute presentation. All group members must be present for the presentation and participate. No notes/scripts are allowed during the presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nTwo questions/hypotheses you are interested in exploring\nTwo data visualizations exploring the questions—both must be multivariate (i.e., involving 2+ variables) and in different formats\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/hospitals.html#timeline",
    "href": "health/eda/hospitals.html#timeline",
    "title": "EDA project: hospital ratings",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 12 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review.\nTuesday, June 17 at 1pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "health/eda/hospitals.html#data",
    "href": "health/eda/hospitals.html#data",
    "title": "EDA project: hospital ratings",
    "section": "Data",
    "text": "Data\nThis dataset contains information on hospital ratings curated by the CORGIS Dataset Project. The goal of this project was to: “allow consumers to directly compare across hospitals performance measure information related to heart attack, emergency department care, preventive care, stroke care, and other conditions. The data is part of an Administration-wide effort to increase the availability and accessibility of information on quality, utilization, and costs for effective, informed decision-making.” The original source of data can be found here.\nEach row of the dataset corresponds to a single hospital and the columns with definitions borrowed from the online glossary are:\n\nFacility.Name: name of the hospital\nFacility.City: city in which the hospital is located\nFacility.State: two letter capitalized abbreviation of the State in which the hospital is located\nFacility.Type: type of organization operating the hospital: one of Government, Private, Proprietary, Church, or Unknown\nRating.Overall: overall rating between 1 and 5 stars, with 5 stars being the highest rating; -1 represents no rating\nRating.Mortality: Above, Same, Below, or Unknown comparison to national hospital mortality\nRating.Safety: Above, Same, Below, or Unknown comparison to national hospital safety\nRating.Readmission: Above, Same, Below, or Unknown comparison to national hospital readmission\nRating.Experience: Above, Same, Below, or Unknown comparison to national hospital patience experience\nRating.Effectiveness: Above, Same, Below, or Unknown comparison to national hospital effectiveness of care\nRating.Timeliness: Above, Same, Below, or Unknown comparison to national hospital timeliness of care\nRating.Imaging: Above, Same, Below, or Unknown comparison to national hospital effective use of imaging\nProcedure.Heart Attack.Cost: Average cost of care for heart attacks\nProcedure.Heart Attack.Quality: Lower, Average, Worse, or Unknown comparison to national quality of care for heart attacks\nProcedure.Heart Attack.Value: Lower, Average, Worse, or Unknown comparison to national cost of care for heart attacks\nProcedure.Heart Failure.Cost: Average cost of care for heart failure\nProcedure.Heart Failure.Quality: Lower, Average, Worse, or Unknown comparison to national quality of care for heart failures\nProcedure.Heart Failure.Value: Lower, Average, Worse, or Unknown comparison to national cost of care for heart failures\nProcedure.Pneumonia.Cost: Average cost of care for pneumonia\nProcedure.Pneumonia.Quality: Lower, Average, Worse, or Unknown comparison to national quality of care for pneumonia\nProcedure.Pneumonia.Value: Lower, Average, Worse, or Unknown comparison to national cost of care for pneumonia\nProcedure.Hip Knee.Cost: Average cost of care for hip or knee conditions\nProcedure.Hip Knee.Quality: Lower, Average, Worse, or Unknown comparison to national quality of care for hip or knee conditions\nProcedure.Hip Knee.Value: Lower, Average, Worse, or Unknown comparison to national cost of care for hip or knee conditions"
  },
  {
    "objectID": "health/eda/hospitals.html#starter-code",
    "href": "health/eda/hospitals.html#starter-code",
    "title": "EDA project: hospital ratings",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nhospitals &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/hospitals.csv\")"
  },
  {
    "objectID": "health/eda/covid_california.html#overview",
    "href": "health/eda/covid_california.html#overview",
    "title": "EDA project: COVID vaccine distribution in California",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 5 and conclude with a 6-minute presentation on Tuesday, June 17 during lab.\nStudents will be randomly placed into groups of 2–3 and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/covid_california.html#deliverables",
    "href": "health/eda/covid_california.html#deliverables",
    "title": "EDA project: COVID vaccine distribution in California",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 6-minute presentation. All group members must be present for the presentation and participate. No notes/scripts are allowed during the presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nTwo questions/hypotheses you are interested in exploring\nTwo data visualizations exploring the questions—both must be multivariate (i.e., involving 2+ variables) and in different formats\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/covid_california.html#timeline",
    "href": "health/eda/covid_california.html#timeline",
    "title": "EDA project: COVID vaccine distribution in California",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 12 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review.\nTuesday, June 17 at 1pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "health/eda/covid_california.html#data",
    "href": "health/eda/covid_california.html#data",
    "title": "EDA project: COVID vaccine distribution in California",
    "section": "Data",
    "text": "Data\nThis dataset contains county-level information on weekly COVID vaccine distribution in California in 2021 and 2022. The data are available online and published by the California Department of Public Health.\nEach row in the dataset corresponds to a county in California in a given week (of 2021 and 2022) and the columns are:\n\ncounty: name of the county\n\nyear: year (2021 or 2022)\n\nweek: week number within a year\n\nfirst_date: first date of the week\n\nlast_date: last date of the week\n\ndoses_shipped: number of COVID-19 vaccine doses shipped in a particular week (includes direct federal allocations and federal pharmacy partnership programs but does not include doses shipped to the following federal entities: Indian Health Service, Department of Defense, U.S. Federal Bureau of Prisons, and Veterans Affairs)\ncumulative_doses_shipped: cumulative number of COVID-19 vaccine doses shipped up to that week\ndoses_delivered: number of COVID-19 vaccine doses delivered in a particular week (includes direct federal allocations and federal pharmacy partnership programs but does not include doses delivered to the following federal entities: Indian Health Service, Department of Defense, U.S. Federal Bureau of Prisons, and Veterans Affairs)\ncumulative_doses_delivered: cumulative number of COVID-19 vaccine doses delivered up to a particular week\n\ncdc_pharmacy_doses_delivered: number of COVID-19 vaccine doses delivered to the CDC Pharmacy Partnership for Long-Term Care Facility (LTCF) Program and the Federal Retail Pharmacy Program in a particular week\n\ncumulative_cdc_pharmacy_doses_delivered: cumulative number of COVID-19 vaccine doses delivered to the CDC Pharmacy Partnership for Long-Term Care Facility (LTCF) Program and the Federal Retail Pharmacy Program up to a particular week\n{pfizer, moderna, jj}_doses_shipped: number of {Pfizer, Moderna, Janssen (Johnson & Johnson)} vaccine doses shipped in a particular week\n\ncumulative_{pfizer, moderna, jj}_doses_shipped: cumulative number of {Pfizer, Moderna, Janssen (Johnson & Johnson)} vaccine doses shipped up to a particular week\n{pfizer, moderna, jj}_doses_delivered: number of {Pfizer, Moderna, Janssen (Johnson & Johnson)} vaccine doses delivered in a particular week\ncumulative_{pfizer, moderna, jj}_doses_delivered: cumulative number of {Pfizer, Moderna, Janssen (Johnson & Johnson)} vaccine doses delivered up to a particular week"
  },
  {
    "objectID": "health/eda/covid_california.html#starter-code",
    "href": "health/eda/covid_california.html#starter-code",
    "title": "EDA project: COVID vaccine distribution in California",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\ncovid_cases_deaths &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/covid_california.csv\")"
  },
  {
    "objectID": "health/eda/maternal.html#overview",
    "href": "health/eda/maternal.html#overview",
    "title": "EDA project: maternal healthcare disparities",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 5 and conclude with a 6-minute presentation on Tuesday, June 17 during lab.\nStudents will be randomly placed into groups of 2–3 and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/maternal.html#deliverables",
    "href": "health/eda/maternal.html#deliverables",
    "title": "EDA project: maternal healthcare disparities",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 6-minute presentation. All group members must be present for the presentation and participate. No notes/scripts are allowed during the presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nTwo questions/hypotheses you are interested in exploring\nTwo data visualizations exploring the questions—both must be multivariate (i.e., involving 2+ variables) and in different formats\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/maternal.html#timeline",
    "href": "health/eda/maternal.html#timeline",
    "title": "EDA project: maternal healthcare disparities",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 12 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review.\nTuesday, June 17 at 1pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "health/eda/maternal.html#data",
    "href": "health/eda/maternal.html#data",
    "title": "EDA project: maternal healthcare disparities",
    "section": "Data",
    "text": "Data\nThis dataset contains information on maternal healthcare disparities. The Centers for Disease Control and Prevention WONDER program helps track information related to birth records, parent demographics and risk factors, pregnancy history and pre-natal care characteristics. This data source could help identify combinations of risk factors more commonly associated with adverse outcomes which could then be utilized to develop better pre-natal care programs or targeted interventions to reduce disparities and improve patient outcomes across all ethnicity.\nThe dataset is a sample of data from the CDC Wonder database for available birth records from 2019 that has been aggregated by state and a few conditions (the number of prior births now deceased and whether the mother smoked or had pre-pregnancy diabetes or pre-pregnancy hypertension). For example, the first row corresponds to the set of births that were born to women in Alabama who had no prior births deceased, smoked, and had both diabetes and hypertension pre-pregnancy. There were 12 such births, and the following variables (e.g. mother’s age) describe the mothers/infants in that set of 12.\n\nState: name of the state\nPriorBirthsNowDeceased: number of prior births now deceased\nTobaccoUse: whether the mother uses tobacco products\nPrePregnancyDiabetes: whether the mother had diabetes prior to becoming pregnant\nPrePregnancyHypertension: whether the mother had hypertension prior to becoming pregnant\nBirths: number of births in that state with a defined combination of the previous four conditions (PriorBirthsNowDeceased, TobaccoUse, PrePregnancyDiabetes, PrePregnancyHypertension)\nAverageMotherAge: average mother’s age for the corresponding group of births\nAverageBirthWeight: average birth weight in grams for the corresponding group of births\nAveragePrePregnancyBMI: average pre-pregnancy BMI of the mother for the corresponding group of births\nAverageNumberPrenatalVisits: average number of prenatal visits of the mother for the corresponding group of births\nAverageIntervalSinceLastBirth: average length of time since the last birth for the corresponding group of births"
  },
  {
    "objectID": "health/eda/maternal.html#starter-code",
    "href": "health/eda/maternal.html#starter-code",
    "title": "EDA project: maternal healthcare disparities",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nmaternal &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/maternal.csv\")"
  },
  {
    "objectID": "health/labs/03-visualization.html",
    "href": "health/labs/03-visualization.html",
    "title": "Lab: data visualization",
    "section": "",
    "text": "Let’s start again by reading in the data from yesterday using the read_csv() function after loading the tidyverse:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nheart_disease &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/heart_disease.csv\")\n\nRows: 788 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Gender\ndbl (9): Cost, Age, Interventions, Drugs, ERVisit, Complications, Comorbidit...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "health/labs/03-visualization.html#reading-in-data",
    "href": "health/labs/03-visualization.html#reading-in-data",
    "title": "Lab: data visualization",
    "section": "",
    "text": "Let’s start again by reading in the data from yesterday using the read_csv() function after loading the tidyverse:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nheart_disease &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/heart_disease.csv\")\n\nRows: 788 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Gender\ndbl (9): Cost, Age, Interventions, Drugs, ERVisit, Complications, Comorbidit...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "health/labs/03-visualization.html#previewing-the-data",
    "href": "health/labs/03-visualization.html#previewing-the-data",
    "title": "Lab: data visualization",
    "section": "Previewing the data",
    "text": "Previewing the data\nWrite code that displays the column names of heart_disease. Also, look at the first six rows of your dataset to get an idea of what these variables look like. Which variables are quantitative, and which are categorical?\n\n# INSERT CODE HERE\n\nAs it turns out, even though Drugs and Complications appear to be quantitative - they are actually categorical variables. Specifically, Drugs represents the categorized number of drugs prescribed: 0 if none, 1 if one, 2 if more than one; Complications indicates whether or not the subscriber had complications: 1 if yes, 0 if no. To address this issue for our plots, we can manually recode the variables as factors. For instance, we can modify the Complications variable using a simple if-else statement:\n\nheart_disease &lt;- heart_disease |&gt;\n  mutate(Complications = ifelse(Complications == 0, \"No\", \"Yes\"))\n\nThis is a quick fix to the binary indicator variable since, by default, R orders factor variables in alphabetical order. In this case, “No” is before “Yes” because “N” is before “Y”. We may not want variables in alphabetical order however - we will see how to change this in lecture.\nNext, to update the Drugs variable we will use the fct_recode() function which allows us to manually change the labels of a factor variable:\n\nheart_disease &lt;- heart_disease |&gt;\n  mutate(Drugs = fct_recode(as.factor(Drugs), \n                            \"None\" = \"0\", \"One\" = \"1\", \"More than one\" = \"2\"))\n\nWhy did we have to specify as.factor(Drugs) first then place the numbers in quotation marks?"
  },
  {
    "objectID": "health/labs/03-visualization.html#always-make-a-bar-chart",
    "href": "health/labs/03-visualization.html#always-make-a-bar-chart",
    "title": "Lab: data visualization",
    "section": "Always make a bar chart…",
    "text": "Always make a bar chart…\nNow we’ll use the ggplot() function to create a bar chart of the Drugs variable. To make things easier, we provide the code for you to do this below; just uncomment the code and run it to create the bar chart. In what follows, you must answer some questions about the code and plot.\n\n# Create the bar chart of Drugs:\n# heart_disease |&gt;\n#   ggplot(aes(x = Drugs)) +\n#   geom_bar(fill = \"darkblue\") +\n#   labs(title = \"Number of patients by number of drugs\",\n#        x = \"Number of drugs\",\n#        y = \"Number of patients\")\n\nAnswer the following questions about the code and plot:\n\nIn general, ggplot() code takes the following format: ggplot(blank1, aes(x = blank2)). Looking at the above code, what kind of R object should blank1 be, and what should blank2 be?\nWhat do you think the line geom_bar(fill = \"darkblue\") does?\nWhat do you think the remaining lines of code do (contained in labs())?"
  },
  {
    "objectID": "health/labs/03-visualization.html#more-area-plots-but-bar-charts-are-better",
    "href": "health/labs/03-visualization.html#more-area-plots-but-bar-charts-are-better",
    "title": "Lab: data visualization",
    "section": "More area plots (but bar charts are better!)",
    "text": "More area plots (but bar charts are better!)\nNow we’ll make a few other area plots:\n\nspine chart\npie chart\nrose diagram\n\nYour goal for this part is to create each of these plots. These plots can be created by copy-and-pasting the bar chart code from above and modifying it slightly. Follow these directions to create each of these plots:\n\nspine chart: First, copy-and-paste the bar chart code from above. Then, delete the fill = \"darkblue\" within geom_bar(). Finally, within ggplot(), replace aes(x = Drugs) with aes(x = \"\", fill = Drugs). Also, change the labels in labs() if necessary.\n\n\n# PUT YOUR SPINE CHART CODE HERE\n\n\npie chart: First, copy-and-paste the spine chart code you just made. Then, after geom_bar(), “add” coord_polar(\"y\"). Be sure to put plus signs before and after coord_polar(\"y\"). Also, change the labels in labs() if necessary.\n\n\n# PUT YOUR PIE CHART CODE HERE\n\n\nrose diagram: First, copy-and-paste your original bar chart code. Then, after geom_bar(fill = \"darkblue\"), “add” coord_polar() + scale_y_sqrt(). Be sure to put plus signs before and after coord_polar() + scale_y_sqrt(). Also, change the labels in labs() if necessary. After you make the rose diagram: In 1-2 sentences, what do you think scale_y_sqrt() does, and what is a benefit to including scale_y_sqrt() when making the rose diagram?\n\n\n# PUT YOUR ROSE DIAGRAM CODE HERE"
  },
  {
    "objectID": "health/labs/03-visualization.html#notes-on-colors-in-plots",
    "href": "health/labs/03-visualization.html#notes-on-colors-in-plots",
    "title": "Lab: data visualization",
    "section": "Notes on colors in plots",
    "text": "Notes on colors in plots\nThree types of color scales to work with:\n\nQualitative: distinguishing discrete items that don’t have an order (nominal categorical). Colors should be distinct and equal with none standing out unless otherwise desired for emphasis.\n\n\nDo NOT use a discrete scale on a continuous variable\n\n\nSequential: when data values are mapped to one shade, e.g., for an ordered categorical variable or low to high continuous variable\n\n\nDo NOT use a sequential scale on an unordered variable\n\n\nDivergent: think of it as two sequential scales with a natural midpoint midpoint could represent 0 (assuming +/- values) or 50% if your data spans the full scale\n\n\nDo NOT use a divergent scale on data without natural midpoint\n\n\nOptions for ggplot2 colors\nThe default color scheme is pretty bad to put it bluntly, but ggplot2 has ColorBrewer built in which makes it easy to customize your color scales. For instance, we can make a scatterplot with Cost on the y-axis and Duration on the x-axis and using the geom_point() layer with each point colored by Drugs:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Drugs)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Number of drugs\") +\n  theme_light()\n\n\n\n\n\n\n\n\nWhat does alpha change? We can change the color plot for this plot using scale_color_brewer() function:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Drugs)) +\n  geom_point(alpha = 0.5) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Number of drugs\") +\n  theme_light()\n\n\n\n\n\n\n\n\nWhich do you prefer, the default palette or this new one? You can check out more color palettes here.\nSomething you should keep in mind is to pick a color-blind friendly palette. One simple way to do this is by using the ggthemes package (you need to install it first before running this code!) which has color-blind friendly palettes included:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Drugs)) +\n  geom_point(alpha = 0.5) +\n  # call the function directly from the package using `::` instead of library(ggthemes)\n  ggthemes::scale_color_colorblind() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Number of drugs\") +\n  theme_light()\n\n\n\n\n\n\n\n\nIn terms of displaying color from low to high, the viridis scales are excellent choices (and are also color-blind friendly!). For instance, we can map another quantitative variable (Interventions) to the color:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_light()\n\n\n\n\n\n\n\n\nWhat does this reveal about the plot? What happens if you delete scale_color_viridis_c() + from above? Which do you prefer?"
  },
  {
    "objectID": "health/labs/03-visualization.html#notes-on-themes",
    "href": "health/labs/03-visualization.html#notes-on-themes",
    "title": "Lab: data visualization",
    "section": "Notes on themes",
    "text": "Notes on themes\nYou might have noticed above have various changes to the theme of plots for customization. You will constantly be changing the theme of your plots to optimize the display. Fortunately, there are a number of built-in themes you can use to start with rather than the default theme_gray():\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_gray()\n\n\n\n\n\n\n\n\nFor instance, Quang’s go-to theme is theme_light()\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_light()\n\n\n\n\n\n\n\n\nThere are options such as theme_minimal():\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nor theme_classic():\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nor theme_bw():\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThere are also packages with popular, such as the ggthemes package which includes, for example, theme_economist():\n\nlibrary(ggthemes)\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_economist()\n\n\n\n\n\n\n\n\nand theme_fivethirtyeight(), to name a couple:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       color = \"Interventions\") +\n  theme_fivethirtyeight()\n\n\n\n\n\n\n\n\nWith any theme you have picked, you can then modify specific components directly using the theme() layer. There are many aspects of the plot’s theme to modify, such as my decision to move the legend to the bottom of the figure, drop the legend title, and increase the font size for the y-axis:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost, color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       title = \"Joint distribution of patients' duration and cost\",\n       color = \"Interventions\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.text.y = element_text(size = 14),\n        axis.text.x = element_text(size = 6))\n\n\n\n\n\n\n\n\nIf you’re tired of explicitly customizing every plot in the same way all the time, then you should make a custom theme. It’s quite easy to make a custom theme for ggplot2 and of course there are an incredible number of ways to customize your theme. Below, we modify theme_bw() using the %+replace% argument to a new customized theme named theme_cus() - which is stored as a function:\n\ntheme_cus &lt;- function() {\n  # start with the base font size\n  theme_bw(base_size = 10) %+replace%\n    theme(\n      panel.background  = element_blank(),\n      plot.background = element_rect(fill = \"transparent\", color = NA), \n      legend.position = \"bottom\",\n      legend.background = element_rect(fill = \"transparent\", color = NA),\n      legend.key = element_rect(fill = \"transparent\", color = NA),\n      axis.ticks = element_blank(),\n      panel.grid.major = element_line(color = \"grey90\", linewidth = 0.3), \n      panel.grid.minor = element_blank(),\n      plot.title = element_text(size = 15, hjust = 0, vjust = 0.5, face = \"bold\", \n                                margin = margin(b = 0.2, unit = \"cm\")),\n      plot.subtitle = element_text(size = 12, hjust = 0, vjust = 0.5, \n                                   margin = margin(b = 0.2, unit = \"cm\")),\n      plot.caption = element_text(size = 7, hjust = 1, face = \"italic\", \n                                  margin = margin(t = 0.1, unit = \"cm\")),\n      axis.text.x = element_text(size = 13),\n      axis.text.y = element_text(size = 13)\n    )\n}\n\nCreate the plot from before with this theme:\n\nheart_disease |&gt;\n  ggplot(aes(x = Duration, y = Cost,\n             color = Interventions)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Duration\", \n       y = \"Cost\",\n       title = \"Joint distribution of patients' duration and cost\",\n       color = \"Interventions\") +\n  theme_cus()"
  },
  {
    "objectID": "health/labs/01-intro.html",
    "href": "health/labs/01-intro.html",
    "title": "Lab: getting started with R",
    "section": "",
    "text": "NOTE: To preview this file, click the “Render” button in RStudio. (The Shortcut for rendering/knitting in RStudio is Command + Shift + K for macOS users, or Ctrl + Shift + K for Windows users.)"
  },
  {
    "objectID": "health/labs/01-intro.html#typical-workflow",
    "href": "health/labs/01-intro.html#typical-workflow",
    "title": "Lab: getting started with R",
    "section": "Typical workflow",
    "text": "Typical workflow\n\nWriting R scripts\nYou can type R commands directly into the Console (lower left pane), but this can become quite tedious and annoying when your work becomes more complex. Instead, you can code in R Scripts. An R Script is a file type which R recognizes as storing R commands and is saved as a .R file. R Scripts are useful as we can edit our code before sending it to be run in the console.\nIn RStudio, to open a new R Script: File &gt; New File &gt; R Script.\n\n\nUsing Quarto\nAn Quarto file is a dynamic document for writing reproducible reports and communicating results. It contains the reproducible source code along with the narration that a reader needs to understand your work.\nThere are three important elements to a Quarto file:\n\nA YAML header at the top (surrounded by ---)\nChunks of R code surrounded by ```\nText mixed with simple text formatting like ## Heading and italics\n\n(Note that this file itself is a Quarto document.)\nIf you are familiar with the LaTeX syntax, math mode works like a charm in almost the same way:\n\\[\nf (x) = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( - \\frac{x^2}{2} \\right)\n\\]\nA chunk of embedded R code is the following:\n\n# R code here\nprint(\"Hello World\")\n\n[1] \"Hello World\"\n\n\nAll the lab documents will be Quarto files so you need to know how to render and convert them into a reader-friendly documents. We recommend to render as html file but if you have LaTeX installed, you can change the format to pdf.\nFor more details on Quarto, see the comprehensive manual online and the Quarto chapter of R for Data Science (2e). See also the guide on Markdown Basics for more on Markdown syntax. For code chunk options, see this guide."
  },
  {
    "objectID": "health/labs/01-intro.html#installing-r-packages",
    "href": "health/labs/01-intro.html#installing-r-packages",
    "title": "Lab: getting started with R",
    "section": "Installing R packages",
    "text": "Installing R packages\nR performs a wide variety of functions, such as data manipulation, modeling, and visualization. The extensive code base beyond the built-in functions are managed by packages created from numerous statisticians and developers. The Comprehensive R Archive Network (CRAN) manages the open-source distribution and the quality control of the R packages.\nTo install an R package, using the function install.packages and put the package name in the parentheses and the quote. While this is preferred, for those using RStudio, you can also go to “Tools” then “Install Packages” and then input the package name.\n\ninstall.packages(\"tidyverse\")\n\nImportant: NEVER install new packages in a code block in a .qmd file. That is, the install.packages() function should NEVER be in your code chunks (unless they are commented out using #). The library() function, however, will be used throughout your code: The library() function loads packages only after they are installed.\nIf in any time you get a message says: “Do you want to install from sources the package which needs compilation?” Choose “No” will tend to bring less troubles. (Note: This happens when the bleeding-edge version package is available, but not yet compiled for each OS distribution. In many case, you can just proceed without the source compilation.)\nEach package only needs to be installed once. Whenever you want to use functions defined in the package, you need to load the package with the command:\n\nlibrary(tidyverse)\n\nHere is a list of packages that we may need (but not limited to) in the following lectures and/or labs. Make sure you can install all of them. If you fail to install any package, please update R and RStudio first and check the error message for any other packages that need to install first.\n\nlibrary(tidyverse)\nlibrary(devtools)\nlibrary(ranger)\nlibrary(glmnet)"
  },
  {
    "objectID": "health/labs/01-intro.html#basic-data-type-and-operators",
    "href": "health/labs/01-intro.html#basic-data-type-and-operators",
    "title": "Lab: getting started with R",
    "section": "Basic data type and operators",
    "text": "Basic data type and operators\n\nData type: vector\nThe basic unit of R is a vector. A vector is a collection of values of the same type and the type could be:\n\nnumeric (double/integer number): digits with optional decimal point\n\n\nv1 &lt;- c(1, 5, 8.3, 0.02, 99999)\ntypeof(v1)\n\n[1] \"double\"\n\n\n\ncharacter: a string (or word) in double or single quotes, “…” or ’…’.\n\n\nv2 &lt;- c(\"apple\", \"banana\", \"3 chairs\", \"dimension1\", \"&gt;-&lt;\")\ntypeof(v2)\n\n[1] \"character\"\n\n\n\nlogical: TRUE and FALSE\n\n\nv3 &lt;- c(TRUE, FALSE, FALSE)\ntypeof(v3)\n\n[1] \"logical\"\n\n\nNote: Oftentimes, factor is used to encode a character vector into unique numeric vector.\n\ncities &lt;- c(\"Pittsburgh\", \"Chicago\", \"Pittsburgh\", \"Nashville\", \"Chicago\")\ncities &lt;- factor(cities)\nstr(cities)\n\n Factor w/ 3 levels \"Chicago\",\"Nashville\",..: 3 1 3 2 1\n\ntypeof(cities)\n\n[1] \"integer\"\n\n\n\n\nData type: lists\nVector can store only single data type:\n\ntypeof(c(1, TRUE, \"apple\"))\n\n[1] \"character\"\n\n\nList is a vector of vectors which can store different data types of vectors:\n\nroster &lt;- list(\n  name = c(\"Quang\", \"Princess\", \"Julian\", \"Hao\", \"James\"),\n  role = c(\"Instructor\", \"TA\", \"TA\", \"TA\", \"TA\"),\n  is_TA = c(FALSE, TRUE, TRUE, TRUE, TRUE)\n)\nstr(roster)\n\nList of 3\n $ name : chr [1:5] \"Quang\" \"Princess\" \"Julian\" \"Hao\" ...\n $ role : chr [1:5] \"Instructor\" \"TA\" \"TA\" \"TA\" ...\n $ is_TA: logi [1:5] FALSE TRUE TRUE TRUE TRUE\n\n\nR uses a specific type of list, data frame, containing the same number of rows with unique row names.\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\ntypeof(iris)\n\n[1] \"list\"\n\n\n\n\nOperators\nWe can perform element-wise actions on vectors through the operators:\n\narithmetic: +, -, *, /, ^ (for integer division, %/% is quotient, %% is remainder)\n\n\nv1 &lt;- c(1,2,3)\nv2 &lt;- c(4,5,6)\n\nv1 + v2\n\n[1] 5 7 9\n\nv1 * v2\n\n[1]  4 10 18\n\nv2 %% v1\n\n[1] 0 1 0\n\n\n\nrelation: &gt;, &gt;=, &lt; ,&lt;=, ==, !=\n\n\n5 &gt; 4\n\n[1] TRUE\n\n5 &lt;= 4\n\n[1] FALSE\n\n33 == 22\n\n[1] FALSE\n\n33 != 22\n\n[1] TRUE\n\n\n\nlogic: ! (not), & (and), | (or)\n\n\n(5 &gt; 6) | (2 &lt; 3)\n\n[1] TRUE\n\n(5 &gt; 6) & (2 &lt; 3)\n\n[1] FALSE\n\n!(5 &gt; 6) & (2 &lt; 3)\n\n[1] TRUE\n\n\n\nsequence: i:j (: operator, i and j are any two arbitrary numbers)\n\n\n1:5\n\n[1] 1 2 3 4 5\n\n5:1\n\n[1] 5 4 3 2 1\n\n-1:-5\n\n[1] -1 -2 -3 -4 -5\n\n-1:5\n\n[1] -1  0  1  2  3  4  5"
  },
  {
    "objectID": "health/labs/01-intro.html#loading-.csv-files",
    "href": "health/labs/01-intro.html#loading-.csv-files",
    "title": "Lab: getting started with R",
    "section": "Loading .csv files",
    "text": "Loading .csv files\nMost of the data provided to you are in .csv format. In the code chunk below, we use the read_csv() function (from the readr package, part of the tidyverse) to load a dataset that is saved in a folder located in the SURE GitHub repository. In quotations, insert the file path where the dataset is located, which in this case is online. However, typically you’ll save .csv files locally first and put them in an organized folder to access later.\n\nlibrary(tidyverse)\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/heart_disease.csv\")\nhead(nba_stats)"
  },
  {
    "objectID": "health/labs/01-intro.html#looking-for-help",
    "href": "health/labs/01-intro.html#looking-for-help",
    "title": "Lab: getting started with R",
    "section": "Looking for help",
    "text": "Looking for help\nIf you have any R problem, the best step is to use the help() function (or equivalently the ?). For example,\n\nhelp(str)\nhelp(lm)\n\nOr you can use the command ?…\n\n?str\n?lm\n\nDouble question marks can lead to a more general search.\n\n??predict\n\nYou should ALWAYS consult the R help documentation first before attempting to google around (or ask ChatGPT) for a solution."
  },
  {
    "objectID": "health/labs/01-intro.html#exercises",
    "href": "health/labs/01-intro.html#exercises",
    "title": "Lab: getting started with R",
    "section": "Exercises",
    "text": "Exercises\n\nCreate four vectors, v1 and v2 are numeric vectors, v3 is a character vector and v4 is a logic vector. Make sure the length of v1 and v2 are the same. (Hint: a way to check the length is to use the function length())\n\n\n# R code here\n\n\nPreform add, minus, product and division on v1 and v2.\n\n\n# R code here\n\n\nCreate four statements with both relation and logic operators, that 2 of them return TRUE and 2 of them return FALSE.\n\n\n# R code here\n\n\nCreate 2 sequences with length 20, one in an increasing order and the other in a decreasing order.\n\n\n# R code here"
  },
  {
    "objectID": "health/labs/01-intro.html#text-formatting-in-quarto",
    "href": "health/labs/01-intro.html#text-formatting-in-quarto",
    "title": "Lab: getting started with R",
    "section": "Text formatting in Quarto",
    "text": "Text formatting in Quarto\nThere are a lot of ways to format text in a Quarto document, e.g., italics and bold (just scan through this .qmd file to see how this was done). See this guide for more tips/tricks. In particular, check out the Markdown Basics and other guides under Authoring. See also this guide on R code chunk options.\nAs you’ll see throughout this summer (and especially with your project), well-formatted .html files can be a great way to showcase data science results to the public online. ## Customizing RStudio\nRStudio theme\nRStudio can be customized with different themes. To explore built-in themes,\n\nNavigate to the menu bar at the top of your screen\nChoose Tools &gt; Global Options &gt; Appearance\nChange your RStudio theme under Editor theme\n\n(FYI, Quang uses the Tomorrow Night Bright theme.)\nNote that within the Appearance tab, there are also options for changing your Editor font, Editor font size, etc.\nRStudio panes\nWithin RStudio, there are several panes (e.g., Console, Help, Environment, History, Plots, etc.). To customize, go to Tools &gt; Global Options &gt; Pane Layout, and arrange the panes as you see fit.\nFeel free to explore other options within the Tools &gt; Global Option menu."
  },
  {
    "objectID": "sports/eda/wwc_passing.html#overview",
    "href": "sports/eda/wwc_passing.html#overview",
    "title": "EDA project: Women’s World Cup passing",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 5 and conclude with a 6-minute presentation on Tuesday, June 17 during lab.\nStudents will be randomly placed into groups of 2–3 and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/wwc_passing.html#deliverables",
    "href": "sports/eda/wwc_passing.html#deliverables",
    "title": "EDA project: Women’s World Cup passing",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 6-minute presentation. All group members must be present for the presentation and participate. No notes/scripts are allowed during the presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nTwo questions/hypotheses you are interested in exploring\nTwo data visualizations exploring the questions—both must be multivariate (i.e., involving 2+ variables) and in different formats\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "sports/eda/wwc_passing.html#timeline",
    "href": "sports/eda/wwc_passing.html#timeline",
    "title": "EDA project: Women’s World Cup passing",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 12 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review.\nTuesday, June 17 at 1pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "sports/eda/wwc_passing.html#data",
    "href": "sports/eda/wwc_passing.html#data",
    "title": "EDA project: Women’s World Cup passing",
    "section": "Data",
    "text": "Data\nThis dataset contains all (completed) pass attempts from the 2023 FIFA Women’s World Cup, courtesy of StatsBomb and accessed via the StatsBombR package.\nEach row in the dataset corresponds to a pass attempt and the columns are:\n\nperiod: period of the game (1: first half, 2: second half, 3: first extra time period, 4: second extra time period)\n\nminute & second: minute & second of the game\nduration: duration of the pass (seconds)\n\nunder_pressure: whether the pass is under pressure\n\ncounterpress: whether the pass is under a counterpress event (defined as a pressing action occurring within five seconds of an open play turnover)\n\nplay_pattern.name: play pattern\nteam.name: team name\nplayer.name: passer name\n\nposition.name: passer position\npass.{switch, aerial_won, cross, through_ball, ..., no_touch, deflected, miscommunication}: different pass binary indicators (should be self-explanatory, NA means FALSE)\npass.recipient.name: pass recipient name\npass.height.name: type of pass height\npass.body_part.name: body part used to make the pass\npass.type.name: type of pass\npass.outcome.name: outcome of the pass\npass.technique.name: technique of the pass\nlocation.{x, y}: (x, y) location when pass takes place. (Note: all locations have been standardized so that the possession team is going from left to right. Higher x values means closer to the opposing team’s goal, and higher y values means more toward the left wing)\npass.end_location.{x, y}: (x, y) coordinates of pass ending location\npass.length: length of the pass\npass.angle: angle of the pass\nTimeInPoss: time elapsed since the start of possession\n\nTimeToPossEnd: time remaining until end of possession"
  },
  {
    "objectID": "sports/eda/wwc_passing.html#starter-code",
    "href": "sports/eda/wwc_passing.html#starter-code",
    "title": "EDA project: Women’s World Cup passing",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nwwc_passes &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/wwc_passes.csv\")\n\nIn case you’re curious, the code to build this dataset can be found below.\n\nlibrary(tidyverse)\nlibrary(StatsBombR)\n\nwwc &lt;- FreeCompetitions() |&gt; \n  filter(competition_id == \"72\" & season_id == \"107\") |&gt; \n  FreeMatches() |&gt; \n  free_allevents() |&gt; \n  allclean()\n\nwwc_passes &lt;- wwc |&gt;\n  filter(type.name == \"Pass\", !is.na(pass.recipient.name)) |&gt; \n  janitor::remove_empty() |&gt; \n  select(period, minute, second, duration, under_pressure, counterpress, \n         play_pattern.name, team.name, player.name, position.name, \n         pass.switch:pass.through_ball, pass.shot_assist:pass.goal_assist, \n         pass.cut_back:pass.miscommunication, pass.recipient.name, pass.height.name, \n         pass.body_part.name, pass.type.name, pass.outcome.name, pass.technique.name,  \n         location.x, location.y, pass.end_location.x, pass.end_location.y, \n         pass.length, pass.angle, TimeInPoss, TimeToPossEnd)"
  },
  {
    "objectID": "sports/eda/nfl_passing.html#overview",
    "href": "sports/eda/nfl_passing.html#overview",
    "title": "EDA project: NFL passing",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 5 and conclude with a 6-minute presentation on Tuesday, June 17 during lab.\nStudents will be randomly placed into groups of 2–3 and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/nfl_passing.html#deliverables",
    "href": "sports/eda/nfl_passing.html#deliverables",
    "title": "EDA project: NFL passing",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 6-minute presentation. All group members must be present for the presentation and participate. No notes/scripts are allowed during the presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nTwo questions/hypotheses you are interested in exploring\nTwo data visualizations exploring the questions—both must be multivariate (i.e., involving 2+ variables) and in different formats\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "sports/eda/nfl_passing.html#timeline",
    "href": "sports/eda/nfl_passing.html#timeline",
    "title": "EDA project: NFL passing",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 12 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review.\nTuesday, June 17 at 1pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "sports/eda/nfl_passing.html#data",
    "href": "sports/eda/nfl_passing.html#data",
    "title": "EDA project: NFL passing",
    "section": "Data",
    "text": "Data\nThis dataset contains pass attempts from weeks 1–9 of the 2022 NFL regular season, courtesy of the nflreadr package (part of the nflverse) and the 2025 NFL Big Data Bowl.\nEach row in the dataset corresponds to a pass attempt and the columns are:\n\ngame_id: unique identifier for a game\n\nplay_id: unique identifier for a play (within a game)\npasser_player_name: name of the player that attempted the pass\npasser_player_id: unique identifier for the player that attempted the pass\nposteam: abbreviation for the team with possession\ncomplete_pass: indicator denoting whether or not the pass was completed\ninterception: indicator denoting whether or not the pass was intercepted by the defense\nyards_gained: yards gained (or lost) by the possessing team, excluding yards gained via fumble recoveries and laterals\ntouchdown: indicator denoting if the play resulted in a touchdown\npass_location: categorical location of pass\npass_length: categorical length of pass\nair_yards: distance in yards perpendicular to the line of scrimmage at where the targeted receiver either caught or didn’t catch the ball\nyards_after_catch: distance in yards perpendicular to the yard line where the receiver made the reception to where the play ended\nshotgun: indicator for whether or not the play was in shotgun formation\nno_huddle: indicator for whether or not the play was in no_huddle formation\ndefteam: abbreviation for the team on defense\nposteam_type: indicating whether the posteam team is home or away\nyardline_100: distance in the number of yards from the opponent’s endzone for the posteam\nside_of_field: abbreviation for which team’s side of the field the team with possession is currently on\ndown: down for the given play\nqtr: quarter of the game (5 is overtime)\nhalf_seconds_remaining: seconds remaining in the half\ngame_half: indicating which half the play is in\nhome_team: abbreviation for the home team\naway_team: abbreviation for the away team\nhome_score: total points scored by the home team\naway_score: total points scored by the away team\ntarget_player_id: unique identifier for the intended receiver\n\ntarget_player_name: name of the intended receiver\n\nhad_pass_reception: whether the pass was caught by the intended receiver\nmotion_since_lineset: whether the intended receiver went in motion after they were initially set at the line of scrimmage\nroute_ran: route ran by the intended receiver\n\noffense_formation: formation used by the offense\ntarget_x: x coordinate (along the end zone direction) of targeted receiver when the pass arrived (Note: all locations have been standardized so that the possession team is going from left to right. Higher x values means closer to the target end zone)\n\ntarget_y: y coordinate (along the sideline direction) of targeted receiver when the pass arrived\n\ntime_to_throw: time (in seconds) elapsed between snap and pass"
  },
  {
    "objectID": "sports/eda/nfl_passing.html#starter-code",
    "href": "sports/eda/nfl_passing.html#starter-code",
    "title": "EDA project: NFL passing",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nnfl_passes &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/nfl_passes.csv\")"
  },
  {
    "objectID": "sports/eda/ufa_throwing.html#overview",
    "href": "sports/eda/ufa_throwing.html#overview",
    "title": "EDA project: UFA throwing",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 5 and conclude with a 6-minute presentation on Tuesday, June 17 during lab.\nStudents will be randomly placed into groups of 2–3 and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/ufa_throwing.html#deliverables",
    "href": "sports/eda/ufa_throwing.html#deliverables",
    "title": "EDA project: UFA throwing",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 6-minute presentation. All group members must be present for the presentation and participate. No notes/scripts are allowed during the presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nTwo questions/hypotheses you are interested in exploring\nTwo data visualizations exploring the questions—both must be multivariate (i.e., involving 2+ variables) and in different formats\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "sports/eda/ufa_throwing.html#timeline",
    "href": "sports/eda/ufa_throwing.html#timeline",
    "title": "EDA project: UFA throwing",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 12 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review.\nTuesday, June 17 at 1pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "sports/eda/ufa_throwing.html#data",
    "href": "sports/eda/ufa_throwing.html#data",
    "title": "EDA project: UFA throwing",
    "section": "Data",
    "text": "Data\nThis dataset contains throw attempts in the Ultimate Frisbee Association from 2021 to 2024. The Ultimate Frisbee Association (UFA), formerly known as the American Ultimate Disc League (AUDL), is the top professional ultimate league in North America. The data were used in the 2025 SSAC paper by Eberhard, Miller and Sandholz and made available on GitHub.\nEach row in the dataset corresponds to a throw attempt and the columns are:\n\nthrower: thrower name identifier\nthrower_x: (standardized) x coordinate of thrower (position along the sideline direction; 0 represents the middle of the field, positive values denote the left side (with respect to facing the target end zone), and negative values denote the right side)\nthrower_y: y coordinate of thrower (position along the end zone direction; higher y means closer to the target end zone)\nreceiver: receiver name identifier\nreceiver_x: (standardized) x coordinate of receiver (position along the sideline direction; 0 represents the middle of the field, positive values denote the left side (with respect to facing the target end zone), and negative values denote the right side)\nreceiver_y: y coordinate of receiver (position along the end zone direction; higher y means closer to the target end zone)\nreceiver: receiver name identifier\nturnover: whether the throw results in a turnover\npossession_num: possession number within a game\npossession_throw: throw number within a possession\ngame_quarter: quarter within a game\nis_home_team: whether the offense is the home team\nhome_team_score: home team score\naway_team_score: away team score\ngameID: unique identifier for game\nhome_teamID: home team name identifier\naway_teamID: away team name identifier\ntimes: game time remaining (seconds)\nhome_team_win: whether home team wins\nscore_diff: score difference between home and away teams\ngoal: whether the throw results in a goal\nthrow_distance: throw distance (yards)\nx_diff: difference in x coordinates between thrower and receiver\ny_diff: difference in y coordinates between thrower and receiver\n\nthrow_angle: throw angle (radians, 0 means forward, \\(\\pi\\) means backward, positive values mean to the right, and negative values mean to the left of the thrower)"
  },
  {
    "objectID": "sports/eda/ufa_throwing.html#starter-code",
    "href": "sports/eda/ufa_throwing.html#starter-code",
    "title": "EDA project: UFA throwing",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nufa_throws &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/ufa_throws.csv\")\n\nIn case you’re curious, the code to build this dataset can be found below. (Note that the data were originally downloaded from the MoneyPuck site.)\n\nufa_throws &lt;- read_csv(\"https://raw.githubusercontent.com/BradenEberhard/Expected-Throwing-Value/refs/heads/main/data/all_games_1024.csv\")\n\nufa_throws &lt;- ufa_throws |&gt; \n  mutate(\n    goal = ifelse(receiver_y &gt; 100 & turnover == 0, 1, 0),\n    throw_distance = sqrt((receiver_x - thrower_x) ^ 2 + (receiver_y - thrower_y) ^ 2),\n    x_diff = receiver_x - thrower_x,\n    y_diff = receiver_y - thrower_y,\n    throw_angle = atan2(y_diff, x_diff)\n  ) |&gt; \n  select(-quarter_point, -total_points, -current_line)"
  },
  {
    "objectID": "sports/labs/03-visualization.html",
    "href": "sports/labs/03-visualization.html",
    "title": "Lab: data visualization",
    "section": "",
    "text": "Let’s start again by reading in the data from yesterday using the read_csv() function after loading the tidyverse:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/nba_stats.csv\")\n\nRows: 654 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): player, team, position\ndbl (20): age, games, games_started, minutes_played, field_goals, field_goal...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "sports/labs/03-visualization.html#reading-in-data",
    "href": "sports/labs/03-visualization.html#reading-in-data",
    "title": "Lab: data visualization",
    "section": "",
    "text": "Let’s start again by reading in the data from yesterday using the read_csv() function after loading the tidyverse:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/nba_stats.csv\")\n\nRows: 654 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): player, team, position\ndbl (20): age, games, games_started, minutes_played, field_goals, field_goal...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "sports/labs/03-visualization.html#previewing-the-data",
    "href": "sports/labs/03-visualization.html#previewing-the-data",
    "title": "Lab: data visualization",
    "section": "Previewing the data",
    "text": "Previewing the data\nWrite code that displays the column names of nba_stats. Also, look at the first six rows of your dataset to get an idea of what these variables look like. Which variables are quantitative, and which are categorical?\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "sports/labs/03-visualization.html#always-make-a-bar-chart",
    "href": "sports/labs/03-visualization.html#always-make-a-bar-chart",
    "title": "Lab: data visualization",
    "section": "Always make a bar chart…",
    "text": "Always make a bar chart…\nNow we’ll use the ggplot() function to create a bar chart of the position variable. To make things easier, we provide the code for you to do this below; just uncomment the code and run it to create the bar graph. In what follows, you must answer some questions about the code and plot.\n\n# Create the bar graph of position:\n# nba_stats |&gt;\n#   ggplot(aes(x = position)) +\n#   geom_bar(fill = \"darkblue\") +\n#   labs(title = \"Number of NBA players by position\",\n#        x = \"Position\",\n#        y = \"Number of players\",\n#        caption = \"Source: Basketball-Reference.com\")\n\nAnswer the following questions about the code and plot:\n\nIn general, ggplot() code takes the following format: ggplot(blank1, aes(x = blank2)). Looking at the above code, what kind of R object should blank1 be, and what should blank2 be?\nWhat do you think the line geom_bar(fill = \"darkblue\") does?\nWhat do you think the remaining lines of code do (contained in labs())?"
  },
  {
    "objectID": "sports/labs/03-visualization.html#more-area-plots-but-bar-charts-are-better",
    "href": "sports/labs/03-visualization.html#more-area-plots-but-bar-charts-are-better",
    "title": "Lab: data visualization",
    "section": "More area plots (but bar charts are better!)",
    "text": "More area plots (but bar charts are better!)\nNow we’ll make a few other area plots:\n\nspine chart\npie chart\nrose diagram\n\nYour goal for this part is to create each of these plots. These plots can be created by copy-and-pasting the bar chart code from above and modifying it slightly. Follow these directions to create each of these plots:\n\nspine chart: First, copy-and-paste the bar chart code from above. Then, delete the fill = \"darkblue\" within geom_bar(). Finally, within ggplot(), replace aes(x = position) with aes(x = \"\", fill = position). Also, change the labels in labs() if necessary.\n\n\n# PUT YOUR SPINE CHART CODE HERE\n\n\npie chart: First, copy-and-paste the spine chart code you just made. Then, after geom_bar(), “add” coord_polar(\"y\"). Be sure to put plus signs before and after coord_polar(\"y\"). Also, change the labels in labs() if necessary.\n\n\n# PUT YOUR PIE CHART CODE HERE\n\n\nrose diagram: First, copy-and-paste your original bar chart code. Then, after geom_bar(fill = \"darkblue\"), “add” coord_polar() + scale_y_sqrt(). Be sure to put plus signs before and after coord_polar() + scale_y_sqrt(). Also, change the labels in labs() if necessary. After you make the rose diagram: In 1-2 sentences, what do you think scale_y_sqrt() does, and what is a benefit to including scale_y_sqrt() when making the rose diagram?\n\n\n# PUT YOUR ROSE DIAGRAM CODE HERE"
  },
  {
    "objectID": "sports/labs/03-visualization.html#notes-on-colors-in-plots",
    "href": "sports/labs/03-visualization.html#notes-on-colors-in-plots",
    "title": "Lab: data visualization",
    "section": "Notes on colors in plots",
    "text": "Notes on colors in plots\nThree types of color scales to work with:\n\nQualitative: distinguishing discrete items that don’t have an order (nominal categorical). Colors should be distinct and equal with none standing out unless otherwise desired for emphasis.\n\n\nDo NOT use a discrete scale on a continuous variable\n\n\nSequential: when data values are mapped to one shade, e.g., for an ordered categorical variable or low to high continuous variable\n\n\nDo NOT use a sequential scale on an unordered variable\n\n\nDivergent: think of it as two sequential scales with a natural midpoint midpoint could represent 0 (assuming +/- values) or 50% if your data spans the full scale\n\n\nDo NOT use a divergent scale on data without natural midpoint\n\n\nOptions for ggplot2 colors\nThe default color scheme is pretty bad to put it bluntly, but ggplot2 has ColorBrewer built in which makes it easy to customize your color scales. For instance, we can make a scatterplot with three_pointers on the y-axis and offensive_rebounds on the x-axis and using the geom_point() layer with each point colored by position:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = position)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Position\") +\n  theme_light()\n\n\n\n\n\n\n\n\nWhat does alpha change? We can change the color plot for this plot using scale_color_brewer() function:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers,\n             color = position)) +\n  geom_point(alpha = 0.5) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Position\") +\n  theme_light()\n\n\n\n\n\n\n\n\nWhich do you prefer, the default palette or this new one? You can check out more color palettes here.\nSomething you should keep in mind is to pick a color-blind friendly palette. One simple way to do this is by using the ggthemes package (you need to install it first before running this code!) which has color-blind friendly palettes included:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = position)) +\n  geom_point(alpha = 0.5) +\n  # call the function directly from the package using `::` instead of library(ggthemes)\n  ggthemes::scale_color_colorblind() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Position\") +\n  theme_light()\n\n\n\n\n\n\n\n\nIn terms of displaying color from low to high, the viridis scales are excellent choices (and are also color-blind friendly!). For instance, we can map another continuous variable (minutes_played) to the color:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_light()\n\n\n\n\n\n\n\n\nWhat does this reveal about the plot? What happens if you delete scale_color_viridis_c() + from above? Which do you prefer?"
  },
  {
    "objectID": "sports/labs/03-visualization.html#notes-on-themes",
    "href": "sports/labs/03-visualization.html#notes-on-themes",
    "title": "Lab: data visualization",
    "section": "Notes on themes",
    "text": "Notes on themes\nYou might have noticed above have various changes to the theme of plots for customization. You will constantly be changing the theme of your plots to optimize the display. Fortunately, there are a number of built-in themes you can use to start with rather than the default theme_gray():\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_gray()\n\n\n\n\n\n\n\n\nFor instance, Quang’s go-to theme is theme_light()\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_light()\n\n\n\n\n\n\n\n\nThere are options such as theme_minimal():\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nor theme_classic():\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nor theme_bw():\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThere are also packages with popular, such as the ggthemes package which includes, for example, theme_economist():\n\nlibrary(ggthemes)\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_economist()\n\n\n\n\n\n\n\n\nand theme_fivethirtyeight(), to name a couple:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       color = \"Minutes played\") +\n  theme_fivethirtyeight()\n\n\n\n\n\n\n\n\nWith any theme you have picked, you can then modify specific components directly using the theme() layer. There are many aspects of the plot’s theme to modify, such as my decision to move the legend to the bottom of the figure, drop the legend title, and increase the font size for the y-axis:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       title = \"Joint distribution of three-pointers and offensive rebounds\",\n       subtitle = \"NBA statistics from 2024-25 regular season\",\n       color = \"Minutes played\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        axis.text.y = element_text(size = 14),\n        axis.text.x = element_text(size = 6))\n\n\n\n\n\n\n\n\nIf you’re tired of explicitly customizing every plot in the same way all the time, then you should make a custom theme. It’s quite easy to make a custom theme for ggplot2 and of course there are an incredible number of ways to customize your theme. Below, we modify theme_bw() using the %+replace% argument to a new customized theme named theme_cus() - which is stored as a function:\n\ntheme_cus &lt;- function() {\n  # start with the base font size\n  theme_bw(base_size = 10) %+replace%\n    theme(\n      panel.background  = element_blank(),\n      plot.background = element_rect(fill = \"transparent\", color = NA), \n      legend.position = \"bottom\",\n      legend.background = element_rect(fill = \"transparent\", color = NA),\n      legend.key = element_rect(fill = \"transparent\", color = NA),\n      axis.ticks = element_blank(),\n      panel.grid.major = element_line(color = \"grey90\", linewidth = 0.3), \n      panel.grid.minor = element_blank(),\n      plot.title = element_text(size = 15, hjust = 0, vjust = 0.5, face = \"bold\", \n                                margin = margin(b = 0.2, unit = \"cm\")),\n      plot.subtitle = element_text(size = 12, hjust = 0, vjust = 0.5, \n                                   margin = margin(b = 0.2, unit = \"cm\")),\n      plot.caption = element_text(size = 7, hjust = 1, face = \"italic\", \n                                  margin = margin(t = 0.1, unit = \"cm\")),\n      axis.text.x = element_text(size = 13),\n      axis.text.y = element_text(size = 13)\n    )\n}\n\nCreate the plot from before with this theme:\n\nnba_stats |&gt;\n  ggplot(aes(x = offensive_rebounds, y = three_pointers, color = minutes_played)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(x = \"Offensive rebounds\", \n       y = \"Three-pointers\",\n       title = \"Joint distribution of three-pointers and offensive rebounds\",\n       subtitle = \"NBA statistics from 2024-25 regular season\",\n       color = \"Minutes played\") +\n  theme_cus()"
  },
  {
    "objectID": "sports/labs/01-intro.html",
    "href": "sports/labs/01-intro.html",
    "title": "Lab: getting started with R",
    "section": "",
    "text": "NOTE: To preview this file, click the “Render” button in RStudio. (The Shortcut for rendering/knitting in RStudio is Command + Shift + K for macOS users, or Ctrl + Shift + K for Windows users.)"
  },
  {
    "objectID": "sports/labs/01-intro.html#typical-workflow",
    "href": "sports/labs/01-intro.html#typical-workflow",
    "title": "Lab: getting started with R",
    "section": "Typical workflow",
    "text": "Typical workflow\n\nWriting R scripts\nYou can type R commands directly into the Console (lower left pane), but this can become quite tedious and annoying when your work becomes more complex. Instead, you can code in R Scripts. An R Script is a file type which R recognizes as storing R commands and is saved as a .R file. R Scripts are useful as we can edit our code before sending it to be run in the console.\nIn RStudio, to open a new R Script: File &gt; New File &gt; R Script.\n\n\nUsing Quarto\nAn Quarto file is a dynamic document for writing reproducible reports and communicating results. It contains the reproducible source code along with the narration that a reader needs to understand your work.\nThere are three important elements to a Quarto file:\n\nA YAML header at the top (surrounded by ---)\nChunks of R code surrounded by ```\nText mixed with simple text formatting like ## Heading and italics\n\n(Note that this file itself is a Quarto document.)\nIf you are familiar with the LaTeX syntax, math mode works like a charm in almost the same way:\n\\[\nf (x) = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( - \\frac{x^2}{2} \\right)\n\\]\nA chunk of embedded R code is the following:\n\n# R code here\nprint(\"Hello World\")\n\n[1] \"Hello World\"\n\n\nAll the lab documents will be Quarto files so you need to know how to render and convert them into a reader-friendly documents. We recommend to render as html file but if you have LaTeX installed, you can change the format to pdf.\nFor more details on Quarto, see the comprehensive manual online and the Quarto chapter of R for Data Science (2e). See also the guide on Markdown Basics for more on Markdown syntax. For code chunk options, see this guide."
  },
  {
    "objectID": "sports/labs/01-intro.html#installing-r-packages",
    "href": "sports/labs/01-intro.html#installing-r-packages",
    "title": "Lab: getting started with R",
    "section": "Installing R packages",
    "text": "Installing R packages\nR performs a wide variety of functions, such as data manipulation, modeling, and visualization. The extensive code base beyond the built-in functions are managed by packages created from numerous statisticians and developers. The Comprehensive R Archive Network (CRAN) manages the open-source distribution and the quality control of the R packages.\nTo install an R package, using the function install.packages and put the package name in the parentheses and the quote. While this is preferred, for those using RStudio, you can also go to “Tools” then “Install Packages” and then input the package name.\n\ninstall.packages(\"tidyverse\")\n\nImportant: NEVER install new packages in a code block in a .qmd file. That is, the install.packages() function should NEVER be in your code chunks (unless they are commented out using #). The library() function, however, will be used throughout your code: The library() function loads packages only after they are installed.\nIf in any time you get a message says: “Do you want to install from sources the package which needs compilation?” Choose “No” will tend to bring less troubles. (Note: This happens when the bleeding-edge version package is available, but not yet compiled for each OS distribution. In many case, you can just proceed without the source compilation.)\nEach package only needs to be installed once. Whenever you want to use functions defined in the package, you need to load the package with the command:\n\nlibrary(tidyverse)\n\nHere is a list of packages that we may need (but not limited to) in the following lectures and/or labs. Make sure you can install all of them. If you fail to install any package, please update R and RStudio first and check the error message for any other packages that need to install first.\n\nlibrary(tidyverse)\nlibrary(devtools)\nlibrary(ranger)\nlibrary(glmnet)"
  },
  {
    "objectID": "sports/labs/01-intro.html#basic-data-type-and-operators",
    "href": "sports/labs/01-intro.html#basic-data-type-and-operators",
    "title": "Lab: getting started with R",
    "section": "Basic data type and operators",
    "text": "Basic data type and operators\n\nData type: vector\nThe basic unit of R is a vector. A vector is a collection of values of the same type and the type could be:\n\nnumeric (double/integer number): digits with optional decimal point\n\n\nv1 &lt;- c(1, 5, 8.3, 0.02, 99999)\ntypeof(v1)\n\n[1] \"double\"\n\n\n\ncharacter: a string (or word) in double or single quotes, “…” or ’…’.\n\n\nv2 &lt;- c(\"apple\", \"banana\", \"3 chairs\", \"dimension1\", \"&gt;-&lt;\")\ntypeof(v2)\n\n[1] \"character\"\n\n\n\nlogical: TRUE and FALSE\n\n\nv3 &lt;- c(TRUE, FALSE, FALSE)\ntypeof(v3)\n\n[1] \"logical\"\n\n\nNote: Oftentimes, factor is used to encode a character vector into unique numeric vector.\n\nplayer_type &lt;- c(\"Batter\", \"Batter\", \"Hitter\", \"Batter\", \"Hitter\")\nplayer_type &lt;- factor(player_type)\nstr(player_type)\n\n Factor w/ 2 levels \"Batter\",\"Hitter\": 1 1 2 1 2\n\ntypeof(player_type)\n\n[1] \"integer\"\n\n\n\n\nData type: lists\nVector can store only single data type:\n\ntypeof(c(1, TRUE, \"apple\"))\n\n[1] \"character\"\n\n\nList is a vector of vectors which can store different data types of vectors:\n\nroster &lt;- list(\n  name = c(\"Quang\", \"Yuchen\", \"Sara\", \"Erin\", \"Leigh\"),\n  role = c(\"Instructor\", \"TA\", \"TA\", \"TA\", \"TA\"),\n  is_TA = c(FALSE, TRUE, TRUE, TRUE, TRUE)\n)\nstr(roster)\n\nList of 3\n $ name : chr [1:5] \"Quang\" \"Yuchen\" \"Sara\" \"Erin\" ...\n $ role : chr [1:5] \"Instructor\" \"TA\" \"TA\" \"TA\" ...\n $ is_TA: logi [1:5] FALSE TRUE TRUE TRUE TRUE\n\n\nR uses a specific type of list, data frame, containing the same number of rows with unique row names.\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\ntypeof(iris)\n\n[1] \"list\"\n\n\n\n\nOperators\nWe can perform element-wise actions on vectors through the operators:\n\narithmetic: +, -, *, /, ^ (for integer division, %/% is quotient, %% is remainder)\n\n\nv1 &lt;- c(1,2,3)\nv2 &lt;- c(4,5,6)\n\nv1 + v2\n\n[1] 5 7 9\n\nv1 * v2\n\n[1]  4 10 18\n\nv2 %% v1\n\n[1] 0 1 0\n\n\n\nrelation: &gt;, &gt;=, &lt; ,&lt;=, ==, !=\n\n\n5 &gt; 4\n\n[1] TRUE\n\n5 &lt;= 4\n\n[1] FALSE\n\n33 == 22\n\n[1] FALSE\n\n33 != 22\n\n[1] TRUE\n\n\n\nlogic: ! (not), & (and), | (or)\n\n\n(5 &gt; 6) | (2 &lt; 3)\n\n[1] TRUE\n\n(5 &gt; 6) & (2 &lt; 3)\n\n[1] FALSE\n\n!(5 &gt; 6) & (2 &lt; 3)\n\n[1] TRUE\n\n\n\nsequence: i:j (: operator, i and j are any two arbitrary numbers)\n\n\n1:5\n\n[1] 1 2 3 4 5\n\n5:1\n\n[1] 5 4 3 2 1\n\n-1:-5\n\n[1] -1 -2 -3 -4 -5\n\n-1:5\n\n[1] -1  0  1  2  3  4  5"
  },
  {
    "objectID": "sports/labs/01-intro.html#loading-.csv-files",
    "href": "sports/labs/01-intro.html#loading-.csv-files",
    "title": "Lab: getting started with R",
    "section": "Loading .csv files",
    "text": "Loading .csv files\nMost of the data provided to you are in .csv format. In the code chunk below, we use the read_csv() function (from the readr package, part of the tidyverse) to load a dataset that is saved in a folder located in the SURE GitHub repository. In quotations, insert the file path where the dataset is located, which in this case is online. However, typically you’ll save .csv files locally first and put them in an organized folder to access later.\n\nlibrary(tidyverse)\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/nba_stats.csv\")\nhead(nba_stats)"
  },
  {
    "objectID": "sports/labs/01-intro.html#looking-for-help",
    "href": "sports/labs/01-intro.html#looking-for-help",
    "title": "Lab: getting started with R",
    "section": "Looking for help",
    "text": "Looking for help\nIf you have any R problem, the best step is to use the help() function (or equivalently the ?). For example,\n\nhelp(str)\nhelp(lm)\n\nOr you can use the command ?…\n\n?str\n?lm\n\nDouble question marks can lead to a more general search.\n\n??predict\n\nYou should ALWAYS consult the R help documentation first before attempting to google around (or ask ChatGPT) for a solution."
  },
  {
    "objectID": "sports/labs/01-intro.html#exercises",
    "href": "sports/labs/01-intro.html#exercises",
    "title": "Lab: getting started with R",
    "section": "Exercises",
    "text": "Exercises\n\nCreate four vectors, v1 and v2 are numeric vectors, v3 is a character vector and v4 is a logic vector. Make sure the length of v1 and v2 are the same. (Hint: a way to check the length is to use the function length())\n\n\n# R code here\n\n\nPreform add, minus, product and division on v1 and v2.\n\n\n# R code here\n\n\nCreate four statements with both relation and logic operators, that 2 of them return TRUE and 2 of them return FALSE.\n\n\n# R code here\n\n\nCreate 2 sequences with length 20, one in an increasing order and the other in a decreasing order.\n\n\n# R code here"
  },
  {
    "objectID": "sports/labs/01-intro.html#text-formatting-in-quarto",
    "href": "sports/labs/01-intro.html#text-formatting-in-quarto",
    "title": "Lab: getting started with R",
    "section": "Text formatting in Quarto",
    "text": "Text formatting in Quarto\nThere are a lot of ways to format text in a Quarto document, e.g., italics and bold (just scan through this .qmd file to see how this was done). See this guide for more tips/tricks. In particular, check out the Markdown Basics and other guides under Authoring. See also this guide on R code chunk options.\nAs you’ll see throughout this summer (and especially with your project), well-formatted .html files can be a great way to showcase data science results to the public online. ## Customizing RStudio\nRStudio theme\nRStudio can be customized with different themes. To explore built-in themes,\n\nNavigate to the menu bar at the top of your screen\nChoose Tools &gt; Global Options &gt; Appearance\nChange your RStudio theme under Editor theme\n\n(FYI, Quang uses the Tomorrow Night Bright theme.)\nNote that within the Appearance tab, there are also options for changing your Editor font, Editor font size, etc.\nRStudio panes\nWithin RStudio, there are several panes (e.g., Console, Help, Environment, History, Plots, etc.). To customize, go to Tools &gt; Global Options &gt; Pane Layout, and arrange the panes as you see fit.\nFeel free to explore other options within the Tools &gt; Global Option menu."
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Date\nTitle\nMaterials\n\n\n\n\nLecture 0\nJune 2\nWelcome to SURE 2025"
  },
  {
    "objectID": "lectures.html#contents",
    "href": "lectures.html#contents",
    "title": "Lectures",
    "section": "",
    "text": "Date\nTitle\nMaterials\n\n\n\n\nLecture 0\nJune 2\nWelcome to SURE 2025"
  },
  {
    "objectID": "health.html",
    "href": "health.html",
    "title": "Health",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "git-setup.html#step-1-create-a-github-account",
    "href": "git-setup.html#step-1-create-a-github-account",
    "title": "Git and GitHub Setup",
    "section": "Step 1: Create a GitHub account",
    "text": "Step 1: Create a GitHub account\nRegister for a GitHub account at https://github.com if you do not have one. It is completely free to use. You may use any username you prefer; we will later ask for your username so we can keep track."
  },
  {
    "objectID": "git-setup.html#step-2-install-git",
    "href": "git-setup.html#step-2-install-git",
    "title": "Git and GitHub Setup",
    "section": "Step 2: Install Git",
    "text": "Step 2: Install Git\n(Windows)\n\nGo to https://git-scm.com/download/win\nNavigate to “Click here to download” on the first line and click on it\nFollow the installation instructions\n\n(macOS)\n\nOpen the Terminal app on your computer (Finder \\(\\rightarrow\\) Applications \\(\\rightarrow\\) Terminal)\nGo to https://brew.sh and copy/paste the chunk under “Install Homebrew” to the Terminal\nOnce Homebrew is installed, type this into the Terminal: brew install git"
  },
  {
    "objectID": "git-setup.html#step-3-configure-git",
    "href": "git-setup.html#step-3-configure-git",
    "title": "Git and GitHub Setup",
    "section": "Step 3: Configure Git",
    "text": "Step 3: Configure Git\n\nAfter installation, you need to configure Git. This can be done directly in R:\n\n\n# uncomment and run the following line to install the usethis package\n# install.packages(\"usethis\")\nusethis::use_git_config(user.name = \"Your Name\", \n                        user.email = \"your-github@email.address\")\n\n\nUse your full name for the user.name field and the same email as your GitHub account for user.email\nYou then need to create a personal access token for authentication as follows:\n\n\nusethis::create_github_token()\n\n\nThis will direct you to the GitHub site on your browser (you may have to log in). On this site:\n\nUnder “Note”, type in some description for this token (e.g., “Summer 2025 GitHub token”)\nFor “Expiration”, set an expiration date for this token (e.g., 90 days) or make it permanent (i.e. choose “No expiration” if you don’t want to deal with this again in the future)\nUnder “Select scopes”, recommended scopes will be pre-selected. Stick with these for now.\n\nNext, click on “Generate token”\nCopy the token to your clipboard (or leave the browser window open, so you can come back to copy the token later)\nIn RStudio, run the following to get a prompt where you can paste your token:\n\n\n# uncomment and run the following line to install the gitcreds package\n# install.packages(\"gitcreds\")\ngitcreds::gitcreds_set()\n\nYou should then be ready to use GitHub!"
  },
  {
    "objectID": "git-setup.html#step-4-create-a-github-repository",
    "href": "git-setup.html#step-4-create-a-github-repository",
    "title": "Git and GitHub Setup",
    "section": "Step 4: Create a GitHub repository",
    "text": "Step 4: Create a GitHub repository\nWe will follow the paradigm of “GitHub first”. What this means is that when we create a repository, we will create it on GitHub first, then link a local repository to it from inside RStudio.\nAfter you’ve logged in, to create a GitHub repository\n\nGo to https://github.com/new\nName the repository (give it a meaningful name)\nGive the repository a description (don’t leave this blank although this is optional)\nDecide whether to keep the repository public or private (for now, let’s just keep it public)\nClick on “Initialize this repository with a README”. For now, there’s no need to “Add .gitignore” or “Choose a license”\nClick on “Create Repository”"
  },
  {
    "objectID": "git-setup.html#step-5-connect-rstudio-to-the-github-repository",
    "href": "git-setup.html#step-5-connect-rstudio-to-the-github-repository",
    "title": "Git and GitHub Setup",
    "section": "Step 5: Connect RStudio to the GitHub repository",
    "text": "Step 5: Connect RStudio to the GitHub repository\n\nGo to the browser page for your GitHub repository\nClick on Code (in the same line as “Go to file”). Under HTTPS, copy the URL\nIn RStudio, click on File &gt; New Project.... Next, click on “Version Control” and then on “Git”. Paste the URL you just copied into “Repository URL”\nType the name for the folder on your computer associated with this repository into Project directory name\n\nYou can choose whatever name you want, but it is recommended to give a name similar to the repository name on GitHub\n\nMake sure “Create project as subdirectory of:” points to where you want to locate this new folder\nClick on “Create Project”\nAt this point, you should find that the “Files” pane (in the bottom right of RStudio) is listing the files in your local repository."
  },
  {
    "objectID": "git-setup.html#step-6-modify-the-repository",
    "href": "git-setup.html#step-6-modify-the-repository",
    "title": "Git and GitHub Setup",
    "section": "Step 6: Modify the repository",
    "text": "Step 6: Modify the repository\nTo add a new file from your local repository to GitHub:\n\nIn RStudio, open a new file (could be anything - e.g. R Script, Quarto document, etc.).\nFill the file with some code/comments/etc. (This is just for illustration purpose, to show how you can add a file to GitHub from your computer)\nSave the file. At this point, this file should show up in the “Git” pane (in the top right of RStudio)\nCheck the box under “Staged” in the Git pane to stage the file for a commit\nClick on “Commit” in the Git pane\nIn the new window that opens, add a “Commit message”, then click on the “Commit” button\nClick on “Push” to push your changes from your local repository to the remote repository on GitHub\n\nIf you encountered no errors then you’re done! While working on a single project you will repeatedly perform the tasks in Step 6: make changes to files, commit changes, then push changes\nEvery time you want to create a new repository, you can just start with Step 4, use GitHub, copy the repository into RStudio, then repeatedly update, commit, and push.\nAsk us for help if you have any questions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SURE 2025",
    "section": "",
    "text": "Welcome to the Summer Undergraduate Research Experience in Statistics (SURE) 2025, hosted by the Department of Statistics & Data Science at Carnegie Mellon University. This program focuses on statistics and data science methodology with applications in healthcare and sports analytics.\nOn this site, the Lectures tab contains all lecture slides. The Health and Sports tabs contain materials for health and sports camps, respectively."
  },
  {
    "objectID": "sports/labs/02-wrangling.html",
    "href": "sports/labs/02-wrangling.html",
    "title": "Lab: data wrangling",
    "section": "",
    "text": "Our data are usually stored as a .csv file and after loading a .csv file into RStudio, we will have a “data frame”. A data frame can be considered a special case of matrix where each column represents a measurement or variable of interest for each observation which correspond to the rows of the dataset. After loading the tidyverse suite of packages, we use the read_csv() function to load the 2025 NBA regular season stats dataset from yesterday:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/nba_stats.csv\")\n\nRows: 654 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): player, team, position\ndbl (20): age, games, games_started, minutes_played, field_goals, field_goal...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBy default, read_csv() reads in the dataset as a tbl (aka tibble) object instead of a data.frame object. You can read about the differences here, but it’s not that meaningful for purposes.\nWe can use the functions slice_head() and slice_tail() to view a sample of the data. Use the slice_head() function to view the first 6 rows, then use the slice_tail() function to view the last 3 rows:\n\n# INSERT CODE HERE\n\nView the dimensions of the data with dim():\n\n# INSERT CODE HERE\n\nQuickly view summary statistics for all variables with the summary() function:\n\n# Uncomment the following code by deleting the # at the front\n# summary(nba_stats)\n\nView the data structure types with str():\n\n# str(nba_stats)\n\nWhat’s the difference between the output from the two functions?"
  },
  {
    "objectID": "sports/labs/02-wrangling.html#reading-and-previewing-data",
    "href": "sports/labs/02-wrangling.html#reading-and-previewing-data",
    "title": "Lab: data wrangling",
    "section": "",
    "text": "Our data are usually stored as a .csv file and after loading a .csv file into RStudio, we will have a “data frame”. A data frame can be considered a special case of matrix where each column represents a measurement or variable of interest for each observation which correspond to the rows of the dataset. After loading the tidyverse suite of packages, we use the read_csv() function to load the 2025 NBA regular season stats dataset from yesterday:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnba_stats &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/nba_stats.csv\")\n\nRows: 654 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): player, team, position\ndbl (20): age, games, games_started, minutes_played, field_goals, field_goal...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBy default, read_csv() reads in the dataset as a tbl (aka tibble) object instead of a data.frame object. You can read about the differences here, but it’s not that meaningful for purposes.\nWe can use the functions slice_head() and slice_tail() to view a sample of the data. Use the slice_head() function to view the first 6 rows, then use the slice_tail() function to view the last 3 rows:\n\n# INSERT CODE HERE\n\nView the dimensions of the data with dim():\n\n# INSERT CODE HERE\n\nQuickly view summary statistics for all variables with the summary() function:\n\n# Uncomment the following code by deleting the # at the front\n# summary(nba_stats)\n\nView the data structure types with str():\n\n# str(nba_stats)\n\nWhat’s the difference between the output from the two functions?"
  },
  {
    "objectID": "sports/labs/02-wrangling.html#data-manipulation-with-dplyr",
    "href": "sports/labs/02-wrangling.html#data-manipulation-with-dplyr",
    "title": "Lab: data wrangling",
    "section": "Data manipulation with dplyr",
    "text": "Data manipulation with dplyr\nAn easier way to manipulate the data frame is through the dplyr package, which is in the tidyverse suite of packages. The operations we can do include: selecting specific columns, filtering for rows, re-ordering rows, adding new columns and summarizing data. The “split-apply-combine” concept can be achieved by dplyr.\n\nSelecting columns with select()\nThe function select() can be use to select certain column with the column names. First create a new table called nba_stats_pg that only contains the player and games columns:\n\n# INSERT CODE HERE\n\nTo select all columns except a specific column, use the - (subtraction) operator. For example, view the output from uncommenting the following line of code:\n\n# select(nba_stats, -player)\n\nTo select a range of columns by name (that are in consecutive order), use the : (colon) operator. For example, view the output from uncommenting the following line of code:\n\n# select(nba_stats, player:games)\n\nTo select all columns that start with certain character strings, use the function starts_with(). Other matching options are:\n\nends_with(): select columns that end with a character string\ncontains(): select columns that contain a character string\nmatches(): select columns that match a regular expression\none_of(): select columns names that are from a group of names\n\n\n# Uncomment the following lines of code\n# select(nba_stats, starts_with(\"three\"))\n# select(nba_stats, contains(\"throw\"))\n\n\n\nExtracting rows using filter()\nWe can also extract the rows/observations that satisfy certain criteria. Try extracting the rows with more than 500 assists:\n\n# INSERT CODE HERE\n\nWe can also filter on multiple criteria. Subset the rows with age above 30 and the team is either “HOU” or “GSW”:\n\n# INSERT CODE HERE\n\n\n\nArranging rows using arrange()\nTo arrange the data frame by a specific order we need to use the function arrange(). The default is by increasing order and the desc() function will provide the decreasing order. First arrange the nba_stats table by personal_fouls in ascending order:\n\n# INSERT CODE HERE\n\nNext by descending order:\n\n# INSERT CODE HERE\n\nTry combining a pipeline of select(), filter(), and arrange() steps together with the |&gt; operator by:\n\nSelecting the player, team, age, and games columns,\nFilter to select only rows with games above 50,\nSort by age in descending order\n\n\n# INSERT CODE HERE\n\n\n\nCreating new columns using mutate()\nSometimes the data does not include the variable that we are interested in and we need to manipulate the current variables to add new variables into the data frame. Create a new column fouls_per_game by taking the personal_fouls and dividing by games (reassign this output to the nba_stats table following the commented code chunk so this column is added to the table):\n\n# nba_stats &lt;- nba_stats |&gt;\n#   mutate(INSERT CODE HERE)\n\n\n\nCreating summaries with summarize()\nTo create summary statistics for a given column in the data frame, we can use summarize() function. Compute the mean, min, and max number of assists:\n\n# INSERT CODE HERE\n\nThe advantage of summarize() is more obvious if we combine it with group_by(), the group operators. Since players at the different position tend to have very different statistics, first group_by() position and then compute the same summary statistics for assists:\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "sports/labs/04-github.html",
    "href": "sports/labs/04-github.html",
    "title": "Using GitHub for project collaboration",
    "section": "",
    "text": "The purpose of this lab is to help you set up Git and GitHub on your computer, and use them to collaborate on your EDA project."
  },
  {
    "objectID": "sports/labs/04-github.html#goal",
    "href": "sports/labs/04-github.html#goal",
    "title": "Using GitHub for project collaboration",
    "section": "",
    "text": "The purpose of this lab is to help you set up Git and GitHub on your computer, and use them to collaborate on your EDA project."
  },
  {
    "objectID": "sports/labs/04-github.html#task-0-github-registration-and-git-installation",
    "href": "sports/labs/04-github.html#task-0-github-registration-and-git-installation",
    "title": "Using GitHub for project collaboration",
    "section": "Task 0: GitHub registration and Git installation",
    "text": "Task 0: GitHub registration and Git installation\n(Note: You were already being asked to complete these prior to the start of the program. Proceed to Task 1 if you already completed this task.)\nGitHub account. Register for a (free) GitHub account at https://github.com.  (if you already have a GitHub account, feel free to ignore this)\nDownload Git.  (if Git is already installed on your computer, the following instructions still hold for updating Git to its latest version)\n(Windows)\n\nGo to https://git-scm.com/download/win\nNavigate to “Click here to download” on the first line and click on it\nFollow the installation instructions\n\n(macOS)\n\nOpen the Terminal app on your computer (Finder \\(\\rightarrow\\) Applications \\(\\rightarrow\\) Terminal)\nGo to https://brew.sh and copy/paste the chunk under “Install Homebrew” to the Terminal\nOnce Homebrew is installed, type this into the Terminal: brew install git"
  },
  {
    "objectID": "sports/labs/04-github.html#task-1-git-configuration",
    "href": "sports/labs/04-github.html#task-1-git-configuration",
    "title": "Using GitHub for project collaboration",
    "section": "Task 1: Git configuration",
    "text": "Task 1: Git configuration\n\nMake sure you’ve already (i) created a GitHub account and (ii) installed Git on your computer\nYou then need to configure Git. This can be done directly in R:\n\n\n# uncomment and run the following line to install the usethis package\n# install.packages(\"usethis\")\nusethis::use_git_config(user.name = \"Your Name\", \n                        user.email = \"your-github@email.address\")\n\n\nUse your full name for the user.name field and the same email as your GitHub account for user.email\nYou then need to create a personal access token for authentication as follows:\n\n\nusethis::create_github_token()\n\n\nThis will direct you to the GitHub site on your browser (you may have to log in). On this site:\n\nUnder “Note”, type in some description for this token (e.g., “Summer 2025 GitHub token”)\nFor “Expiration”, set an expiration date for this token (e.g., 90 days) or make it permanent (i.e. choose “No expiration” if you don’t want to deal with this again in the future)\nUnder “Select scopes”, recommended scopes will be pre-selected. Stick with these for now.\n\nNext, click on “Generate token”\nCopy the token to your clipboard (or leave the browser window open, so you can come back to copy the token later)\nIn RStudio, run the following to get a prompt where you can paste your token:\n\n\n# uncomment and run the following line to install the gitcreds package\n# install.packages(\"gitcreds\")\ngitcreds::gitcreds_set()"
  },
  {
    "objectID": "sports/labs/04-github.html#task-2-eda-project-collaboration-with-git-and-github",
    "href": "sports/labs/04-github.html#task-2-eda-project-collaboration-with-git-and-github",
    "title": "Using GitHub for project collaboration",
    "section": "Task 2: EDA project collaboration with Git and GitHub",
    "text": "Task 2: EDA project collaboration with Git and GitHub\nMake sure every group member finishes Task 1 before proceeding to Task 2, which requires a group effort.\n\nStep 1: Create an EDA project repository on GitHub(Required for ONE group member only)\nEach group should elect ONE person to create a GitHub repository for the EDA project. This repository is to be shared among all group members.\nThe elected group member should do the following to create a new GitHub repository:\n\nAfter you’ve signed in to GitHub, go to https://github.com/new\nName the repository (give it a meaningful name)\nGive the repository a description (don’t leave this blank although this is optional)\nDecide whether to keep the repository public or private (for now, keep it public, so that we can review your code)\nClick on “Initialize this repository with a README”. For now, there’s no need to “Add .gitignore” or “Choose a license”\nClick on “Create Repository”\n\nNext, to add the rest of your group to the repository:\n\nGo to browser page of the GitHub repository you just created and click on “Settings”\nNavigate to the left sidebar and click on “Collaborators”\nClick on “Add people” (under Manage access). Enter the GitHub username for the other group members.\n\n\n\nStep 2: Clone the remote repository to your local computer(Required for ALL group members)\n\nEveryone (except the member responsible for creating the repository) should each get an invitation sent to the email associated with your GitHub account\nCheck your email and accept the invitation\nGo to the browser page for the EDA project GitHub repository (created in Step 1)\nClick on Code (in the same line as “Go to file”). Under HTTPS, copy the URL\nIn RStudio, click on File &gt; New Project.... Next, click on “Version Control” and then on “Git”. Paste the URL you just copied into “Repository URL”\nType the name for the folder on your computer associated with this repository into Project directory name\n\nYou can choose whatever name you want, but it is recommended to give a name similar to the repository name on GitHub\n\nMake sure “Create project as subdirectory of:” points to where you want to locate this new folder\nClick on “Create Project”\nAt this point, you should find that the “Files” pane (in the bottom right of RStudio) is listing the files in your local repository.\n\n\n\nStep 3: Modify the repository (Required for ALL group members)\nEach group member should create their own “sandbox” folder locally on their own computer as follows:\n\nNavigate to the Files pane in RStudio and click on “Folder” to create a new folder (for the new folder name, use your last name.)\nIn RStudio, open a new file (could be anything - e.g. R Script, Quarto document, etc.). Fill the file with some code/comments/etc. (This is just for illustration purpose, to show how you can add a file to GitHub from your computer)\nSave the file inside the folder you just created (with your last name as the folder name). At this point, this file should show up in the “Git” pane (in the top right of RStudio)\nCheck the box under “Staged” in the Git pane to stage the file for a commit\nClick on “Commit” in the Git pane\nIn the new window that opens, add a “Commit message”, then click on the “Commit” button\nClick on “Push” to push your changes from your local repository to the shared remote repository on GitHub\n\n\n\nStep 4: Update the local repository (Required for ALL group members)\n\nFirst, make sure that everyone in your group have completed Step 3\nIn RStudio, navigate to the Git pane and click on “Pull”. A new window will pop up. Once everything is finished running, close the window.\nAt this point, you should find that your Files pane in RStudio is listing the folders that your group members have created, in addition to your own folder\n\nThis task is know as git pull, which updates the local repository to match that content of a shared remote repository\n\n\n\n\nStep 5: Start your EDA project\nIf you encountered no errors then you can feel free to start brainstorming your EDA project with your group.\nFor this project, we ask you to create/update/save files within your own sandbox folder (that you created in Step 3). This will help mitigate the risk of running into trouble when pushing your files to GitHub, especially for those who are new to Git and GitHub. This also allows us to easily review your code.\n\n\n\n\n\n\nImportant notes\n\n\n\nThe GitHub procedure for any project collaboration is\n\nPull new changes\nMake changes on your computer (e.g. create new files, update existing files)\nCommit your local changes (Note: this step may be repeated)\nPull again to avoid merge conflicts\nPush your commit(s) to GitHub\n\nAdvices: Make small, frequent commits. ALWAYS pull before you push.\n\n\nAsk us for help if you run into any issues or have any questions."
  },
  {
    "objectID": "sports/eda/mlb_batting.html#overview",
    "href": "sports/eda/mlb_batting.html#overview",
    "title": "EDA project: NHL shots",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 5 and conclude with an 8-minute presentation on Tuesday, June 17 during lab.\nStudents will be randomly placed into groups of 2–3 and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/mlb_batting.html#deliverables",
    "href": "sports/eda/mlb_batting.html#deliverables",
    "title": "EDA project: NHL shots",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 8-minute presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nThree questions/hypotheses you are interested in exploring\nThree data visualizations exploring the questions, at least two of which must be multivariate. Each visualization must be in a different format from the other two, and you must have at least one categorical and one continuous visualization\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations\n\nNo notes/scripts are allowed during the presentation."
  },
  {
    "objectID": "sports/eda/mlb_batting.html#timeline",
    "href": "sports/eda/mlb_batting.html#timeline",
    "title": "EDA project: NHL shots",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 12 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review.\nTuesday, June 17 at 1pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "sports/eda/mlb_batting.html#data",
    "href": "sports/eda/mlb_batting.html#data",
    "title": "EDA project: NHL shots",
    "section": "Data",
    "text": "Data\nThis dataset contains batted balls from the 2024 MLB regular season (from April 3, 2024 (first day of swing tracking data) onward), courtesy of Baseball Savant and accessed via the sabRmetrics package.\nEach row of the dataset corresponds to a batted ball and the columns are:\n\nbatter_name: name of the batter (Last name, First name)\nbatter_id: unique identifier for the batter\nbatter_side: handedness of hitter, either L (left) or R (right)\nbb_type: batted ball type\npitcher_id: unique identifier for the pitcher\npitch_number: total pitch number of the plate appearance\npitch_type: type of pitch thrown (using Statcast’s algorithm)\npitch_name: full name of the pitch type\npitch_hand: handedness of pitcher\nevents: categorical event denoting the outcome of the batted ball\nhit_coord_x: horizontal coordinate of where a batted ball is first touched by a fielder\nhit_coord_y: vertical coordinate of where a batted ball is first touched by a fielder (note: multiply this by -1 when plotting)\nhit_distance_sc: distance of the batted ball in feet according to Statcast\nlaunch_speed: exit velocity of the ball off the bat (mph)\nlaunch_angle: the vertical angle of the ball off the bat measured from a line parallel to the ground\nbat_speed: speed at the “sweet spot” of the bat, taking the average of the top 90% of a batter’s swings (mph)\nswing_length: distance traveled by bat head from the start of a swing until the impact point (feet)\narm_angle: horizontal line extending from the location of the pitcher’s throwing shoulder and the location of ball at the time of the pitch (0° means perfectly horizontal to the flat ground (a sidearmer), while 90° means straight over the top)\nhit_location: positional number of the player who fielded the ball, possible values are 1-9 for each player (see here)\nrelease_speed: speed of the pitch measured when the ball is released (mph)\neffective_speed: perceived velocity of the ball, i.e., the velocity of the pitch is adjusted for how close it is to home when it is released (mph)\nstrike_zone_{top, bottom}: top/bottom of the batter’s strike zone set by the operator when the ball is halfway to the plate\nrelease_spin_rate: total spin rate of pitch after it is released (rpm)\nextension: how far from the rubber the ball is when it is released (feet)\na{x, y, z}: acceleration of pitch in {x, y, z} dimensions, determined at y = 50 feet (ft/s\\(^2\\))\nv{x0, y0, z0}: velocity of pitch in {x, y, z} dimensions, determined at y = 50 feet (ft/s)\nrelease_pos_y: release position of the ball from the catcher’s perspective (feet)\nrelease_pos_{x, z}: horizontal/vertical release position of the ball from the catcher’s perspective (feet)\nplate_{x, z}: horizontal/vertical position of the ball when it crosses home plate from the catcher’s perspective\npfx_{x, z}: horizontal/vertical movement from the catcher’s perspective (feet)\nif_fielding_alignment: type of infield shift by defense\nof_fielding_alignment: type of outfield shift by defense\ngame_date: date of the game (mm/dd/yyyy)\nballs: number of balls in the count\nstrikes: number of strikes in the count\nouts: number of outs when the batter is up\npre_runner_1b_id: unique identifier for a runner on first base (if there is one)\npre_runner_2b_id: unique identifier for a runner on second base (if there is one)\npre_runner_3b_id: unique identifier for a runner on third base (if there is one)\ninning: the inning number\ninning_topbot: top or bottom of the inning\nhome_score: home team score before batted ball\naway_score: away team score before batted ball\n\npost_home_score: home team score after batted ball\npost_away_score: away team score after batted ball\n\ndes: description of the batted ball and play\n\nNote that a full glossary of the features available from MLB Statcast data can be found here."
  },
  {
    "objectID": "sports/eda/mlb_batting.html#starter-code",
    "href": "sports/eda/mlb_batting.html#starter-code",
    "title": "EDA project: NHL shots",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nmlb_batted_balls &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/mlb_batted_balls.csv\")\n\nIn case you’re curious, the code to build this dataset can be found below.\n\n# devtools::install_github(repo = \"saberpowers/sabRmetrics\")\nlibrary(tidyverse)\nlibrary(sabRmetrics)\nmlb_batted_balls &lt;- download_baseballsavant(start_date = \"2024-04-03\",\n                                            end_date = \"2024-12-31\")\n\nmlb_batted_balls |&gt; \n  filter(type == \"X\") |&gt; \n  select(batter_name, batter_id, bat_side, bb_type,\n         pitcher_id, pitch_number, pitch_type, pitch_name, pitch_hand,\n         events, hit_coord_x, hit_coord_y, launch_speed, launch_angle,\n         bat_speed, swing_length, arm_angle, hit_location, \n         release_speed, effective_speed,\n         strike_zone_top, strike_zone_bottom,\n         release_spin_rate, extension,\n         ax, ay, az, vx0, vy0, vz0, \n         release_pos_x, release_pos_y, release_pos_z,\n         plate_x, plate_z, pfx_x, pfx_z,\n         if_fielding_alignment, of_fielding_alignment,\n         game_date, balls, strikes, outs, inning, inning_topbot,\n         pre_runner_1b_id, pre_runner_2b_id, pre_runner_3b_id,\n         home_score, away_score, post_home_score, post_away_score, des)"
  },
  {
    "objectID": "sports/eda/nhl_shooting.html#overview",
    "href": "sports/eda/nhl_shooting.html#overview",
    "title": "EDA project: NHL shooting",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 5 and conclude with a 6-minute presentation on Tuesday, June 17 during lab.\nStudents will be randomly placed into groups of 2–3 and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/nhl_shooting.html#deliverables",
    "href": "sports/eda/nhl_shooting.html#deliverables",
    "title": "EDA project: NHL shooting",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 6-minute presentation. All group members must be present for the presentation and participate. No notes/scripts are allowed during the presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nTwo questions/hypotheses you are interested in exploring\nTwo data visualizations exploring the questions—both must be multivariate (i.e., involving 2+ variables) and in different formats\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "sports/eda/nhl_shooting.html#timeline",
    "href": "sports/eda/nhl_shooting.html#timeline",
    "title": "EDA project: NHL shooting",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 12 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review.\nTuesday, June 17 at 1pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "sports/eda/nhl_shooting.html#data",
    "href": "sports/eda/nhl_shooting.html#data",
    "title": "EDA project: NHL shooting",
    "section": "Data",
    "text": "Data\nThis dataset contains all shot attempts from the 2024-25 NHL regular season, courtesy of MoneyPuck.com.\nEach row in the dataset corresponds to a shot attempt and the columns are:\n\nshooterPlayerId: player id of the skater taking the shot\nshooterName: first and Last name of the player taking the shot\nteam: team taking the shot\nshooterLeftRight: whether the shooter is a left or right shot\nshooterTimeOnIce: playing time in seconds that have passed since the shooter started their shift\nshooterTimeOnIceSinceFaceoff: minimum of the playing time in seconds since the last faceoff and the playing time that has passed since the shooter started their shift\nevent: whether the shot was a shot on goal (SHOT), goal, (GOAL), or missed the net (MISS)\nlocation: the zone the shot took place in\nshotType: type of shot\nshotAngle: angle of the shot in degrees, positive if the shot is from the left side of the ice.\nshotAnglePlusRebound: difference in angle between the previous shot and this shot if this shot is a rebound, is otherwise set to 0\nshotDistance: distance from the net of the shot in feet, net is defined as being at the (89,0) coordinates\nshotOnEmptyNet: whether the shot was on an empty net\nshotRebound: whether the shot is a rebound, i.e., if the last event was a shot and within 3 seconds of this shot\nshotRush: whether the shot was on a rush, i.e., ff the last event was in another zone and within 4 seconds\nshotWasOnGoal: whether the shot was on net - either a goal or a goalie save\nshotGeneratedRebound: whether the shot generated a rebound shot within 3 seconds of the this shot\nshotGoalieFroze: whether the goalie froze the puck within 1 second of the shot\narenaAdjustedShotDistance: shot distance adjusted for arena recording bias - uses the same methodology as War On Ice proposed by Schuckers and Curro\narenaAdjustedXCord: x coordinate of the arena adjusted shot location, always a positive number\narenaAdjustedYCord: y coordinate of the arena adjusted shot location\ngoalieIdForShot: player id for the goalie the shot is on\ngoalieNameForShot: first and Last name of the goalie the shot is on\nteamCode: team code of the shooting team\nisHomeTeam: whether the shooting team is the home team\nhomeSkatersOnIce: number of skaters on ice for the home team (does not count the goalie)\nawaySkatersOnIce: number of skaters on ice for the away team (does not count the goalie)\ngame_id: game id of the game the shot took place in\nhomeTeamCode: home team in the game\nawayTeamCode: away team in the game\nhomeTeamGoals: home team goals before the shot took place\nawayTeamGoals: away team goals before the shot took place\ntime: seconds into the game of the shot\nperiod: period of the game\n\nNote that a full glossary of the features available for NHL shot data can be found here."
  },
  {
    "objectID": "sports/eda/nhl_shooting.html#starter-code",
    "href": "sports/eda/nhl_shooting.html#starter-code",
    "title": "EDA project: NHL shooting",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nnhl_shots &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/nhl_shots.csv\")\n\nIn case you’re curious, the code to build this dataset can be found below. (Note that the data were originally downloaded from the MoneyPuck site.)\n\n# download and unzip\n# https://peter-tanner.com/moneypuck/downloads/shots_2024.zip\n\nlibrary(tidyverse)\nnhl_shots &lt;- read_csv(\"shots_2024.csv\") # might need to modify file path\n\nnhl_shots &lt;- nhl_shots |&gt; \n  filter(isPlayoffGame == 0) |&gt; \n  select(\n    # shooter info\n    shooterPlayerId, shooterName, team, shooterLeftRight, \n    shooterTimeOnIce, shooterTimeOnIceSinceFaceoff,\n    # shot info\n    event, location, shotType, shotAngle, shotAnglePlusRebound, \n    shotDistance, shotOnEmptyNet, shotRebound, shotRush, \n    shotWasOnGoal, shotGeneratedRebound, shotGoalieFroze,\n    # arena-adjusted locations\n    arenaAdjustedShotDistance, arenaAdjustedXCord, arenaAdjustedYCord,\n    # goalie info\n    goalieIdForShot, goalieNameForShot,\n    # team context\n    teamCode, isHomeTeam, homeSkatersOnIce, awaySkatersOnIce,\n    # game context\n    game_id, homeTeamCode, awayTeamCode, homeTeamGoals, awayTeamGoals,\n    time, period\n  )"
  },
  {
    "objectID": "sports/eda/wwc_shooting.html#overview",
    "href": "sports/eda/wwc_shooting.html#overview",
    "title": "EDA project: Women’s World Cup shooting",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 5 and conclude with a 6-minute presentation on Tuesday, June 17 during lab.\nStudents will be randomly placed into groups of 2–3 and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "sports/eda/wwc_shooting.html#deliverables",
    "href": "sports/eda/wwc_shooting.html#deliverables",
    "title": "EDA project: Women’s World Cup shooting",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 6-minute presentation. All group members must be present for the presentation and participate. No notes/scripts are allowed during the presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nTwo questions/hypotheses you are interested in exploring\nTwo data visualizations exploring the questions—both must be multivariate (i.e., involving 2+ variables) and in different formats\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "sports/eda/wwc_shooting.html#timeline",
    "href": "sports/eda/wwc_shooting.html#timeline",
    "title": "EDA project: Women’s World Cup shooting",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 12 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review.\nTuesday, June 17 at 1pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "sports/eda/wwc_shooting.html#data",
    "href": "sports/eda/wwc_shooting.html#data",
    "title": "EDA project: Women’s World Cup shooting",
    "section": "Data",
    "text": "Data\nThis dataset contains all (open-play) shot attempts from the 2023 FIFA Women’s World Cup, courtesy of StatsBomb and accessed via the StatsBombR package.\nEach row in the dataset corresponds to a shot attempt and the columns are:\n\nperiod: period of the game (1: first half, 2: second half, 3: first extra time period, 4: second extra time period)\n\nminute & second: minute & second of the game\nduration: duration of the shot (seconds)\n\nunder_pressure: whether the shot is under pressure\nplay_pattern.name: play pattern name (leading up to the shot)\npossession_team.name: possession team name\nplayer.name: player taking the shot\nposition.name: player position\nshot.technique.name: technique of the shot\nshot.body_part.name: body part used to take the shot\nshot.outcome.name: outcome of the shot\nshot.{aerial_won, one_on_one, deflected, open_goal, saved_to_post, saved_off_target, redirect}: different shot binary indicators (should be self-explanatory, NA means FALSE)\nlocation.{x, y}: (x, y) location when shot takes place. (Note: all locations have been standardized so that the possession team is going from left to right. Higher x values means closer to the opposing team’s goal, and higher y values means more toward the left wing)\nshot.end_location.{x, y, z}: (x, y, z) coordinates of shot ending location\nplayer.name.GK: goalkeeper name\nlocation.{x, y}.GK: (x, y) location of goalkeeper when shot takes place\nDistToGoal: distance between shot location and center of goal\n\nDistToKeeper: distance between keeper and center goal (NOT between keeper location and shot location)\nAngleToGoal: angle between shot location and center of goal\n\nAngleToKeeper: angle between keeper and center of goal (NOT between keeper location and shot location)\nAngleDeviation: absolute difference between AngleToGoal and AngleToKeeper\navevelocity: (average) shot velocity (over the duration of shot)\n\nDistSGK: distance between shot location and keeper location\ndensity: aggregated inverse distance for each defender behind the ball\ndensity.incone: density for only defenders who are in the cone defined by the shooter and each goal post\nAttackersBehindBall: number of attackers behind the ball\nDefendersBehindBall: number of defenders behind the ball\nDefendersInCone: number of defenders in the cone defined by the shooter and each goal post\n\nInCone.GK: whether the goalkeeper is in the cone defined by the shooter and each goal post\n\nDefArea: area of the smallest square that covers all opposing defenders\n\ndistance.ToD1.360: distance between shooter and closest defender\n\ndistance.ToD2.360: distance between shooter and second-closest defender\n\nTimeInPoss: time elapsed since the start of possession\n\nTimeToPossEnd: time remaining until end of possession"
  },
  {
    "objectID": "sports/eda/wwc_shooting.html#starter-code",
    "href": "sports/eda/wwc_shooting.html#starter-code",
    "title": "EDA project: Women’s World Cup shooting",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nwwc_shots &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/wwc_shots.csv\")\n\nIn case you’re curious, the code to build this dataset can be found below.\n\nlibrary(tidyverse)\nlibrary(StatsBombR)\n\nwwc &lt;- FreeCompetitions() |&gt; \n  filter(competition_id == \"72\" & season_id == \"107\") |&gt; \n  FreeMatches() |&gt; \n  free_allevents() |&gt; \n  allclean()\n\nwwc_shots &lt;- wwc |&gt;\n  filter(type.name == \"Shot\", \n         shot.type.name == \"Open Play\", \n         team.name == possession_team.name) |&gt; \n  janitor::remove_empty() |&gt; \n  select(period, minute, second, duration, under_pressure, \n         play_pattern.name, possession_team.name, player.name, position.name, \n         shot.technique.name, shot.body_part.name, shot.outcome.name, \n         shot.aerial_won, shot.one_on_one,shot.deflected, shot.open_goal, \n         shot.saved_to_post, shot.saved_off_target, shot.redirect, \n         location.x, location.y, shot.end_location.x, \n         shot.end_location.y, shot.end_location.z, \n         player.name.GK, location.x.GK, location.y.GK, \n         DistToGoal, DistToKeeper, AngleToGoal,AngleToKeeper, AngleDeviation, \n         avevelocity, DistSGK, density, density.incone, \n         AttackersBehindBall:distance.ToD2.360, TimeInPoss, TimeToPossEnd) |&gt; \n  mutate(InCone.GK = ifelse(InCone.GK &gt; 0, 1, InCone.GK))"
  },
  {
    "objectID": "health/labs/02-wrangling.html",
    "href": "health/labs/02-wrangling.html",
    "title": "Lab: data wrangling",
    "section": "",
    "text": "Our data are usually stored as a .csv file and after loading a .csv file into RStudio, we will have a “data frame”. A data frame can be considered a special case of matrix where each column represents a measurement or variable of interest for each observation which correspond to the rows of the dataset. After loading the tidyverse suite of packages, we use the read_csv() function to load the heart_disease dataset from yesterday:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nheart_disease &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/heart_disease.csv\")\n\nRows: 788 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Gender\ndbl (9): Cost, Age, Interventions, Drugs, ERVisit, Complications, Comorbidit...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBy default, read_csv() reads in the dataset as a tbl (aka tibble) object instead of a data.frame object. You can read about the differences here, but it’s not that meaningful for purposes.\nWe can use the functions slice_head() and slice_tail() to view a sample of the data. Use the slice_head() function to view the first 6 rows, then use the slice_tail() function to view the last 3 rows:\n\n# INSERT CODE HERE\n\nView the dimensions of the data with dim():\n\n# INSERT CODE HERE\n\nQuickly view summary statistics for all variables with the summary() function:\n\n# Uncomment the following code by deleting the # at the front\n# summary(heart_disease)\n\nView the data structure types with str():\n\n# str(heart_disease)\n\nWhat’s the difference between the output from the two functions?\nYou can find a description of the dataset here."
  },
  {
    "objectID": "health/labs/02-wrangling.html#reading-and-previewing-data",
    "href": "health/labs/02-wrangling.html#reading-and-previewing-data",
    "title": "Lab: data wrangling",
    "section": "",
    "text": "Our data are usually stored as a .csv file and after loading a .csv file into RStudio, we will have a “data frame”. A data frame can be considered a special case of matrix where each column represents a measurement or variable of interest for each observation which correspond to the rows of the dataset. After loading the tidyverse suite of packages, we use the read_csv() function to load the heart_disease dataset from yesterday:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nheart_disease &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/heart_disease.csv\")\n\nRows: 788 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Gender\ndbl (9): Cost, Age, Interventions, Drugs, ERVisit, Complications, Comorbidit...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBy default, read_csv() reads in the dataset as a tbl (aka tibble) object instead of a data.frame object. You can read about the differences here, but it’s not that meaningful for purposes.\nWe can use the functions slice_head() and slice_tail() to view a sample of the data. Use the slice_head() function to view the first 6 rows, then use the slice_tail() function to view the last 3 rows:\n\n# INSERT CODE HERE\n\nView the dimensions of the data with dim():\n\n# INSERT CODE HERE\n\nQuickly view summary statistics for all variables with the summary() function:\n\n# Uncomment the following code by deleting the # at the front\n# summary(heart_disease)\n\nView the data structure types with str():\n\n# str(heart_disease)\n\nWhat’s the difference between the output from the two functions?\nYou can find a description of the dataset here."
  },
  {
    "objectID": "health/labs/02-wrangling.html#data-manipulation-with-dplyr",
    "href": "health/labs/02-wrangling.html#data-manipulation-with-dplyr",
    "title": "Lab: data wrangling",
    "section": "Data manipulation with dplyr",
    "text": "Data manipulation with dplyr\nAn easier way to manipulate the data frame is through the dplyr package, which is in the tidyverse suite of packages. The operations we can do include: selecting specific columns, filtering for rows, re-ordering rows, adding new columns and summarizing data. The “split-apply-combine” concept can be achieved by dplyr.\n\nSelecting columns with select()\nThe function select() can be use to select certain column with the column names. First create a new table called heart_disease_ad that only contains the Age and Drugs columns:\n\n# INSERT CODE HERE\n\nTo select all columns except a specific column, use the - (subtraction) operator. For example, view the output from uncommenting the following line of code:\n\n# select(heart_disease, -Interventions)\n\nTo select a range of columns by name (that are in consecutive order), use the : (colon) operator. For example, view the output from uncommenting the following line of code:\n\n# select(heart_disease, Drugs:Duration)\n\nTo select all columns that start with certain character strings, use the function starts_with(). Other matching options are:\n\nends_with(): select columns that end with a character string\ncontains(): select columns that contain a character string\nmatches(): select columns that match a regular expression\none_of(): select columns names that are from a group of names\n\n\n# Uncomment the following lines of code\n# select(heart_disease, starts_with(\"Com\"))\n# select(heart_disease, contains(\"er\"))\n\n\n\nExtracting rows using filter()\nWe can also extract the rows/observations that satisfy certain criteria. Try extracting the rows with Cost of more than 400:\n\n# INSERT CODE HERE\n\nWe can also filter on multiple criteria. Subset the rows that are male patients and 60+ years old.\n\n# INSERT CODE HERE\n\n\n\nArranging rows using arrange()\nTo arrange the data frame by a specific order we need to use the function arrange(). The default is by increasing order and the desc() will provide the decreasing order. First arrange the heart_disease table by Duration in ascending order:\n\n# INSERT CODE HERE\n\nNext by descending order:\n\n# INSERT CODE HERE\n\nTry combining a pipeline of select(), filter(), and arrange() steps together with the |&gt; operator by:\n\nSelecting the Age, Cost, ERVisit, and Duration columns\nSubset only patients who are 60 or older\nSort by Duration in descending order\n\n\n# INSERT CODE HERE\n\n\n\nCreating new columns using mutate()\nSometimes the data does not include the variable that we are interested in and we need to manipulate the current variables to add new variables into the data frame. Create a new column cost_per_day by taking the Cost and dividing by Duration (reassign this output to the heart_disease table following the commented code chunk so this column is added to the table):\n\n# heart_disease &lt;- heart_disease |&gt;\n#   mutate(INSERT CODE HERE)\n\n\n\nCreating summaries with summarize()\nTo create summary statistics for a given column in the data frame, we can use summarize() function. Compute the mean, min, and max amount of Cost:\n\n# INSERT CODE HERE\n\nThe advantage of summarize() is more obvious if we combine it with group_by(), the group operators. Try to group_by() the Gender column first and then compute the same summary statistics for Cost:\n\n# INSERT CODE HERE"
  },
  {
    "objectID": "health/labs/04-github.html",
    "href": "health/labs/04-github.html",
    "title": "Using GitHub for project collaboration",
    "section": "",
    "text": "The purpose of this lab is to help you set up Git and GitHub on your computer, and use them to collaborate on your EDA project."
  },
  {
    "objectID": "health/labs/04-github.html#goal",
    "href": "health/labs/04-github.html#goal",
    "title": "Using GitHub for project collaboration",
    "section": "",
    "text": "The purpose of this lab is to help you set up Git and GitHub on your computer, and use them to collaborate on your EDA project."
  },
  {
    "objectID": "health/labs/04-github.html#task-0-github-registration-and-git-installation",
    "href": "health/labs/04-github.html#task-0-github-registration-and-git-installation",
    "title": "Using GitHub for project collaboration",
    "section": "Task 0: GitHub registration and Git installation",
    "text": "Task 0: GitHub registration and Git installation\n(Note: You were already being asked to complete these prior to the start of the program. Proceed to Task 1 if you already completed this task.)\nGitHub account. Register for a (free) GitHub account at https://github.com.  (if you already have a GitHub account, feel free to ignore this)\nDownload Git.  (if Git is already installed on your computer, the following instructions still hold for updating Git to its latest version)\n(Windows)\n\nGo to https://git-scm.com/download/win\nNavigate to “Click here to download” on the first line and click on it\nFollow the installation instructions\n\n(macOS)\n\nOpen the Terminal app on your computer (Finder \\(\\rightarrow\\) Applications \\(\\rightarrow\\) Terminal)\nGo to https://brew.sh and copy/paste the chunk under “Install Homebrew” to the Terminal\nOnce Homebrew is installed, type this into the Terminal: brew install git"
  },
  {
    "objectID": "health/labs/04-github.html#task-1-git-configuration",
    "href": "health/labs/04-github.html#task-1-git-configuration",
    "title": "Using GitHub for project collaboration",
    "section": "Task 1: Git configuration",
    "text": "Task 1: Git configuration\n\nMake sure you’ve already (i) created a GitHub account and (ii) installed Git on your computer\nYou then need to configure Git. This can be done directly in R:\n\n\n# uncomment and run the following line to install the usethis package\n# install.packages(\"usethis\")\nusethis::use_git_config(user.name = \"Your Name\", user.email = \"your-github@email.address\")\n\n\nUse your full name for the user.name field and the same email as your GitHub account for user.email\nYou then need to create a personal access token for authentication as follows:\n\n\nusethis::create_github_token()\n\n\nThis will direct you to the GitHub site on your browser (you may have to log in). On this site:\n\nUnder “Note”, type in some description for this token (e.g., “Summer 2025 GitHub token”)\nFor “Expiration”, set an expiration date for this token (e.g., 90 days) or make it permanent (i.e. choose “No expiration” if you don’t want to deal with this again in the future)\nUnder “Select scopes”, recommended scopes will be pre-selected. Stick with these for now.\n\nNext, click on “Generate token”\nCopy the token to your clipboard (or leave the browser window open, so you can come back to copy the token later)\nIn RStudio, run the following to get a prompt where you can paste your token:\n\n\n# uncomment and run the following line to install the gitcreds package\n# install.packages(\"gitcreds\")\ngitcreds::gitcreds_set()"
  },
  {
    "objectID": "health/labs/04-github.html#task-2-eda-project-collaboration-with-git-and-github",
    "href": "health/labs/04-github.html#task-2-eda-project-collaboration-with-git-and-github",
    "title": "Using GitHub for project collaboration",
    "section": "Task 2: EDA project collaboration with Git and GitHub",
    "text": "Task 2: EDA project collaboration with Git and GitHub\nMake sure every group member finishes Task 1 before proceeding to Task 2, which requires a group effort.\n\nStep 1: Create an EDA project repository on GitHub(Required for ONE group member only)\nEach group should elect ONE person to create a GitHub repository for the EDA project. This repository is to be shared among all group members.\nThe elected group member should do the following to create a new GitHub repository:\n\nAfter you’ve signed in to GitHub, go to https://github.com/new\nName the repository (give it a meaningful name)\nGive the repository a description (don’t leave this blank although this is optional)\nDecide whether to keep the repository public or private (for now, keep it public, so that we can review your code)\nClick on “Initialize this repository with a README”. For now, there’s no need to “Add .gitignore” or “Choose a license”\nClick on “Create Repository”\n\nNext, to add the rest of your group to the repository:\n\nGo to browser page of the GitHub repository you just created and click on “Settings”\nNavigate to the left sidebar and click on “Collaborators”\nClick on “Add people” (under Manage access). Enter the GitHub username for the other group members.\n\n\n\nStep 2: Clone the remote repository to your local computer(Required for ALL group members)\n\nEveryone (except the member responsible for creating the repository) should each get an invitation sent to the email associated with your GitHub account\nCheck your email and accept the invitation\nGo to the browser page for the EDA project GitHub repository (created in Step 1)\nClick on Code (in the same line as “Go to file”). Under HTTPS, copy the URL\nIn RStudio, click on File &gt; New Project.... Next, click on “Version Control” and then on “Git”. Paste the URL you just copied into “Repository URL”\nType the name for the folder on your computer associated with this repository into Project directory name\n\nYou can choose whatever name you want, but it is recommended to give a name similar to the repository name on GitHub\n\nMake sure “Create project as subdirectory of:” points to where you want to locate this new folder\nClick on “Create Project”\nAt this point, you should find that the “Files” pane (in the bottom right of RStudio) is listing the files in your local repository.\n\n\n\nStep 3: Modify the repository (Required for ALL group members)\nEach group member should create their own “sandbox” folder locally on their own computer as follows:\n\nNavigate to the Files pane in RStudio and click on “Folder” to create a new folder (for the new folder name, use your last name.)\nIn RStudio, open a new file (could be anything - e.g. R Script, Quarto document, etc.). Fill the file with some code/comments/etc. (This is just for illustration purpose, to show how you can add a file to GitHub from your computer)\nSave the file inside the folder you just created (with your last name as the folder name). At this point, this file should show up in the “Git” pane (in the top right of RStudio)\nCheck the box under “Staged” in the Git pane to stage the file for a commit\nClick on “Commit” in the Git pane\nIn the new window that opens, add a “Commit message”, then click on the “Commit” button\nClick on “Push” to push your changes from your local repository to the shared remote repository on GitHub\n\n\n\nStep 4: Update the local repository (Required for ALL group members)\n\nFirst, make sure that everyone in your group have completed Step 3\nIn RStudio, navigate to the Git pane and click on “Pull”. A new window will pop up. Once everything is finished running, close the window.\nAt this point, you should find that your Files pane in RStudio is listing the folders that your group members have created, in addition to your own folder\n\nThis task is know as git pull, which updates the local repository to match that content of a shared remote repository\n\n\n\n\nStep 5: Start your EDA project\nIf you encountered no errors then you can feel free to start brainstorming your EDA project with your group.\nFor this project, we ask you to create/update/save files within your own sandbox folder (that you created in Step 3). This will help mitigate the risk of running into trouble when pushing your files to GitHub, especially for those who are new to Git and GitHub. This also allows us to easily review your code.\n\n\n\n\n\n\nImportant notes\n\n\n\nThe GitHub procedure for any project collaboration is\n\nPull new changes\nMake changes on your computer (e.g. create new files, update existing files)\nCommit your local changes (Note: this step may be repeated)\nPull again to avoid merge conflicts\nPush your commit(s) to GitHub\n\nAdvices: Make small, frequent commits. ALWAYS pull before you push.\n\n\nAsk us for help if you run into any issues or have any questions."
  },
  {
    "objectID": "health/eda/covid_hospitalizations.html#overview",
    "href": "health/eda/covid_hospitalizations.html#overview",
    "title": "EDA project: COVID hospitalizations in Pennsylvania",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 5 and conclude with a 6-minute presentation on Tuesday, June 17 during lab.\nStudents will be randomly placed into groups of 2–3 and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/covid_hospitalizations.html#deliverables",
    "href": "health/eda/covid_hospitalizations.html#deliverables",
    "title": "EDA project: COVID hospitalizations in Pennsylvania",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 6-minute presentation. All group members must be present for the presentation and participate. No notes/scripts are allowed during the presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nTwo questions/hypotheses you are interested in exploring\nTwo data visualizations exploring the questions—both must be multivariate (i.e., involving 2+ variables) and in different formats\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/covid_hospitalizations.html#timeline",
    "href": "health/eda/covid_hospitalizations.html#timeline",
    "title": "EDA project: COVID hospitalizations in Pennsylvania",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 12 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review.\nTuesday, June 17 at 1pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "health/eda/covid_hospitalizations.html#data",
    "href": "health/eda/covid_hospitalizations.html#data",
    "title": "EDA project: COVID hospitalizations in Pennsylvania",
    "section": "Data",
    "text": "Data\nThis dataset contains county-level hospitalization information on related to COVID-19 patient in Pennsylvania, with the dates ranging from April 1, 2020 to December 31, 2022. The data are available online at the Open Data Pennsylvania website. More information about the hospitalization data can be found here.\nEach row in the dataset corresponds to a county in Pennsylvania on a given date (between April 1, 2020 and December 31, 2020 - note that missing data are present for some of the rows) and the columns are:\n\ncounty: name of county\ndate: date\n\nicu_avail: adult ICU beds available\n\nicu_total: adult ICU beds total\n\nmed_avail: medical/surgical beds available\n\nmed_total: medical/surgical beds total\nped_avail: pediatrics beds available\n\nped_total: pediatrics beds total\n\npic_avail: pediatrics ICU beds available\n\npic_total: pediatrics ICU beds total\n\ncovid_patients: COVID-19 patients hospitalized\ncovid_vents: COVID-19 patients on ventilators\n\nvents_use: total ventilators in use\n\nvents: total ventilators\n\nicu_avail_mean: adult ICU beds available, 14-day average\nicu_total_mean: adult ICU beds total, 14-day average\n\nmed_avail_mean: medical/surgical beds available, 14-day average\nmed_total_mean: medical/surgical beds total, 14-day average\nped_avail_mean: pediatric beds available, 14-day average\nped_total_mean: pediatric beds total, 14-day average\npic_avail_mean: pediatric ICU beds available, 14-day average\npic_total_mean: pediatric ICU beds total, 14-day average\ncovid_patients_mean: COVID-19 patients hospitalized, 14-day average\ncovid_vents_mean: COVID-19 patients on ventilators, 14-day average\nvents_use_mean: total ventilators in use, 14-day average\nvents_mean: total ventilators, 14-day average\n\nicu_percent: adult ICU beds, percent available\n\nmed_percent: medical/surgical beds, percent available\n\nped_percent: pediatric beds, percent available\npic_percent: pediatric ICU beds, percent available\n\ncovid_icu: COVID patients in intensive care unit (ICU)\ncovid_icu_mean: the mean for COVID patients in intensive care unit (ICU), 14-day average.\n\ncounty_fips: a county’s 5-digit code (read more here)\nlongitude: a longitude generic point within the county\nlatitude: a latitude generic point within the county"
  },
  {
    "objectID": "health/eda/covid_hospitalizations.html#starter-code",
    "href": "health/eda/covid_hospitalizations.html#starter-code",
    "title": "EDA project: COVID hospitalizations in Pennsylvania",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\ncovid_hospitalizations &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/covid_hospitalizations.csv\")"
  },
  {
    "objectID": "health/eda/prescriptions.html#overview",
    "href": "health/eda/prescriptions.html#overview",
    "title": "EDA project: opioid prescriptions and claims",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 5 and conclude with a 6-minute presentation on Tuesday, June 17 during lab.\nStudents will be randomly placed into groups of 2–3 and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/prescriptions.html#deliverables",
    "href": "health/eda/prescriptions.html#deliverables",
    "title": "EDA project: opioid prescriptions and claims",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 6-minute presentation. All group members must be present for the presentation and participate. No notes/scripts are allowed during the presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nTwo questions/hypotheses you are interested in exploring\nTwo data visualizations exploring the questions—both must be multivariate (i.e., involving 2+ variables) and in different formats\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/prescriptions.html#timeline",
    "href": "health/eda/prescriptions.html#timeline",
    "title": "EDA project: opioid prescriptions and claims",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 12 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review.\nTuesday, June 17 at 1pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "health/eda/prescriptions.html#data",
    "href": "health/eda/prescriptions.html#data",
    "title": "EDA project: opioid prescriptions and claims",
    "section": "Data",
    "text": "Data\nThis dataset contains information on Medicare Part D Prescription Claims. Under the Medicare Part D Prescription Drug program, information is tracked for opioids and other drugs prescribed by physicians and other health care providers including the number of prescriptions dispensed (original prescriptions and refills), the total drug cost, beneficiary demographics (65+), related claims information, as well as information about the physician/provider such as their specialization and location.\nThe sample of data is proportionally sampled across the states (e.g. 5% from each state), and includes the following columns:\n\nNPI: national provider identifier for the performing provider on the claim\nLastName: provider last name\nFirstName: provider first name\nCity: city where the provider is located\nState: state where the provider is located\nSpecialty: specialty of the provider derived from the Medicare code reported on the claims\nBrandName: brand name of the drug filled\nGenericName: generic name/chemical ingredient of the drug filled\nNumberClaims: number of Medicare Part D claims filled (includes original prescriptions and refills)\nNumber30DayFills: aggregated number of Medicare Part D standardized 30-day fills (number of days supplied dived by 30; if &lt; 1.0, bottom-coded as 1.0; if &gt; 12.0, top-coded as 12.0\nNumberDaysSupply: aggregated number of day’s supply for which the drug is dispersed\nTotalDrugCost: aggregated drug cost paid for all associated claims\nNumberMedicareBeneficiaries: total number of unique Medicare Part D beneficiaries with at least one claim for the drug\nNumberClaims65Older: number of Medicare Part D claims for beneficiaries age 65 and older\nNumber30DayFills65Older: number of Medicare Part D standardized 30-day fills for beneficiaries age 65 and older (see Number30DayFills for standardized definition)\nTotalDrugCost65Older: aggregated total drug cost paid for all associated claims for beneficiaries age 65 and older\nNumberDaysSupply65Older: aggregated number of day’s supply for which this drug was dispensed, for beneficiaries age 65 and older\nNumberMedicareBeneficiaries65Older: number of unique Medicare Part D beneficiaries age 65 and older with at least one claim for the drug\nType: type of drug used: Brand or Generic\nOpioidFlag: whether the drug is an opioid or not an opioid\nSpecialtyCateg: provider specialty in broader categories (see Specialty variable)"
  },
  {
    "objectID": "health/eda/prescriptions.html#starter-code",
    "href": "health/eda/prescriptions.html#starter-code",
    "title": "EDA project: opioid prescriptions and claims",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\nprescriptions &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/prescriptions.csv\")"
  },
  {
    "objectID": "health/eda/covid_cases_deaths.html#overview",
    "href": "health/eda/covid_cases_deaths.html#overview",
    "title": "EDA project: COVID cases and deaths in Pennsylvania",
    "section": "Overview",
    "text": "Overview\nThis project will be released on Thursday, June 5 and conclude with a 6-minute presentation on Tuesday, June 17 during lab.\nStudents will be randomly placed into groups of 2–3 and each group will be randomly assigned a dataset.\nThe goal of this project is to practice understanding the structure of a dataset, and to practice generating and evaluating hypotheses using fundamental EDA and data visualization techniques."
  },
  {
    "objectID": "health/eda/covid_cases_deaths.html#deliverables",
    "href": "health/eda/covid_cases_deaths.html#deliverables",
    "title": "EDA project: COVID cases and deaths in Pennsylvania",
    "section": "Deliverables",
    "text": "Deliverables\nEach group is expected to make slides to accompany the 6-minute presentation. All group members must be present for the presentation and participate. No notes/scripts are allowed during the presentation.\nThe presentation should feature the following:\n\nOverview of the structure of your dataset\nTwo questions/hypotheses you are interested in exploring\nTwo data visualizations exploring the questions—both must be multivariate (i.e., involving 2+ variables) and in different formats\nOne clustering analysis\nConclusions for the hypotheses based on your EDA and data visualizations"
  },
  {
    "objectID": "health/eda/covid_cases_deaths.html#timeline",
    "href": "health/eda/covid_cases_deaths.html#timeline",
    "title": "EDA project: COVID cases and deaths in Pennsylvania",
    "section": "Timeline",
    "text": "Timeline\nThere will be two submission deadlines:\nThursday, June 12 at 5pm ET - Each student will push their individual code for the project thus far to GitHub for review.\nTuesday, June 17 at 1pm ET - Slides and full code must be completed and ready for presentation. Send your slides to Quang (quang@stat.cmu.edu). All code must be written in R; but the slides may be created in any software. Take advantage of examples from lectures, but also feel free to explore online resources that may be relevant. (But be sure to always consult the R help documentation first before attempting to google around or ask ChatGPT.)"
  },
  {
    "objectID": "health/eda/covid_cases_deaths.html#data",
    "href": "health/eda/covid_cases_deaths.html#data",
    "title": "EDA project: COVID cases and deaths in Pennsylvania",
    "section": "Data",
    "text": "Data\nThis dataset contains county-level information on the number of COVID-related cases and deaths for Pennsylvania. In particular, daily case and death counts, averages, and rates are provided for all counties in Pennsylvania, with the dates ranging from April 1, 2020 to December 31, 2022. The data are available online at the Open Data Pennsylvania website. More information about the cases and deaths data can be found here and here.\nEach row in the dataset corresponds to a county in Pennsylvania on a given date (between April 1, 2020 and December 31, 2020) and the columns are:\n\ncounty: name of county\ndate: date\ncases: number of new confirmed and probable cases first reported to the Department of Health on that date\ncases_avg_new: rolling 7-day average of new confirmed and probable case\ncases_cume: cumulative confirmed and probable cases reported through that date\n\ncases_rate: number of new confirmed and probable cases reported that date per 100,000 population\ncases_avg_new_rate: rolling 7-day average of new confirmed and probable cases per 100,000 population\ncases_cume_rate: number of cumulative confirmed and probable cases reported through that date per 100,000 population\n\ndeaths: count of new deaths\n\ndeaths_avg_new: new deaths, 7-day average\ndeaths_cume: cumulative count of deaths\n\npopulation: population (as of 2019)\n\ndeaths_rate: new deaths per 100,000 population\ndeaths_avg_new_rate: new deaths per 100,000 population, 7-day average\ndeaths_cume_rate: cumulative count of deaths per 100,000 population\nlongitude: a longitude generic point within the county\nlatitude: a latitude generic point within the county"
  },
  {
    "objectID": "health/eda/covid_cases_deaths.html#starter-code",
    "href": "health/eda/covid_cases_deaths.html#starter-code",
    "title": "EDA project: COVID cases and deaths in Pennsylvania",
    "section": "Starter code",
    "text": "Starter code\n\nlibrary(tidyverse)\ncovid_cases_deaths &lt;- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/covid_cases_deaths.csv\")"
  },
  {
    "objectID": "lectures/04-quantitative.html#quantitative-data",
    "href": "lectures/04-quantitative.html#quantitative-data",
    "title": "Data visualization: quantitative data",
    "section": "Quantitative data",
    "text": "Quantitative data\nTwo different versions of quantitative data:\nDiscrete: countable and has clear space between values (i.e. whole number only)\n\nExamples: number of goals scored in a game, number of children in a family\n\nContinuous: can take any value within some interval\n\nExamples: price of houses in Pittsburgh, water temperature, wind speed"
  },
  {
    "objectID": "lectures/04-quantitative.html#data",
    "href": "lectures/04-quantitative.html#data",
    "title": "Data visualization: quantitative data",
    "section": "Data",
    "text": "Data\nTaylor Swift songs via the taylor package (data dictionary here)\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nlibrary(taylor)\nnames(taylor_all_songs)\n\n [1] \"album_name\"          \"ep\"                  \"album_release\"      \n [4] \"track_number\"        \"track_name\"          \"artist\"             \n [7] \"featuring\"           \"bonus_track\"         \"promotional_release\"\n[10] \"single_release\"      \"track_release\"       \"danceability\"       \n[13] \"energy\"              \"key\"                 \"loudness\"           \n[16] \"mode\"                \"speechiness\"         \"acousticness\"       \n[19] \"instrumentalness\"    \"liveness\"            \"valence\"            \n[22] \"tempo\"               \"time_signature\"      \"duration_ms\"        \n[25] \"explicit\"            \"key_name\"            \"mode_name\"          \n[28] \"key_mode\"            \"lyrics\"             \n\ntaylor_all_songs &lt;- taylor_all_songs |&gt; \n  mutate(duration = duration_ms / 60000)"
  },
  {
    "objectID": "lectures/04-quantitative.html#summarizing-1d-quantitative-data",
    "href": "lectures/04-quantitative.html#summarizing-1d-quantitative-data",
    "title": "Data visualization: quantitative data",
    "section": "Summarizing 1D quantitative data",
    "text": "Summarizing 1D quantitative data\n\nCenter: mean, median, number and location of modes\n\n\n\nSpread: range, variance, standard deviation, IQR, etc.\n\n\n\n\nShape: skew vs symmetry, outliers, heavy vs light tails, etc.\n\n\n\nCompute various statistics in R with summary(), mean(), median(), quantile(), range(), sd(), var(), etc.\n\n\nExample: Summarizing the duration of Taylor Swift songs\n\nsummary(taylor_all_songs$duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  2.198   3.543   3.930   3.985   4.343  10.217      12 \n\nsd(taylor_all_songs$duration, na.rm = TRUE)\n\n[1] 0.7527544"
  },
  {
    "objectID": "lectures/04-quantitative.html#boxplots-visualize-summary-statistics",
    "href": "lectures/04-quantitative.html#boxplots-visualize-summary-statistics",
    "title": "Data visualization: quantitative data",
    "section": "Boxplots visualize summary statistics",
    "text": "Boxplots visualize summary statistics\n\n\nPros:\n\nDisplays outliers, percentiles, spread, skew\nUseful for side-by-side comparison\n\nCons:\n\nDoes not display the full distribution shape\nDoes not display modes\n\nThe expert weighed in…\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration)) +\n  geom_boxplot() +\n  theme(axis.text.y = element_blank())"
  },
  {
    "objectID": "lectures/04-quantitative.html#histograms-display-1d-continuous-distributions",
    "href": "lectures/04-quantitative.html#histograms-display-1d-continuous-distributions",
    "title": "Data visualization: quantitative data",
    "section": "Histograms display 1D continuous distributions",
    "text": "Histograms display 1D continuous distributions\n\n\n\\(\\displaystyle \\text{# total obs.} = \\sum_{j=1}^k \\text{# obs. in bin }j\\)\nPros:\n\nDisplays full shape of distribution\nEasy to interpret\n\nCons:\n\nHave to choose number of bins and bin locations (will revisit later)\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration)) +\n  geom_histogram()"
  },
  {
    "objectID": "lectures/04-quantitative.html#display-the-data-points-directly-with-beeswarm-plots",
    "href": "lectures/04-quantitative.html#display-the-data-points-directly-with-beeswarm-plots",
    "title": "Data visualization: quantitative data",
    "section": "Display the data points directly with beeswarm plots",
    "text": "Display the data points directly with beeswarm plots\n\n\nPros:\n\nDisplays each data point\nEasy to view full shape of distribution\n\nCons:\n\nCan be overbearing with large datasets\nWhich algorithm for arranging points?\n\n\n\nlibrary(ggbeeswarm)\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration, y = \"\")) +\n  geom_beeswarm(cex = 2)"
  },
  {
    "objectID": "lectures/04-quantitative.html#smooth-summary-with-violin-plots",
    "href": "lectures/04-quantitative.html#smooth-summary-with-violin-plots",
    "title": "Data visualization: quantitative data",
    "section": "Smooth summary with violin plots",
    "text": "Smooth summary with violin plots\n\n\nPros:\n\nDisplays full shape of distribution\nCan easily layer…\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration, y = \"\")) +\n  geom_violin()"
  },
  {
    "objectID": "lectures/04-quantitative.html#smooth-summary-with-violin-plots-box-plots",
    "href": "lectures/04-quantitative.html#smooth-summary-with-violin-plots-box-plots",
    "title": "Data visualization: quantitative data",
    "section": "Smooth summary with violin plots + box plots",
    "text": "Smooth summary with violin plots + box plots\n\n\nPros:\n\nDisplays full shape of distribution\nCan easily layer… with box plots on top\n\nCons:\n\nSummary of data via density estimate\nMirror image is duplicate information\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration, y = \"\")) +\n  geom_violin() +\n  geom_boxplot(width = 0.4)"
  },
  {
    "objectID": "lectures/04-quantitative.html#what-do-visualizations-of-continuous-distributions-display",
    "href": "lectures/04-quantitative.html#what-do-visualizations-of-continuous-distributions-display",
    "title": "Data visualization: quantitative data",
    "section": "What do visualizations of continuous distributions display?",
    "text": "What do visualizations of continuous distributions display?\nProbability that continuous variable \\(X\\) takes a particular value is 0\ne.g. \\(P(\\) duration \\(= 3) = 0\\) (why?)\nFor continuous variables, the cumulative distribution function (CDF) is \\[F(x) = P(X \\leq x)\\]\nFor \\(n\\) observations, the empirical CDF (ECDF) can be computed based on the observed data \\[\\hat{F}_n(x)  = \\frac{\\text{# obs. with variable} \\leq x}{n} = \\frac{1}{n} \\sum_{i=1}^{n} I (x_i \\leq x)\\]\nwhere \\(I()\\) is the indicator function, i.e. ifelse(x_i &lt;= x, 1, 0)"
  },
  {
    "objectID": "lectures/04-quantitative.html#display-full-distribution-with-ecdf-plot",
    "href": "lectures/04-quantitative.html#display-full-distribution-with-ecdf-plot",
    "title": "Data visualization: quantitative data",
    "section": "Display full distribution with ECDF plot",
    "text": "Display full distribution with ECDF plot\n\n\nPros:\n\nDisplays all of your data at once\nAs \\(n \\rightarrow \\infty\\), the ECDF \\(\\hat F_n(x)\\) converges to the true CDF \\(F(x)\\)\n\nCons:\n\nWhat are the cons?\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration)) +\n  stat_ecdf()"
  },
  {
    "objectID": "lectures/04-quantitative.html#rug-plots-display-raw-data",
    "href": "lectures/04-quantitative.html#rug-plots-display-raw-data",
    "title": "Data visualization: quantitative data",
    "section": "Rug plots display raw data",
    "text": "Rug plots display raw data\n\n\nPros:\n\nDisplays raw data points\nUseful supplement for summaries and 2D plots\n\nCons:\n\nCan be overbearing for large datasets\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration)) +\n  geom_rug(alpha = 0.5)"
  },
  {
    "objectID": "lectures/04-quantitative.html#rug-plots-supplement-other-displays",
    "href": "lectures/04-quantitative.html#rug-plots-supplement-other-displays",
    "title": "Data visualization: quantitative data",
    "section": "Rug plots supplement other displays",
    "text": "Rug plots supplement other displays\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration)) +\n  geom_histogram() +\n  geom_rug(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = duration)) +\n  stat_ecdf() +\n  geom_rug(alpha = 0.5)"
  },
  {
    "objectID": "lectures/04-quantitative.html#summarizing-2d-quantitative-data",
    "href": "lectures/04-quantitative.html#summarizing-2d-quantitative-data",
    "title": "Data visualization: quantitative data",
    "section": "Summarizing 2D quantitative data",
    "text": "Summarizing 2D quantitative data\n\nDirection/trend (positive, negative)\nStrength of the relationship (strong, moderate, weak)\nLinearity (linear, non-linear)\n\n\nBig picture\n\nScatterplots are by far the most common visual\nRegression analysis is by far the most popular analysis (we will have a class on this)\nRelationships may vary across other variables, e.g., categorical variables"
  },
  {
    "objectID": "lectures/04-quantitative.html#making-scatterplots",
    "href": "lectures/04-quantitative.html#making-scatterplots",
    "title": "Data visualization: quantitative data",
    "section": "Making scatterplots",
    "text": "Making scatterplots\n\n\n\nUse geom_point()\nDisplaying the joint (bivariate) distribution\nWhat is the obvious flaw with this plot?\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = loudness, y = energy)) +\n  geom_point(color = \"darkred\", size = 4)"
  },
  {
    "objectID": "lectures/04-quantitative.html#making-scatterplots-always-adjust-the-transparency-alpha",
    "href": "lectures/04-quantitative.html#making-scatterplots-always-adjust-the-transparency-alpha",
    "title": "Data visualization: quantitative data",
    "section": "Making scatterplots: always adjust the transparency (alpha)",
    "text": "Making scatterplots: always adjust the transparency (alpha)\n\n\n\nAdjust the transparency of points via alpha to visualize overlap\nProvides better understanding of joint frequency\nEspecially important with larger datasets\nSee also: ggblend\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = loudness, y = energy)) +\n  geom_point(color = \"darkred\", size = 4, alpha = 0.5)"
  },
  {
    "objectID": "lectures/04-quantitative.html#summarizing-2d-quantitative-data-1",
    "href": "lectures/04-quantitative.html#summarizing-2d-quantitative-data-1",
    "title": "Data visualization: quantitative data",
    "section": "Summarizing 2D quantitative data",
    "text": "Summarizing 2D quantitative data\n\n\n\nScatterplot\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = loudness, y = energy)) +\n  geom_point(color = \"darkred\", size = 4, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nCorrelation coefficient\n\n\ncor(taylor_all_songs$loudness, \n    taylor_all_songs$energy, \n    use = \"complete.obs\")\n\n[1] 0.7910765\n\n\nNote: the default correlation you get from cor() is Pearson correlation coefficient\nOther correlations:\n\nSpearman’s correlation\nKendall rank correlation coefficient\nand more"
  },
  {
    "objectID": "lectures/04-quantitative.html#when-the-correlations-high",
    "href": "lectures/04-quantitative.html#when-the-correlations-high",
    "title": "Data visualization: quantitative data",
    "section": "When the correlation’s high…",
    "text": "When the correlation’s high…"
  },
  {
    "objectID": "lectures/04-quantitative.html#displaying-trend-line-linear-regression-a-preview",
    "href": "lectures/04-quantitative.html#displaying-trend-line-linear-regression-a-preview",
    "title": "Data visualization: quantitative data",
    "section": "Displaying trend line: linear regression(a preview)",
    "text": "Displaying trend line: linear regression(a preview)\n\n\n\nDisplay regression line forenergy ~ loudness\n95% confidence intervals by default\nEstimating the conditional expectation of energy | loudness\n\ni.e., \\(\\mathbb{E}[\\) energy \\(\\mid\\) loudness \\(]\\)\n\n\n\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = loudness, y = energy)) +\n  geom_point(color = \"darkred\", size = 4, alpha = 0.5) +\n  geom_smooth(method = \"lm\", linewidth = 2)"
  },
  {
    "objectID": "lectures/04-quantitative.html#summarizing-2d-quantitative-data-2",
    "href": "lectures/04-quantitative.html#summarizing-2d-quantitative-data-2",
    "title": "Data visualization: quantitative data",
    "section": "Summarizing 2D quantitative data",
    "text": "Summarizing 2D quantitative data\n\n\nAdd rug plots to supplement scatterplot\n\ntaylor_all_songs |&gt; \n  ggplot(aes(x = loudness, y = energy)) +\n  geom_point(color = \"darkred\", size = 4, alpha = 0.5) +\n  geom_rug(alpha = 0.4)"
  },
  {
    "objectID": "lectures/04-quantitative.html#pairs-plot",
    "href": "lectures/04-quantitative.html#pairs-plot",
    "title": "Data visualization: quantitative data",
    "section": "Pairs plot",
    "text": "Pairs plot\n\nlibrary(GGally)\ntaylor_all_songs |&gt; \n  select(danceability, energy, loudness, tempo) |&gt; \n  ggpairs()"
  },
  {
    "objectID": "lectures/04-quantitative.html#continuous-by-categorical-side-by-side-plots",
    "href": "lectures/04-quantitative.html#continuous-by-categorical-side-by-side-plots",
    "title": "Data visualization: quantitative data",
    "section": "Continuous by categorical: side by side plots",
    "text": "Continuous by categorical: side by side plots\n\ntaylor_all_songs |&gt; \n  filter(album_name %in% c(\"Lover\", \"folklore\", \"evermore\", \"Midnights\")) |&gt;\n  ggplot(aes(x = duration, y = album_name)) +\n  geom_violin() +\n  geom_boxplot(width = 0.4)"
  },
  {
    "objectID": "lectures/04-quantitative.html#continuous-by-categorical-color",
    "href": "lectures/04-quantitative.html#continuous-by-categorical-color",
    "title": "Data visualization: quantitative data",
    "section": "Continuous by categorical: color",
    "text": "Continuous by categorical: color\n\ntaylor_all_songs |&gt; \n  filter(album_name %in% c(\"Lover\", \"folklore\", \"evermore\", \"Midnights\")) |&gt;\n  ggplot(aes(x = duration, color = album_name)) +\n  stat_ecdf(linewidth = 1) +\n  scale_color_albums() + # from the taylor package \n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/04-quantitative.html#continuous-by-categorical-ridgeline-plot-joyplot",
    "href": "lectures/04-quantitative.html#continuous-by-categorical-ridgeline-plot-joyplot",
    "title": "Data visualization: quantitative data",
    "section": "Continuous by categorical: ridgeline plot (joyplot)",
    "text": "Continuous by categorical: ridgeline plot (joyplot)\nFor more, see this tutorial\n\nlibrary(ggridges)\ntaylor_all_songs |&gt; \n  filter(album_name %in% c(\"Lover\", \"folklore\", \"evermore\", \"Midnights\")) |&gt;\n  ggplot(aes(x = duration, y = album_name)) +\n  geom_density_ridges(scale = 1)"
  },
  {
    "objectID": "lectures/04-quantitative.html#what-about-for-histograms",
    "href": "lectures/04-quantitative.html#what-about-for-histograms",
    "title": "Data visualization: quantitative data",
    "section": "What about for histograms?",
    "text": "What about for histograms?\n\ntaylor_all_songs |&gt; \n  filter(album_name %in% c(\"Lover\", \"folklore\", \"evermore\", \"Midnights\")) |&gt;\n  ggplot(aes(x = duration, fill = album_name)) +\n  geom_histogram(alpha = 0.6, bins = 15) +\n  scale_fill_albums()"
  },
  {
    "objectID": "lectures/04-quantitative.html#what-about-facets",
    "href": "lectures/04-quantitative.html#what-about-facets",
    "title": "Data visualization: quantitative data",
    "section": "What about facets?",
    "text": "What about facets?\nDifference between facet_wrap and facet_grid\n\ntaylor_all_songs |&gt; \n  filter(album_name %in% c(\"Lover\", \"folklore\", \"evermore\", \"Midnights\")) |&gt;\n  ggplot(aes(x = duration)) +\n  geom_histogram(bins = 15) +\n  facet_wrap(~ album_name, nrow = 1)"
  },
  {
    "objectID": "lectures/04-quantitative.html#what-about-facets-1",
    "href": "lectures/04-quantitative.html#what-about-facets-1",
    "title": "Data visualization: quantitative data",
    "section": "What about facets?",
    "text": "What about facets?\n\ntaylor_all_songs |&gt; \n  filter(album_name %in% c(\"Lover\", \"folklore\", \"evermore\", \"Midnights\")) |&gt;\n  ggplot(aes(x = duration)) +\n  geom_histogram(bins = 15) +\n  facet_grid(album_name ~ ., margins = TRUE)"
  },
  {
    "objectID": "lectures/03-categorical.html#data",
    "href": "lectures/03-categorical.html#data",
    "title": "Data visualization: categorical data",
    "section": "Data",
    "text": "Data\n\nFlying ettiquette survey\nPublicly available on GitHub and also via the ggmosaic package (the dataset is called fly).\nWhat does each row represent here?\n\n\nlibrary(tidyverse)\ntheme_set(theme_light()) # setting the ggplot theme\nlibrary(ggmosaic) # make sure to install it first\nflying_etiquette &lt;- fly |&gt; \n  filter(!is.na(do_you_recline), !is.na(rude_to_recline))\nnames(flying_etiquette)\n\n [1] \"id\"                             \"flight_freq\"                   \n [3] \"do_you_recline\"                 \"height\"                        \n [5] \"has_child_under_18\"             \"three_seats_two_arms\"          \n [7] \"two_seats_one_arm\"              \"window_shade\"                  \n [9] \"rude_to_move_to_unsold_seat\"    \"rude_to_talk_to_neighbor\"      \n[11] \"six_hr_flight_leave_seat\"       \"reclining_obligation_to_behind\"\n[13] \"rude_to_recline\"                \"eliminate_reclining\"           \n[15] \"rude_to_switch_seats_friends\"   \"rude_to_switch_seats_family\"   \n[17] \"rude_to_wake_neighbor_bathroom\" \"rude_to_wake_neighbor_walk\"    \n[19] \"rude_to_bring_baby\"             \"rude_to_bring_unruly_child\"    \n[21] \"use_electronics_takeoff\"        \"smoked_inflight\"               \n[23] \"gender\"                         \"age\"                           \n[25] \"household_income\"               \"education\"                     \n[27] \"region\""
  },
  {
    "objectID": "lectures/03-categorical.html#categorical-data",
    "href": "lectures/03-categorical.html#categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Categorical data",
    "text": "Categorical data\nTwo different versions of categorical data:\nNominal: categorical variables having unordered scales\n\nExamples: race, gender, species, etc,\n\nOrdinal: ordered categories; levels with a meaningful order\n\nExamples: education level, grades, ranks"
  },
  {
    "objectID": "lectures/03-categorical.html#factors-in-r",
    "href": "lectures/03-categorical.html#factors-in-r",
    "title": "Data visualization: categorical data",
    "section": "Factors in R",
    "text": "Factors in R\n\nIn R, factors are used to work with categorical variables\nR treats factors as ordinal - defaults to alphabetical\n\nMay need to manually define the factor levels (e.g., the reference level)\n\nSee the forcats package (automatically loaded with tidyverse)\n\n\nclass(flying_etiquette$do_you_recline)\n\n[1] \"factor\"\n\nlevels(flying_etiquette$do_you_recline)\n\n[1] \"never\"               \"once in a while\"     \"about half the time\"\n[4] \"usually\"             \"always\""
  },
  {
    "objectID": "lectures/03-categorical.html#summarizing-1d-categorical-data",
    "href": "lectures/03-categorical.html#summarizing-1d-categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Summarizing 1D categorical data",
    "text": "Summarizing 1D categorical data\nHow often do these respondents recline?\nFrequency tables (counts)\n\ntable(flying_etiquette$do_you_recline)\n\n\n              never     once in a while about half the time             usually \n                170                 256                 117                 175 \n             always \n                136 \n\n# flying_etiquette |&gt; \n#   group_by(do_you_recline) |&gt;\n#   summarize(n = n(), .groups = \"drop\")\n\nflying_etiquette |&gt; \n  count(do_you_recline)\n\n# A tibble: 5 × 2\n  do_you_recline          n\n  &lt;fct&gt;               &lt;int&gt;\n1 never                 170\n2 once in a while       256\n3 about half the time   117\n4 usually               175\n5 always                136"
  },
  {
    "objectID": "lectures/03-categorical.html#summarizing-1d-categorical-data-1",
    "href": "lectures/03-categorical.html#summarizing-1d-categorical-data-1",
    "title": "Data visualization: categorical data",
    "section": "Summarizing 1D categorical data",
    "text": "Summarizing 1D categorical data\nProportion table\n\nprop.table(table(flying_etiquette$do_you_recline))\n\n\n              never     once in a while about half the time             usually \n          0.1990632           0.2997658           0.1370023           0.2049180 \n             always \n          0.1592506 \n\nflying_etiquette |&gt; \n  count(do_you_recline) |&gt; \n  mutate(prop = n / sum(n))\n\n# A tibble: 5 × 3\n  do_you_recline          n  prop\n  &lt;fct&gt;               &lt;int&gt; &lt;dbl&gt;\n1 never                 170 0.199\n2 once in a while       256 0.300\n3 about half the time   117 0.137\n4 usually               175 0.205\n5 always                136 0.159"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-1d-categorical-data",
    "href": "lectures/03-categorical.html#visualizing-1d-categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Visualizing 1D categorical data",
    "text": "Visualizing 1D categorical data\n\n\nCreate a bar chart with geom_bar()\n\nMap do_you_recline to the x-axis\nCounts of each category are displayed on the y-axis\n\n\nflying_etiquette |&gt; \n  ggplot(aes(x = do_you_recline)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/03-categorical.html#behind-the-scenes-of-geom_bar",
    "href": "lectures/03-categorical.html#behind-the-scenes-of-geom_bar",
    "title": "Data visualization: categorical data",
    "section": "Behind the scenes of geom_bar()",
    "text": "Behind the scenes of geom_bar()\n\nstart with the data\naggregate and count the number of observations in each bar\nmap to plot aesthetics"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-1d-categorical-data-1",
    "href": "lectures/03-categorical.html#visualizing-1d-categorical-data-1",
    "title": "Data visualization: categorical data",
    "section": "Visualizing 1D categorical data",
    "text": "Visualizing 1D categorical data\n\n\nInstead of geom_bar(), do this “by hand” (Quang prefers this way)\n\naggregate and obtain the counts first with count() or (group_by and summarize())\nthen use geom_col()\n\n\nflying_etiquette |&gt;\n  count(do_you_recline, name = \"count\") |&gt; \n  ggplot(aes(x = do_you_recline, y = count)) +\n  geom_col()"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-1d-categorical-data-2",
    "href": "lectures/03-categorical.html#visualizing-1d-categorical-data-2",
    "title": "Data visualization: categorical data",
    "section": "Visualizing 1D categorical data",
    "text": "Visualizing 1D categorical data\n\n\nFlip your bar chart axes!\nJust simply replace x with y (Quang prefers this way)\n\nflying_etiquette |&gt;\n  ggplot(aes(y = do_you_recline)) +\n  geom_bar()\n\nOr use coord_flip()\n\nflying_etiquette |&gt; \n  ggplot(aes(x = do_you_recline)) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/03-categorical.html#what-does-a-bar-chart-show",
    "href": "lectures/03-categorical.html#what-does-a-bar-chart-show",
    "title": "Data visualization: categorical data",
    "section": "What does a bar chart show?",
    "text": "What does a bar chart show?\n\n\nMarginal distribution: probability that a categorical variable \\(X\\) (e.g., do_you_recline) takes each particular category value \\(x\\) (always, usually, …, never)\n\nFrequency bar charts (earlier version) give info about sample size, but this could be labeled in the chart (use geom_text() or geom_label())\nNow, we create a proportion/percent bar chart to display the individual probabilities\nThis shows the probability mass function (PMF) for discrete variables\n\n(e.g. \\(P(\\) do_you_recline \\(=\\) never\\()\\))\n\n\n\n\nflying_etiquette |&gt; \n  count(do_you_recline) |&gt; \n  mutate(prop = n / sum(n)) |&gt; \n  ggplot(aes(x = prop, y = do_you_recline)) +\n  geom_col()   # + geom_label(aes(label = n), hjust = 1)"
  },
  {
    "objectID": "lectures/03-categorical.html#population-vs-sample",
    "href": "lectures/03-categorical.html#population-vs-sample",
    "title": "Data visualization: categorical data",
    "section": "Population vs sample",
    "text": "Population vs sample\nPopulation: The collection of all subjects of interest\nSample: A representative subset of the population of interest\n\nThe survey respondents is just a subset of all airplane flyers\n\n\nEmpirical distribution: estimating the true marginal distribution with observed (sample) data\n\n\n\nEstimate \\(P(\\) do_you_recline = \\(C_j\\)) with \\(\\hat p_j\\) for each category \\(C_j\\) (e.g., \\(\\hat p_\\texttt{always}\\), …, \\(\\hat p_\\texttt{never}\\))\n\nStandard error for each \\(\\hat p_j\\): \\(\\quad \\displaystyle \\text{SE}(\\hat{p}_j) = \\sqrt{\\frac{\\hat{p}_j (1 - \\hat{p}_j)}{n}}\\)"
  },
  {
    "objectID": "lectures/03-categorical.html#adding-confidence-intervals-to-bar-chart",
    "href": "lectures/03-categorical.html#adding-confidence-intervals-to-bar-chart",
    "title": "Data visualization: categorical data",
    "section": "Adding confidence intervals to bar chart",
    "text": "Adding confidence intervals to bar chart\n\n\n\n\nflying_etiquette |&gt; \n  count(do_you_recline) |&gt; \n  mutate(prop = n / sum(n),\n         se = sqrt(prop * (1 - prop) / sum(n)),\n         lower = prop - 2 * se,\n         upper = prop + 2 * se) |&gt; \n  ggplot(aes(x = prop, y = do_you_recline)) +\n  geom_col() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), \n                color = \"blue\", \n                width = 0.2, \n                linewidth = 1)"
  },
  {
    "objectID": "lectures/03-categorical.html#ordering-factors-in-a-bar-chart",
    "href": "lectures/03-categorical.html#ordering-factors-in-a-bar-chart",
    "title": "Data visualization: categorical data",
    "section": "Ordering factors in a bar chart",
    "text": "Ordering factors in a bar chart\n\n\nOrder the bars by proportion\n(Let’s also flip the axes)\n\nflying_etiquette |&gt; \n  count(do_you_recline) |&gt; \n  mutate(\n    prop = n / sum(n),\n    se = sqrt(prop * (1 - prop) / sum(n)),\n    lower = prop - 2 * se,\n    upper = prop + 2 * se,\n    do_you_recline = fct_reorder(do_you_recline, prop)\n  ) |&gt; \n  ggplot(aes(x = prop, y = do_you_recline)) +\n  geom_col() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), \n                color = \"blue\", \n                width = 0.2, \n                linewidth = 1)"
  },
  {
    "objectID": "lectures/03-categorical.html#pie-charts-dont-make-them",
    "href": "lectures/03-categorical.html#pie-charts-dont-make-them",
    "title": "Data visualization: categorical data",
    "section": "Pie charts… don’t make them",
    "text": "Pie charts… don’t make them\nWhy?\n\nhttps://www.data-to-viz.com/caveat/pie.html\nhttps://github.com/cxli233/FriendsDontLetFriends\n\n3D pie charts?… even worse"
  },
  {
    "objectID": "lectures/03-categorical.html#inference-for-1d-categorical-data",
    "href": "lectures/03-categorical.html#inference-for-1d-categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Inference for 1D categorical data",
    "text": "Inference for 1D categorical data\nChi-square test for 1D categorical data\n\nNull hypothesis: \\(H_0\\): \\(p_1 = p_2 = \\cdots = p_K\\)\nTest statistic: \\(\\displaystyle \\chi^2 = \\sum_{j=1}^K \\frac{(O_j - E_j)^2}{E_j}\\), where\n\n\\(O_j\\): observed counts in category \\(j\\)\n\\(E_j\\) : expected counts under the null (i.e., \\(n/K\\) or each category is equally likely to occur)\n\n\n\nchisq.test(table(flying_etiquette$do_you_recline))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(flying_etiquette$do_you_recline)\nX-squared = 66.644, df = 4, p-value = 1.159e-13"
  },
  {
    "objectID": "lectures/03-categorical.html#hypothesis-testing-in-general",
    "href": "lectures/03-categorical.html#hypothesis-testing-in-general",
    "title": "Data visualization: categorical data",
    "section": "Hypothesis testing in general",
    "text": "Hypothesis testing in general\nComputing \\(p\\)-values works like this:\n\nChoose a test statistic\nCompute the test statistic using the data\nIs test statistic “unusual” compared to what we would expect under the null?\nCompare \\(p\\)-value to the target error rate (“significance level”) \\(\\alpha\\)\n\nTypically choose \\(\\alpha = 0.05\\) (the origins of 0.05)"
  },
  {
    "objectID": "lectures/03-categorical.html#summarizing-2d-categorical-data",
    "href": "lectures/03-categorical.html#summarizing-2d-categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Summarizing 2D categorical data",
    "text": "Summarizing 2D categorical data\nContinuing with the flying etiquette survey data, let’s look at the responses to 2 questions\n\ndo_you_recline (Do you ever recline your seat when you fly?)\nrude_to_recline (Is it rude to recline your seat on a plane?)\n\nHow many levels does each variable have?\n\ntable(flying_etiquette$do_you_recline)\n\n\n              never     once in a while about half the time             usually \n                170                 256                 117                 175 \n             always \n                136 \n\ntable(flying_etiquette$rude_to_recline)\n\n\n      no somewhat      yes \n     502      281       71"
  },
  {
    "objectID": "lectures/03-categorical.html#summarizing-2d-categorical-data-1",
    "href": "lectures/03-categorical.html#summarizing-2d-categorical-data-1",
    "title": "Data visualization: categorical data",
    "section": "Summarizing 2D categorical data",
    "text": "Summarizing 2D categorical data\nTwo-way table (or contingency table, cross tabulation, crosstab)\n\ntable(\"Recline?\" = flying_etiquette$do_you_recline, \n      \"Rude to reline?\" = flying_etiquette$rude_to_recline)\n\n                     Rude to reline?\nRecline?               no somewhat yes\n  never                35       81  54\n  once in a while     116      129  11\n  about half the time  82       35   0\n  usually             145       27   3\n  always              124        9   3\n\nxtabs(~ do_you_recline + rude_to_recline, data = flying_etiquette)\n\n                     rude_to_recline\ndo_you_recline         no somewhat yes\n  never                35       81  54\n  once in a while     116      129  11\n  about half the time  82       35   0\n  usually             145       27   3\n  always              124        9   3"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-2d-categorical-data",
    "href": "lectures/03-categorical.html#visualizing-2d-categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Visualizing 2D categorical data",
    "text": "Visualizing 2D categorical data\n\n\nStacked bar chart: a bar chart of spine charts\nEmphasizes the marginal distribution of each category of x variable\n\ne.g., \\(P(\\) rude_to_recline \\(=\\) somewhat \\()\\)\n\nSimilar to 1D bar charts, start with counting every combination of 2 variables (using count() or group_by() and summarize()), then plot with geom_col()\n\n# flying_etiquette |&gt;\n#   ggplot(aes(x = rude_to_recline,\n#              fill = do_you_recline)) +\n#   geom_bar()\nflying_etiquette |&gt;\n  count(rude_to_recline, do_you_recline) |&gt;\n  ggplot(aes(x = rude_to_recline, y = n, \n             # filled by the other categorical variable\n             fill = do_you_recline)) + \n  geom_col()"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-2d-categorical-data-1",
    "href": "lectures/03-categorical.html#visualizing-2d-categorical-data-1",
    "title": "Data visualization: categorical data",
    "section": "Visualizing 2D categorical data",
    "text": "Visualizing 2D categorical data\n\n\nStacked bar chart (proportion version)\n\nflying_etiquette |&gt;\n  count(rude_to_recline, do_you_recline) |&gt;\n  ggplot(aes(x = rude_to_recline, y = n, \n             fill = do_you_recline)) +\n  geom_col(position = \"fill\")"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-2d-categorical-data-2",
    "href": "lectures/03-categorical.html#visualizing-2d-categorical-data-2",
    "title": "Data visualization: categorical data",
    "section": "Visualizing 2D categorical data",
    "text": "Visualizing 2D categorical data\n\n\nSide-by-side (grouped, dodged) bar chart: a bar chart of bar charts\nShows the conditional distribution of fill variable given x variable\n\ne.g., \\(P(\\) do_you_recline \\(=\\) always \\(\\mid\\) rude_to_recline \\(=\\) somewhat \\()\\)\n\n\nflying_etiquette |&gt;\n  count(rude_to_recline, do_you_recline) |&gt;\n  ggplot(aes(x = rude_to_recline, y = n, \n             fill = do_you_recline)) + \n  geom_col(position = \"dodge\")"
  },
  {
    "objectID": "lectures/03-categorical.html#joint-marginal-and-conditional-probabilities",
    "href": "lectures/03-categorical.html#joint-marginal-and-conditional-probabilities",
    "title": "Data visualization: categorical data",
    "section": "Joint, marginal, and conditional probabilities",
    "text": "Joint, marginal, and conditional probabilities\n\n\nLet \\(X\\) = rude_to_recline and \\(Y\\) = do_you_recline\n\nJoint distribution: frequency of the intersection\n\ne.g., \\(P(X =\\) somewhat \\(, Y =\\) always \\()\\)\n\nMarginal distribution: row sums or column sums\n\ne.g., \\(P(X =\\) somewhat \\()\\), \\(P(Y =\\) always \\()\\)\n\nConditional distribution: probability event \\(X\\) given event \\(Y\\)\n\ne.g., \\(P(X =\\) somewhat \\(\\mid Y =\\) always \\()\\)\n\n\\(\\displaystyle \\qquad \\quad = \\frac{P(X = \\texttt{somewhat}, Y = \\texttt{always})}{P(Y = \\texttt{always})}\\)\n\n\n\nflying_etiquette |&gt; \n  select(do_you_recline, rude_to_recline) |&gt; \n  table()\n\n                     rude_to_recline\ndo_you_recline         no somewhat yes\n  never                35       81  54\n  once in a while     116      129  11\n  about half the time  82       35   0\n  usually             145       27   3\n  always              124        9   3\n\nflying_etiquette |&gt; \n  select(do_you_recline, rude_to_recline) |&gt; \n  table() |&gt; \n  prop.table()\n\n                     rude_to_recline\ndo_you_recline                 no    somewhat         yes\n  never               0.040983607 0.094847775 0.063231850\n  once in a while     0.135831382 0.151053864 0.012880562\n  about half the time 0.096018735 0.040983607 0.000000000\n  usually             0.169789227 0.031615925 0.003512881\n  always              0.145199063 0.010538642 0.003512881"
  },
  {
    "objectID": "lectures/03-categorical.html#joint-marginal-and-conditional-probabilities-1",
    "href": "lectures/03-categorical.html#joint-marginal-and-conditional-probabilities-1",
    "title": "Data visualization: categorical data",
    "section": "Joint, marginal, and conditional probabilities",
    "text": "Joint, marginal, and conditional probabilities\nTwo-way proportion table (the tidyverse way) with pivot_wider\n\nflying_etiquette |&gt;\n  group_by(rude_to_recline, do_you_recline) |&gt;\n  summarize(joint = n() / nrow(flying_etiquette)) |&gt;\n  pivot_wider(names_from = rude_to_recline, values_from = joint, values_fill = 0)\n\n# A tibble: 5 × 4\n  do_you_recline          no somewhat     yes\n  &lt;fct&gt;                &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 never               0.0410   0.0948 0.0632 \n2 once in a while     0.136    0.151  0.0129 \n3 about half the time 0.0960   0.0410 0      \n4 usually             0.170    0.0316 0.00351\n5 always              0.145    0.0105 0.00351"
  },
  {
    "objectID": "lectures/03-categorical.html#categorical-heatmaps",
    "href": "lectures/03-categorical.html#categorical-heatmaps",
    "title": "Data visualization: categorical data",
    "section": "Categorical heatmaps",
    "text": "Categorical heatmaps\n\n\n\nUse geom_tile to display joint distribution of two categorical variables\nAnnotate tiles with labels of percentages using geom_text() and the scales package (a very neat package)\n\n\nflying_etiquette |&gt;\n  group_by(rude_to_recline, do_you_recline) |&gt;\n  summarize(\n    freq = n(), \n    joint = n() / nrow(flying_etiquette)\n  ) |&gt; \n  ggplot(aes(x = rude_to_recline, y = do_you_recline)) +\n  geom_tile(aes(fill = freq), color = \"white\") +\n  geom_text(aes(label = scales::percent(joint))) +\n  scale_fill_gradient2()"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-independence",
    "href": "lectures/03-categorical.html#visualizing-independence",
    "title": "Data visualization: categorical data",
    "section": "Visualizing independence",
    "text": "Visualizing independence\n\n\nMosaic plot\n\nspine chart of spine charts\nwidth: marginal distribution of rude_to_recline\nheight: conditional distribution of do_you_recline | rude_to_recline\narea: joint distribution\n\nUsing a mosaic plot to visually check for independence:\n\ncheck whether all proportions are the same (the boxes line up in a grid)\n\n\n\nflying_etiquette |&gt; \n  select(rude_to_recline, do_you_recline) |&gt; \n  table() |&gt; \n  mosaicplot(main = \"Relationship between reclining frequency and opinion on rudeness\")"
  },
  {
    "objectID": "lectures/03-categorical.html#visualizing-independence-1",
    "href": "lectures/03-categorical.html#visualizing-independence-1",
    "title": "Data visualization: categorical data",
    "section": "Visualizing independence",
    "text": "Visualizing independence\nMosaic plot with ggmosaic package\n\nflying_etiquette |&gt; \n  ggplot() +\n  geom_mosaic(aes(x = product(do_you_recline, rude_to_recline), fill = do_you_recline))"
  },
  {
    "objectID": "lectures/03-categorical.html#inference-for-2d-categorical-data",
    "href": "lectures/03-categorical.html#inference-for-2d-categorical-data",
    "title": "Data visualization: categorical data",
    "section": "Inference for 2D categorical data",
    "text": "Inference for 2D categorical data\nChi-square test for 2D categorical data\n\nNull hypothesis: \\(H_0\\): 2 categorical variables are independent of each other\n\ne.g., no association between do_you_recline and rude_to_recline\n\nTest statistic: \\(\\displaystyle \\chi^2 = \\sum_i^{k_1} \\sum_j^{k_2} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\)\n\n\n# chisq.test(table(flying_etiquette$rude_to_recline, flying_etiquette$do_you_recline))\nflying_etiquette |&gt; \n  select(rude_to_recline, do_you_recline) |&gt; \n  table() |&gt; \n  chisq.test()\n\n\n    Pearson's Chi-squared test\n\ndata:  table(select(flying_etiquette, rude_to_recline, do_you_recline))\nX-squared = 316.73, df = 8, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03-categorical.html#residuals",
    "href": "lectures/03-categorical.html#residuals",
    "title": "Data visualization: categorical data",
    "section": "Residuals",
    "text": "Residuals\nRecall the test statistic: \\(\\displaystyle \\chi^2 = \\sum_i^{k_1} \\sum_j^{k_2} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\)\nDefine the Pearson residuals: \\(\\displaystyle r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}}}\\)\nSome rules of thumb:\n\n\\(r_{ij} \\approx 0\\): observed counts are close to expected counts\n\\(|r_{ij}| &gt; 2\\): significant at \\(\\alpha = 0.05\\)\nvery positive \\(r_{ij}\\): higher than expected\nvery negative \\(r_{ij}\\): lower than expected"
  },
  {
    "objectID": "lectures/03-categorical.html#residuals-1",
    "href": "lectures/03-categorical.html#residuals-1",
    "title": "Data visualization: categorical data",
    "section": "Residuals",
    "text": "Residuals\nMosaic plots with boxes color-coded by Pearson residuals\nTells us which combinations of 2 categorical variables (cells) are much higher/lower than expected\n\nflying_etiquette |&gt; \n  select(rude_to_recline, do_you_recline) |&gt; \n  table() |&gt; \n  mosaicplot(main = \"Relationship between reclining frequency and opinion on rudeness\", shade = TRUE)"
  },
  {
    "objectID": "lectures/03-categorical.html#beyond-2d-facets",
    "href": "lectures/03-categorical.html#beyond-2d-facets",
    "title": "Data visualization: categorical data",
    "section": "Beyond 2D: facets!",
    "text": "Beyond 2D: facets!\n\nflying_etiquette %&gt;%\n  ggplot(aes(x = rude_to_recline, fill = do_you_recline)) + \n  geom_bar() +\n  facet_wrap(~ flight_freq)"
  },
  {
    "objectID": "lectures/03-categorical.html#the-janitor-package",
    "href": "lectures/03-categorical.html#the-janitor-package",
    "title": "Data visualization: categorical data",
    "section": "The janitor package",
    "text": "The janitor package\nThe most popular janitor function is clean_names()… for cleaning column names\n\n# before\niris |&gt; \n  head()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# after\nlibrary(janitor)\niris |&gt; \n  clean_names() |&gt; \n  head()\n\n  sepal_length sepal_width petal_length petal_width species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa"
  },
  {
    "objectID": "lectures/03-categorical.html#tabulation-with-the-janitor-package",
    "href": "lectures/03-categorical.html#tabulation-with-the-janitor-package",
    "title": "Data visualization: categorical data",
    "section": "Tabulation with the janitor package",
    "text": "Tabulation with the janitor package\nThe lesser-known stars of janitor: functions for tabulation of categorical data\n\n\ntabyl\n\nflying_etiquette |&gt; \n  tabyl(do_you_recline)\n\n      do_you_recline   n   percent\n               never 170 0.1990632\n     once in a while 256 0.2997658\n about half the time 117 0.1370023\n             usually 175 0.2049180\n              always 136 0.1592506\n\n\n\nadorn_*() functions\n\nflying_etiquette |&gt; \n  tabyl(do_you_recline, rude_to_recline) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(digits = 2) |&gt; \n  adorn_ns()\n\n      do_you_recline           no     somewhat         yes\n               never 20.59%  (35) 47.65%  (81) 31.76% (54)\n     once in a while 45.31% (116) 50.39% (129)  4.30% (11)\n about half the time 70.09%  (82) 29.91%  (35)  0.00%  (0)\n             usually 82.86% (145) 15.43%  (27)  1.71%  (3)\n              always 91.18% (124)  6.62%   (9)  2.21%  (3)\n\n\n\nFor more, see this overview and this tutorial"
  },
  {
    "objectID": "lectures/06-kmeans.html#recap-unsupervised-learning",
    "href": "lectures/06-kmeans.html#recap-unsupervised-learning",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Recap: Unsupervised learning",
    "text": "Recap: Unsupervised learning\n\nIn unsupervised learning, we are only given a (big) data matrix that are not labeled\nDimension reduction: Can we meaningfully reduce the dimension of the data either so we can visualize it, and potentially do better supervised learning with it?\nPCA answers this questions by finding “interesting directions” and projecting the data on to those directions\nBesides dimension reduction, clustering is another fundamental problem in unsupervised learning"
  },
  {
    "objectID": "lectures/06-kmeans.html#clustering-cluster-analysis",
    "href": "lectures/06-kmeans.html#clustering-cluster-analysis",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Clustering (cluster analysis)",
    "text": "Clustering (cluster analysis)\n\n\nClustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set — ISLR\n\n\n\nGoals: partition of the observations into distinct clusters so that\n\nobservations within clusters are more similar to each other\nobservations in different clusters are more different from each other\n\n\n\n\nThis often involves domain-specific considerations based on knowledge of the data being studied"
  },
  {
    "objectID": "lectures/06-kmeans.html#distance-between-observations",
    "href": "lectures/06-kmeans.html#distance-between-observations",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Distance between observations",
    "text": "Distance between observations\n\nWhat does it means for two or more observations to be similar or different?\n\n\n\nThis require characterizing the distance between observations\n\nClusters: groups of observations that are “close” together\n\n\n\n\n\nThis is easy to do for 2 quantitative variables: just make a scatterplot\n\n\n\nBut how do we define “distance” beyond 2D data?\n\n\nLet \\(\\boldsymbol{x}_i = (x_{i1}, \\dots, x_{ip})\\) be a vector of \\(p\\) features for observation \\(i\\)\nQuestion of interest: How “far away” is \\(\\boldsymbol{x}_i\\) from \\(\\boldsymbol{x}_j\\)?\n\n\nWhen looking at a scatterplot, we’re using Euclidean distance \\[d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\sqrt{(x_{i1} - x_{j1})^2 + \\dots + (x_{ip} - x_{jp})^2}\\]"
  },
  {
    "objectID": "lectures/06-kmeans.html#distances-in-general",
    "href": "lectures/06-kmeans.html#distances-in-general",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Distances in general",
    "text": "Distances in general\n\nThere’s a variety of different types of distance metrics: Manhattan, Mahalanobis, Cosine, Kullback-Leibler, Hellinger, Wasserstein\nWe’re just going to focus on Euclidean distance\n\n\n\nLet \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\) denote the pairwise distance between two observations \\(i\\) and \\(j\\)\n\n\nIdentity: \\(\\boldsymbol{x}_i = \\boldsymbol{x}_j \\Leftrightarrow d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = 0\\)\nNon-negativity: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\geq 0\\)\nSymmetry: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = d(\\boldsymbol{x}_j, \\boldsymbol{x}_i)\\)\nTriangle inequality: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\leq d(\\boldsymbol{x}_i, \\boldsymbol{x}_k) + d(\\boldsymbol{x}_k, \\boldsymbol{x}_j)\\)\n\n\n\n\n\nDistance Matrix: matrix \\(D\\) of all pairwise distances\n\n\\(D_{ij} = d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\)\nwhere \\(D_{ii} = 0\\) and \\(D_{ij} = D_{ji}\\)\n\n\n\\[D = \\begin{pmatrix}\n                0 & D_{12} & \\cdots & D_{1n} \\\\\n                D_{21} & 0 & \\cdots & D_{2n} \\\\\n                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n                D_{n1} & \\cdots & \\cdots & 0\n            \\end{pmatrix}\\]"
  },
  {
    "objectID": "lectures/06-kmeans.html#units-matter-in-clustering",
    "href": "lectures/06-kmeans.html#units-matter-in-clustering",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Units matter in clustering",
    "text": "Units matter in clustering\n\nVariables are typically measured in different units\nOne variable may dominate others when computing Euclidean distance because its range is much larger\nScaling of the variables matters!\nStandardize each variable in the dataset to have mean 0 and standard deviation 1 with scale()"
  },
  {
    "objectID": "lectures/06-kmeans.html#k-means-clustering-1",
    "href": "lectures/06-kmeans.html#k-means-clustering-1",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "\\(k\\)-means clustering",
    "text": "\\(k\\)-means clustering\n\nGoal: partition the observations into a pre-specified number of clusters\n\n\n\nLet \\(C_1, \\dots, C_K\\) denote sets containing indices of observations in each of the \\(k\\) clusters\n\nif observation \\(i\\) is in cluster \\(k\\), then \\(i \\in C_k\\)\n\n\n\n\n\nWe want to minimize the within-cluster variation \\(W(C_k)\\) for each cluster \\(C_k\\) (i.e. the amount by which the observations within a cluster differ from each other)\nThis is equivalent to solving \\[\\underset{C_1, \\dots, C_K}{\\text{minimize }} \\Big\\{ \\sum_{k=1}^K W(C_k) \\Big\\}\\]\nIn other words, we want to partition the observations into \\(K\\) clusters such that the total within-cluster variation, summed over all K clusters, is as small as possible"
  },
  {
    "objectID": "lectures/06-kmeans.html#k-means-clustering-2",
    "href": "lectures/06-kmeans.html#k-means-clustering-2",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "\\(k\\)-means clustering",
    "text": "\\(k\\)-means clustering\nHow do we define within-cluster variation?\n\nUse the (squared) Euclidean distance \\[W(C_k) = \\frac{1}{|C_k|}\\sum_{i,j \\in C_k} d(x_i, x_j)^2 \\,,\\] where \\(|C_k|\\) denote the number of observations in cluster \\(k\\)\nCommonly referred to as the within-cluster sum of squares (WSS)\n\n\nSo how do we solve this?"
  },
  {
    "objectID": "lectures/06-kmeans.html#lloyds-algorithm",
    "href": "lectures/06-kmeans.html#lloyds-algorithm",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Lloyd’s algorithm",
    "text": "Lloyd’s algorithm\n\n\n\nChoose \\(k\\) random centers, aka centroids\nAssign each observation closest center (using Euclidean distance)\nRepeat until cluster assignment stop changing:\n\n\nCompute new centroids as the averages of the updated groups\nReassign each observations to closest center\n\nConverges to a local optimum, not the global\nResults will change from run to run (set the seed!)\nTakes \\(k\\) as an input!"
  },
  {
    "objectID": "lectures/06-kmeans.html#gapminder-data",
    "href": "lectures/06-kmeans.html#gapminder-data",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Gapminder data",
    "text": "Gapminder data\nHealth and income outcomes for 184 countries from 1960 to 2016 from the famous Gapminder project\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nlibrary(dslabs)\nglimpse(gapminder)\n\nRows: 10,545\nColumns: 9\n$ country          &lt;fct&gt; \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\"…\n$ year             &lt;int&gt; 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960,…\n$ infant_mortality &lt;dbl&gt; 115.40, 148.20, 208.00, NA, 59.87, NA, NA, 20.30, 37.…\n$ life_expectancy  &lt;dbl&gt; 62.87, 47.50, 35.98, 62.97, 65.39, 66.86, 65.66, 70.8…\n$ fertility        &lt;dbl&gt; 6.19, 7.65, 7.32, 4.43, 3.11, 4.55, 4.82, 3.45, 2.70,…\n$ population       &lt;dbl&gt; 1636054, 11124892, 5270844, 54681, 20619075, 1867396,…\n$ gdp              &lt;dbl&gt; NA, 13828152297, NA, NA, 108322326649, NA, NA, 966778…\n$ continent        &lt;fct&gt; Europe, Africa, Africa, Americas, Americas, Asia, Ame…\n$ region           &lt;fct&gt; Southern Europe, Northern Africa, Middle Africa, Cari…"
  },
  {
    "objectID": "lectures/06-kmeans.html#gdp-is-severely-skewed-right",
    "href": "lectures/06-kmeans.html#gdp-is-severely-skewed-right",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "GDP is severely skewed right…",
    "text": "GDP is severely skewed right…\n\ngapminder |&gt; \n  ggplot(aes(x = gdp)) + \n  geom_histogram()"
  },
  {
    "objectID": "lectures/06-kmeans.html#some-initial-cleaning",
    "href": "lectures/06-kmeans.html#some-initial-cleaning",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Some initial cleaning…",
    "text": "Some initial cleaning…\n\nEach row is at the country-year level\nFocus on data for 2011 where gdp is not missing\nLog-transform gdp\n\n\nclean_gapminder &lt;- gapminder |&gt;\n  filter(year == 2011, !is.na(gdp)) |&gt;\n  mutate(log_gdp = log(gdp))"
  },
  {
    "objectID": "lectures/06-kmeans.html#k-means-clustering-example",
    "href": "lectures/06-kmeans.html#k-means-clustering-example",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "\\(k\\)-means clustering example",
    "text": "\\(k\\)-means clustering example\nNote: only 2 features are used in this example (gdp and life_expectancy),but in practice, you can (should) include more than two features\n\n\n\nUse the kmeans() function, but must provide number of clusters \\(k\\)\n\n\ninit_kmeans &lt;- clean_gapminder |&gt; \n  select(log_gdp, life_expectancy) |&gt; \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 1)\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(init_kmeans$cluster)\n  ) |&gt;\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/06-kmeans.html#careful-with-units",
    "href": "lectures/06-kmeans.html#careful-with-units",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Careful with units…",
    "text": "Careful with units…\n\n\n\nUse coord_fixed() so that the axes match with unit scales\n\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(init_kmeans$cluster)\n  ) |&gt;\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/06-kmeans.html#standardize-the-variables",
    "href": "lectures/06-kmeans.html#standardize-the-variables",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Standardize the variables!",
    "text": "Standardize the variables!\n\n\n\nUse the scale() function to first standardize the variables, \\(\\frac{\\text{value} - \\text{mean}}{\\text{sd}}\\)\n\n\nclean_gapminder &lt;- clean_gapminder |&gt;\n  mutate(\n    std_log_gdp = as.numeric(scale(log_gdp, center = TRUE, scale = TRUE)),\n    std_life_exp = as.numeric(scale(life_expectancy, center = TRUE, scale = TRUE))\n  )\n\nstd_kmeans &lt;- clean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 1)\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(std_kmeans$cluster)\n  ) |&gt;\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/06-kmeans.html#standardize-the-variables-1",
    "href": "lectures/06-kmeans.html#standardize-the-variables-1",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Standardize the variables!",
    "text": "Standardize the variables!\n\n\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(std_kmeans$cluster)\n  ) |&gt;\n  ggplot(aes(x = std_log_gdp, y = std_life_exp,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/06-kmeans.html#and-if-we-run-it-again",
    "href": "lectures/06-kmeans.html#and-if-we-run-it-again",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "And if we run it again?",
    "text": "And if we run it again?\n\n\nWe get different clustering results!\n\nanother_kmeans &lt;- clean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 1)\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(another_kmeans$cluster)\n  ) |&gt;\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n\nResults depend on initialization\nKeep in mind: the labels / colors are arbitrary"
  },
  {
    "objectID": "lectures/06-kmeans.html#fix-randomness-issue-with-nstart",
    "href": "lectures/06-kmeans.html#fix-randomness-issue-with-nstart",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Fix randomness issue with nstart",
    "text": "Fix randomness issue with nstart\n\n\nRun the algorithm nstart times, then pick the results with lowest total within-cluster variation \\[\\text{total WSS} = \\sum_{k=1}^K W(C_k)\\]\n\nnstart_kmeans &lt;- clean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 30)\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(nstart_kmeans$cluster)\n  ) |&gt; \n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/06-kmeans.html#by-default-r-uses-hartiganwong-method",
    "href": "lectures/06-kmeans.html#by-default-r-uses-hartiganwong-method",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "By default R uses Hartigan–Wong method",
    "text": "By default R uses Hartigan–Wong method\n\n\nUpdates based on changing a single observation\nComputational advantages over re-computing distances for every observation\n\ndefault_kmeans &lt;- clean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  kmeans(algorithm = \"Hartigan-Wong\",\n         centers = 4, nstart = 30) \n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(default_kmeans$cluster)\n  ) |&gt; \n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n\nVery little differences for our purposes…"
  },
  {
    "objectID": "lectures/06-kmeans.html#what-if-we-perform-clustering-with-more-than-2-variables",
    "href": "lectures/06-kmeans.html#what-if-we-perform-clustering-with-more-than-2-variables",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "What if we perform clustering with more than 2 variables?",
    "text": "What if we perform clustering with more than 2 variables?\n\nFirst, get the variables\n\n\ngapminder_features &lt;- gapminder |&gt;\n  filter(year == 2011) |&gt; \n  mutate(log_gdp = log(gdp)) |&gt; \n  select(infant_mortality, life_expectancy, fertility, log_gdp) |&gt; \n  drop_na() \n\n\nNext, standardize the variables (as always)\n\n\nstd_gapminder_features &lt;- gapminder_features |&gt; \n  scale(center = TRUE, scale = TRUE)\n\n\nNow, perform clustering\n\n\nkmeans_many_features &lt;- std_gapminder_features |&gt; \n  kmeans(algorithm = \"Hartigan-Wong\", centers = 4, nstart = 30)"
  },
  {
    "objectID": "lectures/06-kmeans.html#visualizing-clustering-results-with-pca",
    "href": "lectures/06-kmeans.html#visualizing-clustering-results-with-pca",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Visualizing clustering results with PCA",
    "text": "Visualizing clustering results with PCA\n\n\n\nIf there are more than two dimensions (variables), we can perform PCA…\nThen plot the observations (color coded by their cluster assignments) onto the first two principal components\n\nRecall that the first two PCs explain the majority of the variance in the data\n\n\n\nlibrary(factoextra)\nkmeans_many_features |&gt; \n   # need to pass in data used for clustering\n  fviz_cluster(data = std_gapminder_features,\n               geom = \"point\",\n               ellipse = FALSE) +\n  ggthemes::scale_color_colorblind() + \n  theme_light()"
  },
  {
    "objectID": "lectures/06-kmeans.html#beyond-k-means-k-means",
    "href": "lectures/06-kmeans.html#beyond-k-means-k-means",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Beyond \\(k\\)-means: \\(k\\)-means++",
    "text": "Beyond \\(k\\)-means: \\(k\\)-means++\nObjective: initialize the cluster centers before proceeding with the standard \\(k\\)-means clustering algorithm, provide a Better alternative to nstart\n\nIntuition:\n\nrandomly choose a data point the first cluster center\neach subsequent cluster center is chosen from the remaining data points with probability proportional to its squared distance from the point’s closest existing cluster center"
  },
  {
    "objectID": "lectures/06-kmeans.html#the-k-means-algorithm",
    "href": "lectures/06-kmeans.html#the-k-means-algorithm",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "The \\(k\\)-means++ algorithm",
    "text": "The \\(k\\)-means++ algorithm\nPick a random observation to be the center \\(c_1\\) of the first cluster \\(C_1\\)\n\nThis initializes a set of centers \\(\\mathscr C = \\{c_1 \\}\\)\n\n\nThen for each remaining cluster \\(c^* \\in 2, \\dots, K\\):\n\nFor each observation (that is not a center), compute \\(D(x_i) = \\underset{c \\in \\mathscr C}{\\text{min}} \\ d(x_i, c)\\)\n\nDistance between observation and its closest center \\(c \\in \\mathscr C\\)\n\n\n\n\n\nRandomly pick a point \\(x_i\\) with probability: \\(\\displaystyle p_i = \\frac{D^2(x_i)}{\\sum_{j=1}^n D^2(x_j)}\\)\n\n\n\n\nAs distance to closest center increases, the probability of selection increases\nCall this randomly selected observation \\(c^*\\), update \\(\\mathscr C = \\mathscr C \\cup c^*\\)\n\n\n\nThen run \\(k\\)-means using these \\(\\mathscr C\\) as the starting points"
  },
  {
    "objectID": "lectures/06-kmeans.html#k-means-in-r-using-flexclust",
    "href": "lectures/06-kmeans.html#k-means-in-r-using-flexclust",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "\\(k\\)-means++ in R using flexclust",
    "text": "\\(k\\)-means++ in R using flexclust\n\n\n\nlibrary(flexclust)\ninit_kmeanspp &lt;- clean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  kcca(k = 4, control = list(initcent = \"kmeanspp\"))\n\nclean_gapminder |&gt;\n  mutate(\n    country_clusters = as.factor(init_kmeanspp@cluster)\n  ) |&gt;\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n\nNote the use of @ instead of $…"
  },
  {
    "objectID": "lectures/06-kmeans.html#so-how-do-we-choose-the-number-of-clusters",
    "href": "lectures/06-kmeans.html#so-how-do-we-choose-the-number-of-clusters",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "So, how do we choose the number of clusters?",
    "text": "So, how do we choose the number of clusters?\n\nThere is no universally accepted way to conclude that a particular choice of \\(k\\) is optimal!\nFrom Cosma Shalizi’s notes\n\nOne reason you should be intensely skeptical of clustering results — including your own! — is that there is currently very little theory about how to find the right number of clusters. It’s not even completely clear what “the right number of clusters” means!\n\nAdditional readings: here and here"
  },
  {
    "objectID": "lectures/06-kmeans.html#popular-heuristic-elbow-plot-use-with-caution",
    "href": "lectures/06-kmeans.html#popular-heuristic-elbow-plot-use-with-caution",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Popular heuristic: elbow plot (use with caution)",
    "text": "Popular heuristic: elbow plot (use with caution)\nLook at the total within-cluster variation as a function of the number of clusters(do this by hand first)\n\n\n\n# function to perform clustering for each value of k\ngapminder_kmeans &lt;- function(k) {\n  \n  kmeans_results &lt;- clean_gapminder |&gt;\n    select(std_log_gdp, std_life_exp) |&gt;\n    kmeans(centers = k, nstart = 30)\n  \n  kmeans_out &lt;- tibble(\n    clusters = k,\n    total_wss = kmeans_results$tot.withinss\n  )\n  return(kmeans_out)\n}\n\n\n\n# number of clusters to search over\nn_clusters_search &lt;- 2:12\n\n# iterate over each k to compute total wss\nkmeans_search &lt;- n_clusters_search |&gt; \n  map(gapminder_kmeans) |&gt; \n  bind_rows()\n\nkmeans_search |&gt; \n  ggplot(aes(x = clusters, y = total_wss)) +\n  geom_line() + \n  geom_point(size = 4) +\n  scale_x_continuous(breaks = n_clusters_search)"
  },
  {
    "objectID": "lectures/06-kmeans.html#popular-heuristic-elbow-plot-use-with-caution-1",
    "href": "lectures/06-kmeans.html#popular-heuristic-elbow-plot-use-with-caution-1",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Popular heuristic: elbow plot (use with caution)",
    "text": "Popular heuristic: elbow plot (use with caution)\n\n\nChoose \\(k\\) where marginal improvements is low at the bend (hence the elbow)\nThis is just a guideline and should not dictate your choice of \\(k\\)\nOther choices: gap statistic, silhouette"
  },
  {
    "objectID": "lectures/06-kmeans.html#appendix-elbow-method-with-factoextra",
    "href": "lectures/06-kmeans.html#appendix-elbow-method-with-factoextra",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Appendix: elbow method with factoextra",
    "text": "Appendix: elbow method with factoextra\n\n\n\nBased on total WSS\n\n\nlibrary(factoextra)\nclean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  fviz_nbclust(kmeans, method = \"wss\")"
  },
  {
    "objectID": "lectures/06-kmeans.html#appendix-silhouette-method-with-factoextra",
    "href": "lectures/06-kmeans.html#appendix-silhouette-method-with-factoextra",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Appendix: silhouette method with factoextra",
    "text": "Appendix: silhouette method with factoextra\n\n\n\nclean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  fviz_nbclust(kmeans, method = \"silhouette\")"
  },
  {
    "objectID": "lectures/06-kmeans.html#appendix-gap-statistic",
    "href": "lectures/06-kmeans.html#appendix-gap-statistic",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Appendix: gap statistic",
    "text": "Appendix: gap statistic\n\n\n\nlibrary(cluster)\ngapminder_kmeans_gap_stat &lt;- clean_gapminder |&gt; \n  select(std_log_gdp, std_life_exp) |&gt; \n  clusGap(FUN = kmeans, nstart = 30, K.max = 10)\n# view the result \ngapminder_kmeans_gap_stat |&gt; \n  print(method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = select(clean_gapminder, std_log_gdp, std_life_exp), FUNcluster = kmeans, K.max = 10, nstart = 30)\nB=100 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 4\n          logW   E.logW       gap     SE.sim\n [1,] 4.291907 4.599646 0.3077382 0.02903413\n [2,] 3.895768 4.209010 0.3132423 0.02480610\n [3,] 3.692178 4.029632 0.3374541 0.02324887\n [4,] 3.519687 3.863577 0.3438894 0.02437498\n [5,] 3.424102 3.720414 0.2963112 0.01940829\n [6,] 3.361588 3.604618 0.2430300 0.02060756\n [7,] 3.269953 3.517374 0.2474209 0.01986324\n [8,] 3.198896 3.437972 0.2390752 0.01967937\n [9,] 3.135617 3.367203 0.2315862 0.01939875\n[10,] 3.067113 3.301655 0.2345424 0.02067550\n\n\n\n\ngapminder_kmeans_gap_stat |&gt; \n  fviz_gap_stat(maxSE = list(method = \"firstmax\"))"
  },
  {
    "objectID": "lectures/06-kmeans.html#appendix-elbow-plot-for-k-means",
    "href": "lectures/06-kmeans.html#appendix-elbow-plot-for-k-means",
    "title": "Unsupervised learning: \\(k\\)-means clustering",
    "section": "Appendix: elbow plot for \\(k\\)-means++",
    "text": "Appendix: elbow plot for \\(k\\)-means++\n\n\n\ngapminder_kmpp &lt;- function(k) {\n  \n  kmeans_results &lt;- clean_gapminder |&gt;\n    select(std_log_gdp, std_life_exp) |&gt;\n    kcca(k = k, control = list(initcent = \"kmeanspp\"))\n  \n  kmeans_out &lt;- tibble(\n    clusters = k,\n    total_wss = sum(kmeans_results@clusinfo$size * \n                      kmeans_results@clusinfo$av_dist)\n  )\n  return(kmeans_out)\n}\n\nn_clusters_search &lt;- 2:12\nkmpp_search &lt;- n_clusters_search |&gt; \n  map(gapminder_kmpp) |&gt; \n  bind_rows()\nkmpp_search |&gt; \n  ggplot(aes(x = clusters, y = total_wss)) +\n  geom_line() + \n  geom_point(size = 4) +\n  scale_x_continuous(breaks = n_clusters_search)"
  },
  {
    "objectID": "lectures/05-pca.html#unsupervised-learning",
    "href": "lectures/05-pca.html#unsupervised-learning",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nNo response variable (i.e., data are not labeled)\n\n\n\nOnly given a set of features measured on a set of observations\n\n\n\n\nUnsupervised learning is more subjective than supervised learning (difficult to tell how “good” you are doing)\n\n\n\n\nThere is no simple goal for the analysis, such as prediction of a response in supervised learning\n\n\n\n\nUnsupervised learning can be useful as a pre-processing step for supervised learning\n\n\n\nThink of unsupervised learning as an extension of EDA—there’s no unique right answer!"
  },
  {
    "objectID": "lectures/05-pca.html#fundamental-problems-in-unsupervised-learning",
    "href": "lectures/05-pca.html#fundamental-problems-in-unsupervised-learning",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Fundamental problems in unsupervised learning",
    "text": "Fundamental problems in unsupervised learning\n\nDimension reduction: reduce the original dimension of the data to something smaller so we can explore/visualize the data\n\n\nMethods: PCA (this lecture), ICA, t-SNE, UMAP,…\n\n\nClustering: group the observations in the data into different clusters\n\n\nMethods: hard clustering (\\(k\\)-means, hierachical clustering,…), soft clustering (mixture model)"
  },
  {
    "objectID": "lectures/05-pca.html#dimension-reduction-the-big-picture",
    "href": "lectures/05-pca.html#dimension-reduction-the-big-picture",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Dimension reduction: the big picture",
    "text": "Dimension reduction: the big picture\nKey question: How do we visualize the structure of high-dimensional data?\n\nExample: What if you’re given a dataset with 50 variables and are asked to make one visualization that best represents the data? What do you do?\n\n\nTedious task: Make a series of all \\(\\displaystyle \\binom{50}{2} = 1225\\) pairs of plots? Or make a giant correlation heatmap?\n\n\nIntuition: Take high-dimensional data and represent it in 2-3 dimensions, then visualize those dimensions"
  },
  {
    "objectID": "lectures/05-pca.html#motivating-example-starbucks-drinks",
    "href": "lectures/05-pca.html#motivating-example-starbucks-drinks",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Motivating example: Starbucks drinks",
    "text": "Motivating example: Starbucks drinks\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nstarbucks &lt;- read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\"\n) |&gt;\n  # convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nglimpse(starbucks)\n\nRows: 1,147\nColumns: 15\n$ product_name    &lt;chr&gt; \"brewed coffee - dark roast\", \"brewed coffee - dark ro…\n$ size            &lt;chr&gt; \"short\", \"tall\", \"grande\", \"venti\", \"short\", \"tall\", \"…\n$ milk            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, …\n$ whip            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ serv_size_m_l   &lt;dbl&gt; 236, 354, 473, 591, 236, 354, 473, 591, 236, 354, 473,…\n$ calories        &lt;dbl&gt; 3, 4, 5, 5, 3, 4, 5, 5, 3, 4, 5, 5, 3, 4, 5, 5, 35, 50…\n$ total_fat_g     &lt;dbl&gt; 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,…\n$ saturated_fat_g &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ trans_fat_g     &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ cholesterol_mg  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10,…\n$ sodium_mg       &lt;dbl&gt; 5, 10, 10, 10, 5, 10, 10, 10, 5, 5, 5, 5, 5, 5, 5, 5, …\n$ total_carbs_g   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, …\n$ fiber_g         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sugar_g         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, …\n$ caffeine_mg     &lt;dbl&gt; 130, 193, 260, 340, 15, 20, 25, 30, 155, 235, 310, 410…"
  },
  {
    "objectID": "lectures/05-pca.html#dimension-reduction-the-big-picture-1",
    "href": "lectures/05-pca.html#dimension-reduction-the-big-picture-1",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Dimension reduction: the big picture",
    "text": "Dimension reduction: the big picture\n\nIt’s usually really hard to visualize many dimensions at the same time\n\n\n\nOften, it makes a lot of sense to choose 2-3 of the “most important dimensions” and just plot those\n\n\n\n\nPCA is a very common way to define “most important dimensions”\n\n\n\n\nPCA provides the linear combinations of variables that capture the most variation in the data\n\n\n\n\nIt’s common to plot the first two principal components in a scatterplot\n\n\n\n\nIt’s very useful to plot principal components with a biplot\n\nAdds interpretability to the principal components, and helps reveal relationships among the variables"
  },
  {
    "objectID": "lectures/05-pca.html#what-is-the-goal-of-dimension-reduction",
    "href": "lectures/05-pca.html#what-is-the-goal-of-dimension-reduction",
    "title": "Unsupervised learning: principal component analysis",
    "section": "What is the goal of dimension reduction?",
    "text": "What is the goal of dimension reduction?\nWe have \\(p\\) variables (columns) for \\(n\\) observations (rows) BUT which variables are interesting?\n\nCan we find a smaller number of dimensions that captures the interesting structure in the data?\n\nCould examine all pairwise scatterplots of each variable - tedious, manual process\nClustering of variables based on correlation\nCan we find a combination of the original \\(p\\) variables?\n\n\n\nDimension reduction:\n\nFocus on reducing the dimensionality of the feature space (i.e., number of columns)\nRetain most of the information / variability in a lower dimensional space (i.e., reducing the number of columns)\nThe process: (big) \\(n \\times p\\) matrix \\(\\longrightarrow\\) dimension reduction method \\(\\longrightarrow\\) (smaller) \\(n \\times k\\) matrix"
  },
  {
    "objectID": "lectures/05-pca.html#principal-components-analysis-pca",
    "href": "lectures/05-pca.html#principal-components-analysis-pca",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Principal components analysis (PCA)",
    "text": "Principal components analysis (PCA)\nTL;DR\nPCA replaces the original \\(p\\) explanatory variables by fewer linear combinations of them (the “principal components”) that are uncorrelated while also accounting for most of their variabillity\n\n\\[\n\\begin{pmatrix}\n& & \\text{really} & & \\\\\n& & \\text{wide} & & \\\\\n& & \\text{matrix} & &\n\\end{pmatrix}\n\\rightarrow \\text{matrix algebra razzmatazz} \\rightarrow\n\\begin{pmatrix}\n\\text{much}  \\\\\n\\text{thinner}  \\\\\n\\text{matrix}\n\\end{pmatrix}\n\\]\n\nPCA explores the covariance between variables, and combines variables into a smaller set of uncorrelated variables called principal components (PCs)\n\nTurn a \\(n \\times p\\) matrix of correlated variables into a \\(n \\times k\\) matrix of uncorrelated variables"
  },
  {
    "objectID": "lectures/05-pca.html#principal-components-analysis-pca-1",
    "href": "lectures/05-pca.html#principal-components-analysis-pca-1",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Principal components analysis (PCA)",
    "text": "Principal components analysis (PCA)\n\\[\n\\begin{pmatrix}\n& & \\text{really} & & \\\\\n& & \\text{wide} & & \\\\\n& & \\text{matrix} & &\n\\end{pmatrix}\n\\rightarrow \\text{matrix algebra razzmatazz} \\rightarrow\n\\begin{pmatrix}\n\\text{much}  \\\\\n\\text{thinner}  \\\\\n\\text{matrix}\n\\end{pmatrix}\n\\]\n\nEach of the \\(k\\) columns in the right-hand matrix are principal components (PCs), all uncorrelated with each other\n\nPCs are weighted, linear combinations of the original variables\nWeights reveal how different variables are loaded into the PCs\n\nFirst column accounts for most variation in the data, second column for second-most variation, and so on\n\nIntuition: we want a small number of PCs (first few PCs) to account for most of the information/variance in the data"
  },
  {
    "objectID": "lectures/05-pca.html#what-are-principal-components",
    "href": "lectures/05-pca.html#what-are-principal-components",
    "title": "Unsupervised learning: principal component analysis",
    "section": "What are principal components?",
    "text": "What are principal components?\nAssume \\(\\boldsymbol{X}\\) is a \\(n \\times p\\) matrix that is centered and stardardized\n\nTotal variation \\(= p\\), since \\(\\text{Var}(\\boldsymbol{x}_j)\\) = 1 for all \\(j = 1, \\dots, p\\) (due to stardardization)\n\n\nPCA will give us \\(p\\) principal components that are \\(n\\)-length columns, denoted by \\(Z_1, \\dots, Z_p\\)\n\n\n\nThe first principal component is the linear combination that has the largest possible variance\nEach succeeding component has the largest possible variance under the constraint that it is uncorrelated with the preceding components\nA small number of principal components often explains a high percentage of the original variability"
  },
  {
    "objectID": "lectures/05-pca.html#first-principal-component",
    "href": "lectures/05-pca.html#first-principal-component",
    "title": "Unsupervised learning: principal component analysis",
    "section": "First principal component",
    "text": "First principal component\n\\[Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \\dots + \\phi_{p1} X_p\\]\n\n\n\\(\\phi_{j1}\\) are the weights indicating the contributions of each variable \\(j \\in 1, \\dots, p\\)\nWeights are normalized \\(\\displaystyle \\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\n\\(\\phi_{1} = (\\phi_{11}, \\phi_{21}, \\dots, \\phi_{p1})\\) is the loading vector for \\(\\text{PC}_1\\)\n\n\n\n\n\\(Z_1\\) is a linear combination of the \\(p\\) variables that has the largest variance"
  },
  {
    "objectID": "lectures/05-pca.html#second-principal-component",
    "href": "lectures/05-pca.html#second-principal-component",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Second principal component",
    "text": "Second principal component\n\\[Z_2 = \\phi_{12} X_1 + \\phi_{22} X_2 + \\dots + \\phi_{p2} X_p\\]\n\n\\(\\phi_{j2}\\) are the weights indicating the contributions of each variable \\(j \\in 1, \\dots, p\\)\nWeights are normalized \\(\\displaystyle \\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\n\\(\\phi_{2} = (\\phi_{12}, \\phi_{22}, \\dots, \\phi_{p2})\\) is the loading vector for \\(\\text{PC}_2\\)\n\\(Z_2\\) is a linear combination of the \\(p\\) variables that has the largest variance\n\nSubject to constraint it is uncorrelated with \\(Z_1\\)\n\n\n\nRepeat this process to create \\(p\\) principal components\n\nUncorrelated: Each (\\(Z_j, Z_{j'}\\)) is uncorrelated with each other\nOrdered Variance: \\(\\text{Var}(Z_1) &gt; \\text{Var}(Z_2) &gt; \\dots &gt; \\text{Var}(Z_p)\\)\nTotal Variance: \\(\\displaystyle \\sum_{j=1}^p \\text{Var}(Z_j) = p\\)"
  },
  {
    "objectID": "lectures/05-pca.html#visualizing-pca-in-two-dimensions",
    "href": "lectures/05-pca.html#visualizing-pca-in-two-dimensions",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/05-pca.html#visualizing-pca-in-two-dimensions-1",
    "href": "lectures/05-pca.html#visualizing-pca-in-two-dimensions-1",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/05-pca.html#visualizing-pca-in-two-dimensions-2",
    "href": "lectures/05-pca.html#visualizing-pca-in-two-dimensions-2",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/05-pca.html#visualizing-pca-in-two-dimensions-3",
    "href": "lectures/05-pca.html#visualizing-pca-in-two-dimensions-3",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/05-pca.html#visualizing-pca-in-two-dimensions-4",
    "href": "lectures/05-pca.html#visualizing-pca-in-two-dimensions-4",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions\nKey idea: provide low-dimensional linear surfaces that are closest to the observations\n\n\n\nThe above is the minimizing projection residuals viewpoint of PCA\n\n\n\n\nThere’s another viewpoint: maximizing variance\n\nif we project all points onto the solid orange line, we maximize the variance of the resulting projected points across all such orange lines"
  },
  {
    "objectID": "lectures/05-pca.html#data-nutritional-information-of-starbucks-drinks",
    "href": "lectures/05-pca.html#data-nutritional-information-of-starbucks-drinks",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Data: nutritional information of Starbucks drinks",
    "text": "Data: nutritional information of Starbucks drinks\n\nlibrary(tidyverse)\ntheme_set(theme_light())\nstarbucks &lt;- read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\"\n) |&gt;\n  # convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nglimpse(starbucks)\n\nRows: 1,147\nColumns: 15\n$ product_name    &lt;chr&gt; \"brewed coffee - dark roast\", \"brewed coffee - dark ro…\n$ size            &lt;chr&gt; \"short\", \"tall\", \"grande\", \"venti\", \"short\", \"tall\", \"…\n$ milk            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, …\n$ whip            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ serv_size_m_l   &lt;dbl&gt; 236, 354, 473, 591, 236, 354, 473, 591, 236, 354, 473,…\n$ calories        &lt;dbl&gt; 3, 4, 5, 5, 3, 4, 5, 5, 3, 4, 5, 5, 3, 4, 5, 5, 35, 50…\n$ total_fat_g     &lt;dbl&gt; 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,…\n$ saturated_fat_g &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ trans_fat_g     &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ cholesterol_mg  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10,…\n$ sodium_mg       &lt;dbl&gt; 5, 10, 10, 10, 5, 10, 10, 10, 5, 5, 5, 5, 5, 5, 5, 5, …\n$ total_carbs_g   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, …\n$ fiber_g         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sugar_g         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, …\n$ caffeine_mg     &lt;dbl&gt; 130, 193, 260, 340, 15, 20, 25, 30, 155, 235, 310, 410…"
  },
  {
    "objectID": "lectures/05-pca.html#implementing-pca",
    "href": "lectures/05-pca.html#implementing-pca",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Implementing PCA",
    "text": "Implementing PCA\nUse the prcomp() function (based on SVD) for PCA on centered and scaled data\n\nfeat &lt;- starbucks |&gt; \n  select(serv_size_m_l:caffeine_mg)\nstarbucks_pca &lt;- prcomp(feat, center = TRUE, scale. = TRUE)\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000"
  },
  {
    "objectID": "lectures/05-pca.html#computing-principal-components",
    "href": "lectures/05-pca.html#computing-principal-components",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Computing principal components",
    "text": "Computing principal components\nExtract the matrix of principal components (dimension will match original data)\n\nstarbucks_pc_matrix &lt;- starbucks_pca$x\nhead(starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9       PC10        PC11\n[1,] -0.02812472 0.006489978 0.05145094 0.06678083 0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 0.08080545 0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 0.09389227 0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 0.11582260 0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 0.03631676 0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 0.03497690 0.002469611\n\n\nColumns are uncorrelated, such that \\(\\text{Var}(Z_1) &gt; \\text{Var}(Z_2) &gt; \\dots &gt; \\text{Var}(Z_p)\\)"
  },
  {
    "objectID": "lectures/05-pca.html#visualizing-first-two-principal-components",
    "href": "lectures/05-pca.html#visualizing-first-two-principal-components",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Visualizing first two principal components",
    "text": "Visualizing first two principal components\n\n\n\nstarbucks &lt;- starbucks |&gt; \n  mutate(pc1 = starbucks_pc_matrix[,1], \n         pc2 = starbucks_pc_matrix[,2])\nstarbucks |&gt; \n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"PC 1\", y = \"PC 2\")\n\n\nPrincipal components are not interpretable\nMake a biplot with arrows showing the linear relationship between one variable and other variables"
  },
  {
    "objectID": "lectures/05-pca.html#making-pcs-interpretable-with-biplots",
    "href": "lectures/05-pca.html#making-pcs-interpretable-with-biplots",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Making PCs interpretable with biplots",
    "text": "Making PCs interpretable with biplots\nBiplot displays both the space of observations and the space of variables\nCheck out the factoextra package\n\n\n\nlibrary(factoextra)\n# fviz_pca_var(): projection of variables\n# fviz_pca_ind(): display observations with first two PCs\nstarbucks_pca |&gt; \n  fviz_pca_biplot(label = \"var\",\n                  alpha.ind = 0.25,\n                  alpha.var = 0.75,\n                  col.var = \"darkblue\",\n                  repel = TRUE)\n\n\nArrow direction: “as the variable increases…”\nArrow angles: correlation\n\n\\(90^{\\circ}\\): uncorrelated\n\\(&lt; 90^{\\circ}\\): positively correlated\n\\(&gt; 90^{\\circ}\\): negatively correlated\n\nArrow length: strength of relationship with PCs"
  },
  {
    "objectID": "lectures/05-pca.html#how-many-principal-components-to-use",
    "href": "lectures/05-pca.html#how-many-principal-components-to-use",
    "title": "Unsupervised learning: principal component analysis",
    "section": "How many principal components to use?",
    "text": "How many principal components to use?\nIntuition: Additional principal components will add smaller and smaller variance\n\nKeep adding components until the added variance drops off\n\n\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000"
  },
  {
    "objectID": "lectures/05-pca.html#create-a-scree-plot-or-elbow-plot",
    "href": "lectures/05-pca.html#create-a-scree-plot-or-elbow-plot",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Create a scree plot (or elbow plot)",
    "text": "Create a scree plot (or elbow plot)\n\nstarbucks_pca |&gt; \n  fviz_eig(addlabels = TRUE) +\n  geom_hline(yintercept = 100 * (1 / ncol(starbucks_pca$x)), linetype = \"dashed\", color = \"darkred\")\n\n\n\nRule of thumb: horizontal line at \\(1/p\\)"
  },
  {
    "objectID": "lectures/05-pca.html#pca-output",
    "href": "lectures/05-pca.html#pca-output",
    "title": "Unsupervised learning: principal component analysis",
    "section": "PCA output",
    "text": "PCA output\n\n# str(starbucks_pca)\n\nExamine the output after running prcomp()\n\nstarbucks_pca$sdev: singular values (\\(\\sqrt{\\lambda_j}\\))\nstarbucks_pca$rotation: loading matrix (\\(V\\))\nstarbucks_pca$x: principal component scores matrix (\\(Z=XV\\))\n\nCan use the broom package for tidying prcomp()\n\ntidy(starbucks_pca, matrix = \"eigenvalues\") # equivalent to starbucks_pca$sdev\ntidy(starbucks_pca, matrix = \"rotation\") # equivalent to starbucks_pca$rotation\ntidy(starbucks_pca, matrix = \"scores\") # equivalent to starbucks_pca$x"
  },
  {
    "objectID": "lectures/05-pca.html#proportion-of-variance-explained",
    "href": "lectures/05-pca.html#proportion-of-variance-explained",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Proportion of variance explained",
    "text": "Proportion of variance explained\n\nlibrary(broom)\nstarbucks_pca |&gt;\n  tidy(matrix = \"eigenvalues\") |&gt;\n  ggplot(aes(x = PC, y = percent)) +\n  geom_line() + \n  geom_point() +\n  geom_hline(yintercept = 1 / ncol(feat), color = \"darkred\", linetype = \"dashed\") +\n  scale_x_continuous(breaks = 1:ncol(starbucks_pca$x))"
  },
  {
    "objectID": "lectures/05-pca.html#cumulative-proportion-of-variance-explained",
    "href": "lectures/05-pca.html#cumulative-proportion-of-variance-explained",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Cumulative proportion of variance explained",
    "text": "Cumulative proportion of variance explained\n\nlibrary(broom)\nstarbucks_pca |&gt;\n  tidy(matrix = \"eigenvalues\") |&gt;\n  ggplot(aes(x = PC, y = cumulative)) +\n  geom_line() + \n  geom_point() +\n  scale_x_continuous(breaks = 1:ncol(starbucks_pca$x))"
  },
  {
    "objectID": "lectures/05-pca.html#remember-the-spelling",
    "href": "lectures/05-pca.html#remember-the-spelling",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Remember the spelling…",
    "text": "Remember the spelling…\n\n\nfolks. it's principal (not principle) components analysis.\n\n— Stephanie Hicks, PhD (@stephaniehicks) February 17, 2023"
  },
  {
    "objectID": "lectures/05-pca.html#singular-value-decomposition-svd",
    "href": "lectures/05-pca.html#singular-value-decomposition-svd",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Singular value decomposition (SVD)",
    "text": "Singular value decomposition (SVD)\n\\[\nX = U D V^T\n\\]\n\n\\(U\\) and \\(V\\): matrices containing the left and right singular vectors of scaled matrix \\(X\\)\n\\(D\\): diagonal matrix of the singular values\n\n\n\nSVD simplifies matrix-vector multiplication as rotate, scale, and rotate again\n\n\n\n\\(V\\): loading matrix for \\(X\\) with \\(\\phi_{j}\\) as columns\n\n\\(Z = X  V\\): PC matrix\n\n\n\nBonus: Eigenvalue decomposition (or spectral decomposition)\n\n\\(V\\): eigenvectors of \\(X^TX\\) (covariance matrix, \\(^T\\): transpose)\n\\(U\\): eigenvectors of \\(XX^T\\)\nThe singular values (diagonal of \\(D\\)) are square roots of the eigenvalues of \\(X^TX\\) or \\(XX^T\\)\nMeaning that \\(Z = UD\\)"
  },
  {
    "objectID": "lectures/05-pca.html#eigenvalues-guide-dimension-reduction",
    "href": "lectures/05-pca.html#eigenvalues-guide-dimension-reduction",
    "title": "Unsupervised learning: principal component analysis",
    "section": "Eigenvalues guide dimension reduction",
    "text": "Eigenvalues guide dimension reduction\nWe want to choose \\(p^* &lt; p\\) such that we are explaining variation in the data\n\nEigenvalues \\(\\lambda_j\\) for \\(j \\in 1, \\dots, p\\) indicate the variance explained by each component\n\n\\(\\displaystyle \\sum_j^p \\lambda_j = p\\), meaning \\(\\lambda_j \\geq 1\\) indicates \\(\\text{PC}j\\) contains at least one variable’s worth in variability\n\\(\\displaystyle \\frac{\\lambda_j}{p}\\): proportion of variance explained by \\(\\text{PC}j\\)\nArranged in descending order so that \\(\\lambda_1\\) is largest eigenvalue and corresponds to PC1\n\n\n\n\nCompute the cumulative proportion of variance explained (CVE) with \\(p^*\\) components \\[\\text{CVE}_{p^*} = \\sum_j^{p*} \\frac{\\lambda_j}{p}\\] Use scree plot to plot eigenvalues and guide choice for \\(p^* &lt;p\\) by looking for “elbow” (rapid to slow change)"
  },
  {
    "objectID": "sports.html",
    "href": "sports.html",
    "title": "Sports",
    "section": "",
    "text": "TBA"
  }
]