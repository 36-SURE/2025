{
  "hash": "3d3d9cd083fcd4af8d317ad283178485",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Unsupervised learning: $k$-means clustering\"\nauthor: \"<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io/2024](https://36-sure.github.io/2024)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n# Background\n\n## Recap: Unsupervised learning\n\n* In unsupervised learning, we are only given a (big) data matrix that are not labeled\n\n* Dimension reduction: Can we meaningfully reduce the dimension of the data either so we can visualize it, and potentially do better supervised learning with it?\n\n* PCA answers this questions by finding “interesting directions” and projecting the data on to those directions\n\n* Besides dimension reduction, clustering is another fundamental problem in unsupervised learning\n\n## Clustering (cluster analysis)\n\n. . .\n\n>  Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set --- ISLR\n\n. . .\n\n**Goals**: partition of the observations into distinct clusters so that\n\n-   observations **within** clusters are **more similar** to each other\n\n-   observations **in different** clusters are **more different** from each other\n\n. . .\n\n* This often involves domain-specific considerations based on knowledge of the data being studied\n\n\n## Distance between observations\n\n* What does it means for two or more observations to be similar or different?\n\n. . .\n\n* This require characterizing the __distance__ between observations\n\n  *   Clusters: groups of observations that are \"close\" together\n\n. . .\n\n*   This is easy to do for 2 quantitative variables: just make a scatterplot\n\n. . .\n\n**But how do we define \"distance\" beyond 2D data?**\n\n. . .\n\nLet $\\boldsymbol{x}_i = (x_{i1}, \\dots, x_{ip})$ be a vector of $p$ features for observation $i$\n\nQuestion of interest: How \"far away\" is $\\boldsymbol{x}_i$ from $\\boldsymbol{x}_j$?\n\n. . .\n\nWhen looking at a scatterplot, we're using __Euclidean distance__ $$d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\sqrt{(x_{i1} - x_{j1})^2 + \\dots + (x_{ip} - x_{jp})^2}$$\n\n---\n\n## Distances in general\n\n* There's a variety of different types of distance metrics: [Manhattan](https://en.wikipedia.org/wiki/Taxicab_geometry), [Mahalanobis](https://en.wikipedia.org/wiki/Mahalanobis_distance), [Cosine](https://en.wikipedia.org/wiki/Cosine_similarity), [Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), [Hellinger](https://en.wikipedia.org/wiki/Hellinger_distance),\n[Wasserstein](https://en.wikipedia.org/wiki/Wasserstein_metric)\n\n* We're just going to focus on [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)\n\n. . .\n\n* Let $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$ denote the pairwise distance between two observations $i$ and $j$\n\n1. __Identity__: $\\boldsymbol{x}_i = \\boldsymbol{x}_j \\Leftrightarrow d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = 0$\n\n2. __Non-negativity__: $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\geq 0$\n\n3. __Symmetry__: $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = d(\\boldsymbol{x}_j, \\boldsymbol{x}_i)$\n\n4. __Triangle inequality__: $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\leq d(\\boldsymbol{x}_i, \\boldsymbol{x}_k) + d(\\boldsymbol{x}_k, \\boldsymbol{x}_j)$\n\n. . .\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n__Distance Matrix__: matrix $D$ of all pairwise distances\n\n- $D_{ij} = d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$\n\n- where $D_{ii} = 0$ and $D_{ij} = D_{ji}$\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n$$D = \\begin{pmatrix}\n                0 & D_{12} & \\cdots & D_{1n} \\\\\n                D_{21} & 0 & \\cdots & D_{2n} \\\\\n                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n                D_{n1} & \\cdots & \\cdots & 0\n            \\end{pmatrix}$$\n\n:::\n:::\n\n---\n\n## Units matter in clustering\n\n-   Variables are typically measured in different units\n\n-   One variable may *dominate* others when computing Euclidean distance because its range is much larger\n\n-   Scaling of the variables matters!\n\n-   Standardize each variable in the dataset to have mean 0 and standard deviation 1 with `scale()`\n\n<!-- ??? -->\n\n<!-- It is the partitioning of data into homogeneous subgroups -->\n\n<!-- Goal define clusters for which the within-cluster variation is relatively small, i.e. observations within clusters are similar to each other -->\n\n# $k$-means clustering\n\n## $k$-means clustering\n\n-   Goal: partition the observations into a pre-specified number of clusters\n\n. . .\n\n-   Let $C_1, \\dots, C_K$ denote sets containing indices of observations in each of the $k$ clusters\n\n    -   if observation $i$ is in cluster $k$, then $i \\in C_k$\n\n. . .\n\n-   We want to minimize the **within-cluster variation** $W(C_k)$ for each cluster $C_k$ (i.e. the amount by which the observations within a cluster differ from each other)\n\n-   This is equivalent to solving $$\\underset{C_1, \\dots, C_K}{\\text{minimize }} \\Big\\{ \\sum_{k=1}^K W(C_k) \\Big\\}$$\n\n-   In other words, we want to partition the observations into $K$ clusters such that the total within-cluster variation, summed over all K clusters, is as small as possible\n\n## $k$-means clustering\n\nHow do we define within-cluster variation?\n\n-   Use the **(squared) Euclidean distance** $$W(C_k) = \\frac{1}{|C_k|}\\sum_{i,j \\in C_k} d(x_i, x_j)^2 \\,,$$ where $|C_k|$ denote the number of observations in cluster $k$\n\n-   Commonly referred to as the within-cluster sum of squares (WSS)\n\n. . .\n\n**So how do we solve this?**\n\n## [Lloyd's algorithm](https://en.wikipedia.org/wiki/K-means_clustering)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n1)  Choose $k$ random centers, aka **centroids**\n\n2)  Assign each observation closest center (using Euclidean distance)\n\n3)  Repeat until cluster assignment stop changing:\n\n-   Compute new centroids as the averages of the updated groups\n\n-   Reassign each observations to closest center\n\n**Converges to a local optimum**, not the global\n\n**Results will change from run to run** (set the seed!)\n\n**Takes** $k$ as an input!\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif){fig-align='center' width=80%}\n:::\n:::\n\n\n:::\n:::\n\n## Gapminder data\n\nHealth and income outcomes for 184 countries from 1960 to 2016 from the famous [Gapminder project](https://www.gapminder.org/data)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_light())\nlibrary(dslabs)\nglimpse(gapminder)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10,545\nColumns: 9\n$ country          <fct> \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\"…\n$ year             <int> 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960,…\n$ infant_mortality <dbl> 115.40, 148.20, 208.00, NA, 59.87, NA, NA, 20.30, 37.…\n$ life_expectancy  <dbl> 62.87, 47.50, 35.98, 62.97, 65.39, 66.86, 65.66, 70.8…\n$ fertility        <dbl> 6.19, 7.65, 7.32, 4.43, 3.11, 4.55, 4.82, 3.45, 2.70,…\n$ population       <dbl> 1636054, 11124892, 5270844, 54681, 20619075, 1867396,…\n$ gdp              <dbl> NA, 13828152297, NA, NA, 108322326649, NA, NA, 966778…\n$ continent        <fct> Europe, Africa, Africa, Americas, Americas, Asia, Ame…\n$ region           <fct> Southern Europe, Northern Africa, Middle Africa, Cari…\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## GDP is severely skewed right...\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngapminder |> \n  ggplot(aes(x = gdp)) + \n  geom_histogram() \n```\n\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Some initial cleaning...\n\n-   Each row is at the `country`-`year` level\n\n-   Focus on data for 2011 where `gdp` is not missing\n\n-   Log-transform `gdp`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder <- gapminder |>\n  filter(year == 2011, !is.na(gdp)) |>\n  mutate(log_gdp = log(gdp))\n```\n:::\n\n\n\n## $k$-means clustering example \n\n**Note: only 2 features are used in this example (`gdp` and `life_expectancy`),<br>but in practice, you can (should) include more than two features**\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n-   Use the `kmeans()` function, **but must provide number of clusters** $k$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninit_kmeans <- clean_gapminder |> \n  select(log_gdp, life_expectancy) |> \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 1)\n\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(init_kmeans$cluster)\n  ) |>\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") \n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## Careful with units...\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n-   Use `coord_fixed()` so that the axes match with unit scales\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(init_kmeans$cluster)\n  ) |>\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## Standardize the variables!\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n-   Use the `scale()` function to first **standardize the variables**, $\\frac{\\text{value} - \\text{mean}}{\\text{sd}}$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder <- clean_gapminder |>\n  mutate(\n    std_log_gdp = as.numeric(scale(log_gdp, center = TRUE, scale = TRUE)),\n    std_life_exp = as.numeric(scale(life_expectancy, center = TRUE, scale = TRUE))\n  )\n\nstd_kmeans <- clean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 1)\n\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(std_kmeans$cluster)\n  ) |>\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## Standardize the variables!\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(std_kmeans$cluster)\n  ) |>\n  ggplot(aes(x = std_log_gdp, y = std_life_exp,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## And if we run it again?\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nWe get different clustering results!\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanother_kmeans <- clean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 1)\n\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(another_kmeans$cluster)\n  ) |>\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n\n**Results depend on initialization**\n\nKeep in mind: **the labels / colors are arbitrary**\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## Fix randomness issue with `nstart`\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nRun the algorithm `nstart` times, then **pick the results with lowest total within-cluster variation** $$\\text{total WSS} = \\sum_{k=1}^K W(C_k)$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnstart_kmeans <- clean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 30)\n\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(nstart_kmeans$cluster)\n  ) |> \n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## By default `R` uses [Hartigan–Wong method](https://en.wikipedia.org/wiki/K-means_clustering#Hartigan%E2%80%93Wong_method)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nUpdates based on changing a single observation\n\n**Computational advantages over re-computing distances for every observation**\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndefault_kmeans <- clean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  kmeans(algorithm = \"Hartigan-Wong\",\n         centers = 4, nstart = 30) \n\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(default_kmeans$cluster)\n  ) |> \n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n\nVery little differences for our purposes...\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## What if we perform clustering with more than 2 variables?\n\n* First, get the variables\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngapminder_features <- gapminder |>\n  filter(year == 2011) |> \n  mutate(log_gdp = log(gdp)) |> \n  select(infant_mortality, life_expectancy, fertility, log_gdp) |> \n  drop_na() \n```\n:::\n\n\n\n* Next, standardize the variables (as always)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstd_gapminder_features <- gapminder_features |> \n  scale(center = TRUE, scale = TRUE)\n```\n:::\n\n\n\n* Now, perform clustering\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkmeans_many_features <- std_gapminder_features |> \n  kmeans(algorithm = \"Hartigan-Wong\", centers = 4, nstart = 30) \n```\n:::\n\n\n\n## Visualizing clustering results with PCA\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n* If there are more than two dimensions (variables), we can perform PCA...\n\n* Then plot the observations (color coded by their cluster assignments) onto the first two principal components \n\n  * Recall that the first two PCs explain the majority of the variance in the data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(factoextra)\nkmeans_many_features |> \n   # need to pass in data used for clustering\n  fviz_cluster(data = std_gapminder_features,\n               geom = \"point\",\n               ellipse = FALSE) +\n  ggthemes::scale_color_colorblind() + \n  theme_light()\n```\n:::\n\n\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Beyond $k$-means: $k$-means++\n\nObjective: initialize the cluster centers before proceeding with the standard $k$-means clustering algorithm, provide a Better alternative to `nstart`\n\n. . .\n\nIntuition: \n\n* randomly choose a data point the first cluster center\n\n* each subsequent cluster center is chosen from the remaining data points with probability proportional to its squared distance from the point's closest existing cluster center\n\n## The $k$-means++ algorithm\n\nPick a random observation to be the center $c_1$ of the first cluster $C_1$\n\n-   This initializes a set of centers $\\mathscr C = \\{c_1 \\}$\n\n. . .\n\nThen for each remaining cluster $c^* \\in 2, \\dots, K$:\n\n-   For each observation (that is not a center), compute $D(x_i) = \\underset{c \\in \\mathscr C}{\\text{min}} \\ d(x_i, c)$\n\n    -   Distance between observation and its closest center $c \\in \\mathscr C$\n\n. . .\n\n-   Randomly pick a point $x_i$ with probability: $\\displaystyle p_i = \\frac{D^2(x_i)}{\\sum_{j=1}^n D^2(x_j)}$\n\n. . .\n\n-   As distance to closest center increases, the probability of selection increases\n\n-   Call this randomly selected observation $c^*$, update $\\mathscr C = \\mathscr C \\cup c^*$\n\n. . .\n\nThen run $k$-means using these $\\mathscr C$ as the starting points\n\n## $k$-means++ in `R` using [`flexclust`](https://cran.r-project.org/web/packages/flexclust/flexclust.pdf)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(flexclust)\ninit_kmeanspp <- clean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  kcca(k = 4, control = list(initcent = \"kmeanspp\"))\n\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(init_kmeanspp@cluster)\n  ) |>\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n\n**Note the use of `@` instead of `$`...**\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## So, how do we choose the number of clusters?\n\n. . .\n\n**There is no universally accepted way to conclude that a particular choice of $k$ is optimal!**\n\nFrom [Cosma Shalizi's notes](https://www.stat.cmu.edu/~cshalizi/350/lectures/08/lecture-08.pdf)\n\n> One reason you should be intensely skeptical of clustering results --- including your own! --- is that there is currently very little theory about how to find the right number of clusters. It’s not even completely clear what \"the right number of clusters\" means!\n\nAdditional readings: [here](https://www.stat.cmu.edu/~larry/=sml/clustering.pdf) and [here](https://www.stat.cmu.edu/~larry/=sml/Clustering2.pdf)\n\n## Popular heuristic: elbow plot (use with caution)\n\nLook at the total within-cluster variation as a function of the number of clusters<br>(do this by hand first)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# function to perform clustering for each value of k\ngapminder_kmeans <- function(k) {\n  \n  kmeans_results <- clean_gapminder |>\n    select(std_log_gdp, std_life_exp) |>\n    kmeans(centers = k, nstart = 30)\n  \n  kmeans_out <- tibble(\n    clusters = k,\n    total_wss = kmeans_results$tot.withinss\n  )\n  return(kmeans_out)\n}\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# number of clusters to search over\nn_clusters_search <- 2:12\n\n# iterate over each k to compute total wss\nkmeans_search <- n_clusters_search |> \n  map(gapminder_kmeans) |> \n  bind_rows()\n\nkmeans_search |> \n  ggplot(aes(x = clusters, y = total_wss)) +\n  geom_line() + \n  geom_point(size = 4) +\n  scale_x_continuous(breaks = n_clusters_search)\n```\n:::\n\n\n:::\n:::\n\n## Popular heuristic: elbow plot (use with caution)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nChoose $k$ where marginal improvements is low at the bend (hence the elbow)\n\n**This is just a guideline and should not dictate your choice of** $k$\n\nOther choices: [gap statistic](https://web.stanford.edu/~hastie/Papers/gap.pdf), [silhouette](https://en.wikipedia.org/wiki/Silhouette_(clustering))\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-22-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Appendix: elbow method with `factoextra`\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n* Based on total WSS\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(factoextra)\nclean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  fviz_nbclust(kmeans, method = \"wss\")\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n:::\n\n## Appendix: silhouette method with `factoextra`\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  fviz_nbclust(kmeans, method = \"silhouette\")\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n:::\n\n## Appendix: gap statistic\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(cluster)\ngapminder_kmeans_gap_stat <- clean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  clusGap(FUN = kmeans, nstart = 30, K.max = 10)\n# view the result \ngapminder_kmeans_gap_stat |> \n  print(method = \"firstmax\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = select(clean_gapminder, std_log_gdp, std_life_exp), FUNcluster = kmeans, K.max = 10, nstart = 30)\nB=100 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --> Number of clusters (method 'firstmax'): 4\n          logW   E.logW       gap     SE.sim\n [1,] 4.291907 4.599646 0.3077382 0.02903413\n [2,] 3.895768 4.209010 0.3132423 0.02480610\n [3,] 3.692178 4.029632 0.3374541 0.02324887\n [4,] 3.519687 3.863577 0.3438894 0.02437498\n [5,] 3.424102 3.720414 0.2963112 0.01940829\n [6,] 3.361588 3.604618 0.2430300 0.02060756\n [7,] 3.269953 3.517374 0.2474209 0.01986324\n [8,] 3.198896 3.437972 0.2390752 0.01967937\n [9,] 3.135617 3.367203 0.2315862 0.01939875\n[10,] 3.067113 3.301655 0.2345424 0.02067550\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngapminder_kmeans_gap_stat |> \n  fviz_gap_stat(maxSE = list(method = \"firstmax\"))\n```\n\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-28-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n:::\n\n## Appendix: elbow plot for $k$-means++\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngapminder_kmpp <- function(k) {\n  \n  kmeans_results <- clean_gapminder |>\n    select(std_log_gdp, std_life_exp) |>\n    kcca(k = k, control = list(initcent = \"kmeanspp\"))\n  \n  kmeans_out <- tibble(\n    clusters = k,\n    total_wss = sum(kmeans_results@clusinfo$size * \n                      kmeans_results@clusinfo$av_dist)\n  )\n  return(kmeans_out)\n}\n\nn_clusters_search <- 2:12\nkmpp_search <- n_clusters_search |> \n  map(gapminder_kmpp) |> \n  bind_rows()\nkmpp_search |> \n  ggplot(aes(x = clusters, y = total_wss)) +\n  geom_line() + \n  geom_point(size = 4) +\n  scale_x_continuous(breaks = n_clusters_search)\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-kmeans_files/figure-revealjs/unnamed-chunk-29-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n<!-- ## Appendix: $k$-means for image segmentation and compression -->\n\n<!-- Goal: partition an image into multiple segments, where each segment typically represents an object in the image -->\n\n<!-- * Treat each pixel in the image as a point in 3-dimensional space comprising the intensities of the (red, blue, green) channels -->\n\n<!-- * Treat each pixel in the image as a separate data point -->\n\n<!-- *  Apply $k$-means clustering and identify the clusters -->\n\n<!-- *  All the pixels belonging to a cluster are treated as a segment in the image -->\n\n<!-- Then for any $k$, reconstruct the image by replacing each pixel vector with the (red, blue, green) triplet given by the center to which that pixel has been assigned -->\n\n<!-- ## Appendix: $k$-means for image segmentation and compression -->\n\n<!-- ```{r} -->\n<!-- # https://raw.githubusercontent.com/36-SURE/2024/main/data/spongebob.jpeg -->\n<!-- set.seed(2) -->\n<!-- library(jpeg) -->\n<!-- img_raw <- readJPEG(\"../data/spongebob.jpeg\") -->\n<!-- img_width <- dim(img_raw)[1] -->\n<!-- img_height <- dim(img_raw)[2] -->\n<!-- img_tbl <- tibble(x = rep(1:img_height, each = img_width), -->\n<!--                   y = rep(img_width:1, img_height), -->\n<!--                   r = as.vector(img_raw[, , 1]), -->\n<!--                   g = as.vector(img_raw[, , 2]), -->\n<!--                   b = as.vector(img_raw[, , 3])) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- img_kmpp <- function(k) { -->\n<!--   km_fit <- img_tbl |> -->\n<!--     select(r, g, b) |> -->\n<!--     kcca(k = k, control = list(initcent = \"kmeanspp\")) -->\n<!--   km_col <- rgb(km_fit@centers[km_fit@cluster,]) -->\n<!--   return(km_col) -->\n<!-- } -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- img_tbl <- img_tbl |> -->\n<!--   mutate(original = rgb(r, g, b), -->\n<!--          k2 = img_kmpp(2), -->\n<!--          k4 = img_kmpp(4), -->\n<!--          k8 = img_kmpp(8), -->\n<!--          k11 = img_kmpp(11)) |> -->\n<!--   pivot_longer(original:k11, names_to = \"k\", values_to = \"hex\") |> -->\n<!--   mutate(k = factor(k, levels = c(\"original\", \"k2\", \"k4\", \"k8\", \"k11\"))) -->\n<!-- ``` -->\n\n<!-- ## Appendix: $k$-means for image segmentation and compression -->\n\n\n\n\n<!-- ```{r} -->\n<!-- #| eval: false -->\n<!-- img_tbl |>  -->\n<!--   ggplot(aes(x, y, color = hex)) + -->\n<!--   geom_point() + -->\n<!--   scale_color_identity() + -->\n<!--   facet_wrap(~ k, nrow = 1) + -->\n<!--   coord_fixed() + -->\n<!--   theme_classic() -->\n<!-- ``` -->\n\n\n<!-- ```{r} -->\n<!-- #| echo: false -->\n<!-- img_tbl |>  -->\n<!--   ggplot(aes(x, y, color = hex)) + -->\n<!--   geom_point() + -->\n<!--   scale_color_identity() + -->\n<!--   facet_wrap(~ k, nrow = 1) + -->\n<!--   coord_fixed() + -->\n<!--   theme_void() + -->\n<!--   theme(strip.text = element_text(size = 14)) -->\n<!-- ``` -->\n\n\n<!-- ::: columns -->\n\n<!-- ::: {.column width=\"50%\" style=\"text-align: left;\"} -->\n\n<!-- c1 -->\n\n<!-- ::: -->\n\n<!-- ::: {.column width=\"50%\" style=\"text-align: left;\"} -->\n\n<!-- c2 -->\n\n<!-- ::: -->\n\n<!-- ::: -->\n",
    "supporting": [
      "06-kmeans_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}