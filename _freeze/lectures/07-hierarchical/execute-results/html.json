{
  "hash": "30ad6f14ede7a94361fe40b1a449cb17",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Unsupervised learning: hierarchical clustering\"\nauthor: \"<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io/2024](https://36-sure.github.io/2024)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n# Background\n\n## The big picture\n\n* $k$-means clustering: partition the observations into a pre-specified number of clusters\n\n. . .\n\n* Hierarchical clustering: does not require commitment to a particular choice of clusters\n\n  *   In fact, we end up with a tree-like visual representation of the observations, called a dendrogram\n  \n  *   This allows us to view at once the clusterings obtained for each possible number of clusters\n  \n  *   Common approach: agglomerative (bottom-up) hierarchical clustering: build a dendrogram starting from the leaves and combining clusters up to the trunk\n  \n  *   There's also divisive (top-down) hierarchical clustering: start with one large cluster and then break the cluster recursively into smaller and smaller pieces\n\n## Data: NBA player statistics per 100 possessions (2024-25 regular season)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_light())\nnba_players <- read_csv(\"https://raw.githubusercontent.com/36-SURE/2025/main/data/nba_players.csv\")\nhead(nba_players)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 33\n     rk player    age team  pos       g    gs    mp    fg   fga fg_percent   x3p\n  <dbl> <chr>   <dbl> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl>      <dbl> <dbl>\n1   152 A.J. G…    25 MIL   SG       73     7  1659   5.3  12.4      0.429   4.5\n2   379 A.J. L…    24 TOR   SG       26     2   486   7.9  18.8      0.421   3.3\n3   338 AJ Joh…    20 2TM   SG       29    11   639   6.1  15.9      0.385   1.8\n4   185 Aaron …    29 DEN   PF       51    42  1447   8.8  16.5      0.531   2.5\n5   305 Aaron …    28 HOU   PG       62     3   792   7.2  16.5      0.437   4.4\n6   250 Aaron …    25 IND   SF       45    37  1123   8.2  16.2      0.507   3.6\n# ℹ 21 more variables: x3pa <dbl>, x3p_percent <dbl>, x2p <dbl>, x2pa <dbl>,\n#   x2p_percent <dbl>, e_fg_percent <dbl>, ft <dbl>, fta <dbl>,\n#   ft_percent <dbl>, orb <dbl>, drb <dbl>, trb <dbl>, ast <dbl>, stl <dbl>,\n#   blk <dbl>, tov <dbl>, pf <dbl>, pts <dbl>, o_rtg <dbl>, d_rtg <dbl>,\n#   awards <lgl>\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## General setup\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n-   Given a dataset with $p$ variables (columns) and $n$ observations (rows) $x_1,\\dots,x_n$\n\n-   Compute the **distance/dissimilarity** between observations\n\n-   e.g. **Euclidean distance** between observations $i$ and $j$\n\n$$d(x_i, x_j) = \\sqrt{(x_{i1}-x_{j1})^2 + \\cdots + (x_{ip}-x_{jp})^2}$$\n\n**What are the distances between these counties using `x3pa` (3-point attempts) and `trb` (total rebounds)?**\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n<br>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_players |> \n  ggplot(aes(x = x3pa, y = trb)) +\n  geom_point(size = 4)\n```\n\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## Remember to standardize!\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_players <- nba_players |> \n  mutate(\n    std_x3pa = as.numeric(scale(x3pa)),\n    std_trb = as.numeric(scale(trb))\n  )\n\nnba_players |> \n  ggplot(aes(x = std_x3pa, y = std_trb)) +\n  geom_point(size = 2) +\n  coord_fixed()\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## Compute the distance matrix using `dist()`\n\n-   Compute pairwise Euclidean distance\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplayers_dist <- nba_players |> \n  select(std_x3pa, std_trb) |> \n  dist()\n```\n:::\n\n\n\n-   Returns an object of `dist` class... but not a `matrix`\n\n-   Convert to a matrix, then set the row and column names:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplayers_dist_matrix <- as.matrix(players_dist)\nrownames(players_dist_matrix) <- nba_players$player\ncolnames(players_dist_matrix) <- nba_players$player\nplayers_dist_matrix[1:4, 1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             A.J. Green A.J. Lawson AJ Johnson Aaron Gordon\nA.J. Green    0.0000000   0.8626792  1.0457677    1.5086463\nA.J. Lawson   0.8626792   0.0000000  1.3441734    1.1393048\nAJ Johnson    1.0457677   1.3441734  0.0000000    0.9839163\nAaron Gordon  1.5086463   1.1393048  0.9839163    0.0000000\n```\n\n\n:::\n:::\n\n\n\n# Hierarchical clustering\n\n## (Agglomerative) [Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n\nLet's pretend all $n$ observations are in their own cluster\n\n. . .\n\n-   Step 1: Compute the pairwise dissimilarities between each cluster\n\n    -   e.g., distance matrix on previous slides\n\n. . .\n\n-   Step 2: Identify the pair of clusters that are **least dissimilar**\n\n. . .\n\n-   Step 3: Fuse these two clusters into a new cluster!\n\n. . .\n\n-   **Repeat Steps 1 to 3 until all observations are in the same cluster**\n\n. . .\n\n**\"Bottom-up\"**, agglomerative clustering that forms a **tree/hierarchy** of merging\n\nNo mention of any randomness. And no mention of the number of clusters $k$.\n\n## (Agglomerative) [Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nStart with all observations in their own cluster\n\n-   Step 1: Compute the pairwise dissimilarities between each cluster\n\n-   Step 2: Identify the pair of clusters that are **least dissimilar**\n\n-   Step 3: Fuse these two clusters into a new cluster!\n\n-   **Repeat Steps 1 to 3 until all observations are in the same cluster**\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Clusters.svg/250px-Clusters.svg.png){fig-align='center' width=70%}\n:::\n:::\n\n\n:::\n:::\n\n## (Agglomerative) [Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nStart with all observations in their own cluster\n\n-   Step 1: Compute the pairwise dissimilarities between each cluster\n\n-   Step 2: Identify the pair of clusters that are **least dissimilar**\n\n-   Step 3: Fuse these two clusters into a new cluster!\n\n-   **Repeat Steps 1 to 3 until all observations are in the same cluster**\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Hierarchical_clustering_simple_diagram.svg/418px-Hierarchical_clustering_simple_diagram.svg.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\nForms a **dendrogram** (typically displayed from bottom-up)\n:::\n:::\n\n## Dissimilarity between clusters\n\n* We know how to compute distance/dissimilarity between two observations\n\n* **But how do we handle clusters?**\n\n  *   Dissimilarity between a cluster and an observation, or between two clusters\n\n. . .\n\nWe need to choose a **linkage function**. Clusters are built up by **linking them together**\n\n\n## Types of linkage\n\nFirst, compute all pairwise dissimilarities between the observations in the two clusters\n\ni.e., compute the distance matrix between observations, $d(x_i, x_j)$ for $i \\in C_1$ and $j \\in C_2$\n\n. . .\n\n-   **Complete linkage**: use the **maximum** (largest) value of these dissimilarities \\hfill $\\underset{i \\in C_1, j \\in C_2}{\\text{max}} d(x_i, x_j)$ (**maximal** inter-cluster dissimilarity)\n\n. . .\n\n-   **Single linkage**: use the **minimum** (smallest) value of these dissimilarities \\hfill $\\underset{i \\in C_1, j \\in C_2}{\\text{min}} d(x_i, x_j)$ (**minimal** inter-cluster dissimilarity)\n\n. . .\n\n-   **Average linkage**: use the **average** value of these dissimilarities \\hfill $\\displaystyle \\frac{1}{|C_1||C_2|} \\sum_{i \\in C_1} \\sum_{j \\in C_2} d(x_i, x_j)$ (**mean** inter-cluster dissimilarity)\n\n. . .\n\n\n## Complete linkage example\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n-   Use `hclust()` with a `dist()` objsect\n\n-   Use `complete` linkage by default\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_complete <- players_dist |> \n  hclust(method = \"complete\")\n```\n:::\n\n\n\n-   Use `cutree()` to return cluster labels\n\n-   Returns compact clusters (similar to $k$-means)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_players |> \n  mutate(\n    cluster = as.factor(cutree(nba_complete, k = 3))\n  ) |>\n  ggplot(aes(x = std_x3pa, y = std_trb,\n             color = cluster)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## What are we cutting? Dendrograms\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nUse the [`ggdendro`](https://cran.r-project.org/web/packages/ggdendro/index.html) package (instead of `plot()`)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggdendro)\nnba_complete |> \n  ggdendrogram(labels = FALSE, \n               leaf_labels = FALSE,\n               theme_dendro = FALSE) +  \n  labs(y = \"Dissimilarity between clusters\") +\n  theme(axis.text.x = element_blank(), \n        axis.title.x = element_blank(),\n        panel.grid = element_blank())\n```\n:::\n\n\n\n-   Each **leaf** is one observation\n\n-   **Height of branch indicates dissimilarity between clusters**\n\n    -   (After first step) Horizontal position along x-axis means nothing\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n\n## [Textbook example](https://bradleyboehmke.github.io/HOML/hierarchical.html)\n\n<br>\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://bradleyboehmke.github.io/HOML/19-hierarchical_files/figure-html/comparing-dendrogram-to-distances-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Cut dendrograms to obtain cluster labels\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nSpecify the height to cut with `h` (instead of `k`)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/complete-dendro-cut-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nFor example, `cutree(nba_complete, h = 4)`\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/nba-complete-cut-plot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## Single linkage example\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nChange the `method` argument to `single`\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/single-dendro-cut-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nResults in a **chaining** effect\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/nba-single-plot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## Average linkage example\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nChange the `method` argument to `average`\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/average-dendro-cut-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nCloser to `complete` but varies in compactness\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/nba-average-plot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n:::\n\n## More linkage functions\n\n-   **Centroid linkage**: Computes the dissimilarity between the centroid for cluster 1 and the centroid for cluster 2\n\n    -   i.e. distance between the averages of the two clusters\n\n    -   use `method = centroid`\n\n. . .\n\n-   **Ward's linkage**: Merges a pair of clusters to minimize the within-cluster variance\n\n    -   i.e. aim is to minimize the objection function from $K$-means\n\n    -   can use `ward.D` or `ward.D2` (different algorithms)\n\n## Post-clustering analysis\n\n* For context, how does position relate clustering results?\n\n* Two-way table to compare the clustering assignments with player positions\n\n* (What’s the way to visually compare these two variables?)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(\"Cluster\" = cutree(nba_complete, k = 3), \"Position\" = nba_players$pos)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Position\nCluster   C  PF  PG  SF  SG\n      1  15  40  69  51 100\n      2   4  25  15  22  19\n      3  70  20   1   5   1\n```\n\n\n:::\n:::\n\n\n\n* Takeaway: positions tend to fall within particular clusters\n\n## Include more variables\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n* It's easy to **include more variables** - just change the distance matrix\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_players_features <- nba_players |> \n  select(x3pa, x2pa, fta, trb, ast, stl, blk, tov)\n  \nplayer_dist_mult_features <- nba_players_features |> \n  dist()\n```\n:::\n\n\n\n* Then perform hierarchical clustering  as before\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_players_hc_complete <- player_dist_mult_features |> \n  hclust(method = \"complete\") # can try out other methods\n```\n:::\n\n\n\n* Visualize with dendrogram\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_players_hc_complete |> \n  ggdendrogram(labels = FALSE, \n               leaf_labels = FALSE,\n               theme_dendro = FALSE) +\n  labs(y = \"Dissimilarity between clusters\") +\n  theme(axis.text.x = element_blank(), \n        axis.title.x = element_blank(),\n        panel.grid = element_blank())\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n## Visualizing clustering results with PCA\n\n* Similar to $k$-means, if there are more than two dimensions (variables), we can perform PCA\n\n* Then plot the observations onto the first two principal components\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(factoextra)\nfviz_cluster(\n  list(data = nba_players_features,\n       cluster = cutree(nba_players_hc_complete, k = 3)),\n  geom = \"point\",\n  ellipse = FALSE\n) +\n  ggthemes::scale_color_colorblind() +\n  theme_light()\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Choosing number of clusters\n\n* Just like $k$-means, there are heuristics for choosing the number of clusters for hierarchical clustering\n\n* Options: elbow method, silhouette, gap statistic (but again, use these with caution)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_players_features |> \n  fviz_nbclust(FUN = hcut, method = \"wss\")\n\n# silhouette\n# nba_players_features |> \n#   fviz_nbclust(FUN = hcut, method = \"silhouette\")\n\n# gap statistic\n# library(cluster)\n# nba_hc_gap_stat <- nba_players_features |> \n#   clusGap(FUN = hcut, nstart = 30, K.max = 10, B = 50)\n# nba_hc_gap_stat |> \n#   fviz_gap_stat()\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-hierarchical_files/figure-revealjs/unnamed-chunk-22-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n\n## Practical issues\n\n* What dissimilarity measure should be used?\n\n* What type of linkage should be used?\n\n* How many clusters to choose?\n\n* Which features should we use to drive the clustering?\n  \n  *   Categorical variables?\n\n* Hard clustering vs. soft clustering \n\n  *   Hard clustering ($k$-means, hierarchical): assigns each observation to exactly one cluster\n  \n  *   Soft (fuzzy) clustering: assigns each observation a probability of belonging to a cluster\n\n## Appendix: code to build dataset\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rvest)\nnba_url <- \"https://www.basketball-reference.com/leagues/NBA_2025_per_poss.html\"\nnba_players <- nba_url |> \n  read_html() |> \n  html_element(css = \"#per_poss\") |> \n  html_table() |> \n  janitor::clean_names() |> \n  group_by(player) |> \n  slice_max(g) |> \n  ungroup() |> \n  filter(mp >= 200) # keep players with at least 200 minutes played\n```\n:::",
    "supporting": [
      "07-hierarchical_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}