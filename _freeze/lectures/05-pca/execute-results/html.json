{
  "hash": "abec9be2bb175c58b31313f13e3699e6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Unsupervised learning: principal component analysis\"\nauthor: \"<br>SURE 2025<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io/2025](https://36-sure.github.io/2025)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n\n# Background\n\n## Unsupervised learning\n\n* No response variable (i.e., data are not labeled)\n\n. . .\n\n* Only given a set of features measured on a set of observations\n\n. . .\n\n* Unsupervised learning is more subjective than supervised learning (difficult to tell how \"good\" you are doing)\n\n. . .\n\n* There is no simple goal for the analysis, such as prediction of a response in supervised learning\n\n. . .\n\n* Unsupervised learning can be useful as a pre-processing step for supervised learning\n\n. . .\n\nThink of unsupervised learning as **an extension of EDA---there’s no unique right answer!**\n\n## Fundamental problems in unsupervised learning\n\n1. **Dimension reduction:** reduce the original dimension of the data to something smaller so we can explore/visualize the data\n\n  * Methods: **PCA (this lecture)**, ICA, t-SNE, UMAP,...\n\n2. **Clustering**: group the observations in the data into different clusters\n\n  * Methods: hard clustering ($k$-means, hierachical clustering,...), soft clustering (mixture model)\n\n## Dimension reduction: the big picture\n\n**Key question: How do we visualize the structure of high-dimensional data?**\n\n. . .\n\nExample: What if you're given a dataset with 50 variables and are asked to make one visualization that best represents the data? What do you do?\n\n. . .\n\nTedious task: Make a series of all $\\displaystyle \\binom{50}{2} = 1225$ pairs of plots? Or make a giant correlation heatmap?\n\n. . .\n\n**Intuition: Take high-dimensional data and represent it in 2-3 dimensions, then visualize those dimensions**\n\n## Motivating example: Starbucks drinks\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_light())\nstarbucks <- read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\"\n) |>\n  # convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nglimpse(starbucks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,147\nColumns: 15\n$ product_name    <chr> \"brewed coffee - dark roast\", \"brewed coffee - dark ro…\n$ size            <chr> \"short\", \"tall\", \"grande\", \"venti\", \"short\", \"tall\", \"…\n$ milk            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, …\n$ whip            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ serv_size_m_l   <dbl> 236, 354, 473, 591, 236, 354, 473, 591, 236, 354, 473,…\n$ calories        <dbl> 3, 4, 5, 5, 3, 4, 5, 5, 3, 4, 5, 5, 3, 4, 5, 5, 35, 50…\n$ total_fat_g     <dbl> 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,…\n$ saturated_fat_g <dbl> 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ trans_fat_g     <dbl> 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ cholesterol_mg  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10,…\n$ sodium_mg       <dbl> 5, 10, 10, 10, 5, 10, 10, 10, 5, 5, 5, 5, 5, 5, 5, 5, …\n$ total_carbs_g   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, …\n$ fiber_g         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sugar_g         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, …\n$ caffeine_mg     <dbl> 130, 193, 260, 340, 15, 20, 25, 30, 155, 235, 310, 410…\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n## Dimension reduction: the big picture\n\n*   It's usually really hard to visualize many dimensions at the same time\n\n. . .\n\n*   Often, it makes a lot of sense to choose 2-3 of the \"most important dimensions\" and just plot those\n\n. . .\n\n*   PCA is a very common way to define \"most important dimensions\"\n\n. . .\n\n*   PCA provides the linear combinations of variables that capture the most variation in the data\n\n. . .\n\n*   It's common to plot the first two principal components in a scatterplot\n\n. . .\n\n*   It's very useful to plot principal components with a biplot\n\n    *   Adds interpretability to the principal components, and helps reveal relationships among the variables\n\n## What is the goal of dimension reduction?\n\nWe have $p$ variables (columns) for $n$ observations (rows) __BUT__ which variables are __interesting__?\n\n. . .\n\nCan we find a smaller number of dimensions that captures the __interesting__ structure in the data?\n\n  - Could examine all pairwise scatterplots of each variable - tedious, manual process\n\n  - Clustering of variables based on correlation\n\n  - Can we find a combination of the original $p$ variables?\n\n. . .\n\n__Dimension reduction__: \n\n- Focus on reducing the dimensionality of the feature space (i.e., number of columns) \n\n- __Retain__ most of the information / __variability__ in a lower dimensional space (i.e., reducing the number of columns)\n\n- The process: (big) $n \\times p$ matrix $\\longrightarrow$ dimension reduction method $\\longrightarrow$ (smaller) $n \\times k$ matrix\n\n\n## [Principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)\n\n**TL;DR** \n\nPCA replaces the original $p$ explanatory variables by fewer linear combinations of them (the “principal components”) that are uncorrelated while also accounting for most of their variabillity\n\n. . .\n\n\n$$\n\\begin{pmatrix}\n& & \\text{really} & & \\\\\n& & \\text{wide} & & \\\\\n& & \\text{matrix} & &\n\\end{pmatrix}\n\\rightarrow \\text{matrix algebra razzmatazz} \\rightarrow \n\\begin{pmatrix}\n\\text{much}  \\\\\n\\text{thinner}  \\\\\n\\text{matrix} \n\\end{pmatrix}\n$$\n\n- PCA explores the __covariance__ between variables, and combines variables into a smaller set of __uncorrelated__ variables called __principal components (PCs)__\n\n  - Turn a $n \\times p$ matrix of __correlated__ variables into a $n \\times k$ matrix of __uncorrelated__ variables\n\n## [Principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)\n\n\n$$\n\\begin{pmatrix}\n& & \\text{really} & & \\\\\n& & \\text{wide} & & \\\\\n& & \\text{matrix} & &\n\\end{pmatrix}\n\\rightarrow \\text{matrix algebra razzmatazz} \\rightarrow \n\\begin{pmatrix}\n\\text{much}  \\\\\n\\text{thinner}  \\\\\n\\text{matrix} \n\\end{pmatrix}\n$$\n\n- Each of the $k$ columns in the right-hand matrix are __principal components__ (PCs), all uncorrelated with each other\n\n  - PCs are __weighted__, linear combinations of the original variables\n\n  - Weights reveal how different variables are ___loaded___ into the PCs\n\n- First column accounts for most variation in the data, second column for second-most variation, and so on\n\nIntuition: we want a __small number of PCs__ (first few PCs) to account for most of the information/variance in the data\n\n## What are principal components?\n\nAssume $\\boldsymbol{X}$ is a $n \\times p$ matrix that is __centered__ and __stardardized__\n\n. . .\n\n_Total variation_ $= p$, since $\\text{Var}(\\boldsymbol{x}_j)$ = 1 for all $j = 1, \\dots, p$ (due to stardardization)\n\n. . .\n\nPCA will give us $p$ principal components that are $n$-length columns, denoted by $Z_1, \\dots, Z_p$\n\n. . .\n\n*   The first principal component is the linear combination that has the largest possible variance\n\n*   Each succeeding component has the largest possible variance under the constraint that it is uncorrelated with the preceding components\n\n*   A small number of principal components often explains a high percentage of the original variability\n\n## First principal component\n\n$$Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \\dots + \\phi_{p1} X_p$$\n\n. . .\n\n  - $\\phi_{j1}$ are the weights indicating the contributions of each variable $j \\in 1, \\dots, p$\n  \n  - Weights are normalized $\\displaystyle \\sum_{j=1}^p \\phi_{j1}^2 = 1$\n  \n  - $\\phi_{1} = (\\phi_{11}, \\phi_{21}, \\dots, \\phi_{p1})$ is the __loading vector__ for $\\text{PC}_1$\n\n. . .\n  \n  - $Z_1$ is a linear combination of the $p$ variables that has the __largest variance__\n\n\n## Second principal component\n\n$$Z_2 = \\phi_{12} X_1 + \\phi_{22} X_2 + \\dots + \\phi_{p2} X_p$$\n\n  - $\\phi_{j2}$ are the weights indicating the contributions of each variable $j \\in 1, \\dots, p$\n  \n  - Weights are normalized $\\displaystyle \\sum_{j=1}^p \\phi_{j1}^2 = 1$\n  \n  - $\\phi_{2} = (\\phi_{12}, \\phi_{22}, \\dots, \\phi_{p2})$ is the __loading vector__ for $\\text{PC}_2$\n  \n  - $Z_2$ is a linear combination of the $p$ variables that has the __largest variance__\n  \n    - __Subject to constraint it is uncorrelated with $Z_1$__ \n    \n. . .\n\nRepeat this process to create $p$ principal components\n\n- __Uncorrelated__: Each ($Z_j, Z_{j'}$) is uncorrelated with each other\n\n- __Ordered Variance__: $\\text{Var}(Z_1) > \\text{Var}(Z_2) > \\dots > \\text{Var}(Z_p)$\n\n- __Total Variance__: $\\displaystyle \\sum_{j=1}^p \\text{Var}(Z_j) = p$\n\n\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-pca_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-pca_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-pca_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-pca_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\nKey idea: provide low-dimensional linear surfaces that are closest to the observations\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-pca_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=1344}\n:::\n:::\n\n\n\n. . .\n\n* The above is the **minimizing projection residuals** viewpoint of PCA\n\n. . .\n\n* There's another viewpoint: **maximizing variance**\n\n  *   if we project all points onto the solid orange line, we maximize the variance of the resulting projected points across all such orange lines\n\n# Examples\n\n## Data: nutritional information of Starbucks drinks\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_light())\nstarbucks <- read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\"\n) |>\n  # convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nglimpse(starbucks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,147\nColumns: 15\n$ product_name    <chr> \"brewed coffee - dark roast\", \"brewed coffee - dark ro…\n$ size            <chr> \"short\", \"tall\", \"grande\", \"venti\", \"short\", \"tall\", \"…\n$ milk            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, …\n$ whip            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ serv_size_m_l   <dbl> 236, 354, 473, 591, 236, 354, 473, 591, 236, 354, 473,…\n$ calories        <dbl> 3, 4, 5, 5, 3, 4, 5, 5, 3, 4, 5, 5, 3, 4, 5, 5, 35, 50…\n$ total_fat_g     <dbl> 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,…\n$ saturated_fat_g <dbl> 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ trans_fat_g     <dbl> 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ cholesterol_mg  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10,…\n$ sodium_mg       <dbl> 5, 10, 10, 10, 5, 10, 10, 10, 5, 5, 5, 5, 5, 5, 5, 5, …\n$ total_carbs_g   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, …\n$ fiber_g         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sugar_g         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, …\n$ caffeine_mg     <dbl> 130, 193, 260, 340, 15, 20, 25, 30, 155, 235, 310, 410…\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Implementing PCA\n\nUse the `prcomp()` function (based on SVD) for PCA on centered and scaled data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeat <- starbucks |> \n  select(serv_size_m_l:caffeine_mg)\nstarbucks_pca <- prcomp(feat, center = TRUE, scale. = TRUE)\nsummary(starbucks_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000\n```\n\n\n:::\n:::\n\n\n\n\n## Computing principal components\n\nExtract the matrix of principal components (dimension will match original data)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstarbucks_pc_matrix <- starbucks_pca$x\nhead(starbucks_pc_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9       PC10        PC11\n[1,] -0.02812472 0.006489978 0.05145094 0.06678083 0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 0.08080545 0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 0.09389227 0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 0.11582260 0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 0.03631676 0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 0.03497690 0.002469611\n```\n\n\n:::\n:::\n\n\n\nColumns are uncorrelated, such that $\\text{Var}(Z_1) > \\text{Var}(Z_2) > \\dots > \\text{Var}(Z_p)$ \n\n## Visualizing first two principal components\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstarbucks <- starbucks |> \n  mutate(pc1 = starbucks_pc_matrix[,1], \n         pc2 = starbucks_pc_matrix[,2])\nstarbucks |> \n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"PC 1\", y = \"PC 2\")\n```\n:::\n\n\n\n- Principal components are not interpretable\n\n- Make a __biplot__ with arrows showing the linear relationship between one variable and other variables\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-pca_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n## Making PCs interpretable with biplots\n\nBiplot displays both the space of observations and the space of variables\n\nCheck out the [`factoextra`](http://www.sthda.com/english/wiki/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-visualization) package\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(factoextra)\n# fviz_pca_var(): projection of variables\n# fviz_pca_ind(): display observations with first two PCs\nstarbucks_pca |> \n  fviz_pca_biplot(label = \"var\",\n                  alpha.ind = 0.25,\n                  alpha.var = 0.75,\n                  col.var = \"darkblue\",\n                  repel = TRUE)\n```\n:::\n\n\n\n- Arrow direction: \"as the variable increases...\"\n\n- Arrow angles: correlation\n\n  - $90^{\\circ}$: uncorrelated\n  - $< 90^{\\circ}$: positively correlated\n  - $> 90^{\\circ}$: negatively correlated\n  \n- Arrow length: strength of relationship with PCs\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-pca_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n## How many principal components to use?\n\nIntuition: Additional principal components will add smaller and smaller variance\n\n- Keep adding components until the added variance _drops off_\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(starbucks_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000\n```\n\n\n:::\n:::\n\n\n\n## Create a scree plot (or elbow plot)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstarbucks_pca |> \n  fviz_eig(addlabels = TRUE) +\n  geom_hline(yintercept = 100 * (1 / ncol(starbucks_pca$x)), linetype = \"dashed\", color = \"darkred\")\n```\n\n::: {.cell-output-display}\n![](05-pca_files/figure-revealjs/scree-plot-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n- _Rule of thumb_: horizontal line at $1/p$\n\n## PCA output\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# str(starbucks_pca)\n```\n:::\n\n\n\nExamine the output after running `prcomp()`\n\n* `starbucks_pca$sdev`: singular values ($\\sqrt{\\lambda_j}$)\n* `starbucks_pca$rotation`: loading matrix ($V$)\n*  `starbucks_pca$x`: principal component scores matrix ($Z=XV$)\n\nCan use the `broom` package for tidying `prcomp()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(starbucks_pca, matrix = \"eigenvalues\") # equivalent to starbucks_pca$sdev\ntidy(starbucks_pca, matrix = \"rotation\") # equivalent to starbucks_pca$rotation\ntidy(starbucks_pca, matrix = \"scores\") # equivalent to starbucks_pca$x\n```\n:::\n\n\n\n## Proportion of variance explained\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(broom)\nstarbucks_pca |>\n  tidy(matrix = \"eigenvalues\") |>\n  ggplot(aes(x = PC, y = percent)) +\n  geom_line() + \n  geom_point() +\n  geom_hline(yintercept = 1 / ncol(feat), color = \"darkred\", linetype = \"dashed\") +\n  scale_x_continuous(breaks = 1:ncol(starbucks_pca$x))\n```\n\n::: {.cell-output-display}\n![](05-pca_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Cumulative proportion of variance explained\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(broom)\nstarbucks_pca |>\n  tidy(matrix = \"eigenvalues\") |>\n  ggplot(aes(x = PC, y = cumulative)) +\n  geom_line() + \n  geom_point() +\n  scale_x_continuous(breaks = 1:ncol(starbucks_pca$x))\n```\n\n::: {.cell-output-display}\n![](05-pca_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Remember the spelling...\n\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">folks. it&#39;s _principal_ (not principle) components analysis.</p>&mdash; Stephanie Hicks, PhD (@stephaniehicks) <a href=\"https://twitter.com/stephaniehicks/status/1626428875323367424?ref_src=twsrc%5Etfw\">February 17, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n# Appendix\n\n## [__Singular value decomposition (SVD)__](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n\n$$\nX = U D V^T\n$$\n\n- $U$ and $V$: matrices containing the left and right __singular vectors of scaled matrix $X$__\n\n- $D$: diagonal matrix of the __singular values__\n\n. . .\n\n- SVD simplifies matrix-vector multiplication as __rotate, scale, and rotate again__\n\n. . .\n\n$V$: __loading matrix__ for $X$ with $\\phi_{j}$ as columns\n\n  - $Z = X  V$: PC matrix\n\n. . .\n\nBonus: __Eigenvalue decomposition__ (or spectral decomposition)\n\n- $V$: __eigenvectors__ of $X^TX$ (covariance matrix, $^T$: _transpose_)\n\n- $U$: __eigenvectors__ of $XX^T$\n\n- The singular values (diagonal of $D$) are square roots of the __eigenvalues__ of $X^TX$ or $XX^T$\n\n- Meaning that $Z = UD$\n\n\n\n## Eigenvalues guide dimension reduction\n\nWe want to choose $p^* < p$ such that we are explaining variation in the data\n\n. . .\n\nEigenvalues $\\lambda_j$ for $j \\in 1, \\dots, p$ indicate __the variance explained by each component__\n\n  - $\\displaystyle \\sum_j^p \\lambda_j = p$, meaning $\\lambda_j \\geq 1$ indicates $\\text{PC}j$ contains at least one variable's worth in variability\n  \n  - $\\displaystyle \\frac{\\lambda_j}{p}$: proportion of variance explained by $\\text{PC}j$\n  \n  - Arranged in descending order so that $\\lambda_1$ is largest eigenvalue and corresponds to PC1\n  \n. . .\n  \n  - Compute the cumulative proportion of variance explained (CVE) with $p^*$ components $$\\text{CVE}_{p^*} = \\sum_j^{p*} \\frac{\\lambda_j}{p}$$\nUse [__scree plot__](https://en.wikipedia.org/wiki/Scree_plot) to plot eigenvalues and guide choice for $p^* <p$ by looking for \"elbow\" (rapid to slow change)\n",
    "supporting": [
      "05-pca_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}